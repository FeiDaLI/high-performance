<h2 id="QSjKN">第一章 绪论</h2>
<h3 id="ta5AF">1.1系统性能</h3>
系统性能研究涵盖所有硬件组件和整个软件栈，包括数据路径及软硬件上发生的事。单台服务器通用系统软件栈含操作系统内核、数据库和应用程序层，“全栈”在系统性能语境下指所有相关内容 。

![](https://cdn.nlark.com/yuque/0/2025/png/45533403/1745390187991-595763ca-91f9-4098-ba8e-ff08f668d502.png)

<h3 id="pHAac">1.2 人员</h3>
系统性能工作需多类人员参与，多数人（系统管理员、技术支持等）在本职范围关注性能，复杂问题需团队协作。部分公司设性能工程师，负责维护系统性能，进行系统级分析、容量规划等 ；还有专职于特定应用程序的性能工程师，如Java、MySQL性能工程师，但检查范围有限。

<h3 id="vdSvr">1.3事情</h3>
性能领域工作按理想顺序包括：设置性能目标建模；基于原型归纳性能特征；代码性能分析；软件非回归性测试；基准测试；概念验证测试；配置优化；监控软件；特定问题性能分析 。容量规划包括开发前研究资源占用、部署后监控资源使用。不同公司和产品执行步骤有差异。 

<h3 id="YKSDV">1.4 视角</h3>
性能分析有负载分析和资源分析两个视角。系统管理员常从资源分析视角出发，应用开发者多采用负载分析视角，尝试从两个视角分析对解决挑战性问题有益。 

![](https://cdn.nlark.com/yuque/0/2025/png/45533403/1745390208264-e98f9caa-4e71-4490-979d-a9fdfb3754d1.png)

<h3 id="KffAx">1.5 性能是充满挑战的</h3>
+ **性能是主观的**：与技术学科判断bug存在与否不同，性能好坏判断常模糊，取决于应用开发人员和最终用户预期。可通过定义清晰目标（如目标平均响应时间等）使主观性能客观化。 
+ **系统是复杂的**：系统性能因系统复杂性难确定分析起点，性能问题可能源于子系统交互或连锁故障，瓶颈相互关联，解决需全局方法，涉及多方面技能。 
+ **可能有多个问题并存**：复杂软件常存在多个性能问题，关键是辨别哪些问题重要。可用延时指标量化性能，需量化问题并估计修复后增益。最重要的指标是延时。

<h3 id="TkYUv">1.6 延时</h3>
**延时的定义与意义**

延时测量的是操作等待时间，广义上可表示各类操作（如应用程序请求、数据库查询、文件系统操作等）完成的耗时 。例如从点击链接到网页加载完成的时间。对客户和网站提供商而言，高延时会导致客户流失，是重要指标。

**延时与性能提升计算**

![](https://cdn.nlark.com/yuque/0/2025/png/45533403/1745390344565-8cd4e3c4-b0fb-4f8e-b637-2075fd698033.png)

延时可用于估计最大增速。如一次数据库查询延时为100ms，其中80ms用于等待磁盘读取，通过减少磁盘读取时间（如使用缓存 ），可提升性能，经计算最大增速可达5倍。这种计算方式对其他指标（如每秒I/O操作次数IOPS ）不一定适用，因IOPS受I/O类型等影响，单纯数值变化难以直接衡量性能影响。

**延时在不同场景的含义及获取**

在网络场景中，延时指建立连接的时间，而非数据传输时间。不同章节会对相关术语进行解释，避免因场景不同造成理解差异。虽然延时很有用，但并非所有系统都能随时获取，部分系统只有平均延时，甚至无延时指标。动态跟踪技术可在感兴趣的点测量延时，提供完整延时信息。这部分内容围绕动态跟踪技术展开，主要信息如下：

<h3 id="U75pP">1.7 动态跟踪技术概述</h3>
动态跟踪技术可对所有软件进行监控，适用于真实生产环境。它利用内存中的CPU指令构建监测数据，能获取定制化性能统计数据，相比系统自带统计，大幅提升观测性，让原本因难以观测而无法处理或难处理的问题得到解决或简化。

**与传统观测方法对比**

传统观测方法（如利用系统自带统计分析操作系统内核 ）如同拿着蜡烛在黑屋照亮特定地方，而动态跟踪像手电筒，可按需照亮。

**动态跟踪工具Dtrace**

+ **诞生与移植**：Dtrace是首个适用于生产环境的动态跟踪工具，由Sun Microsystems公司开发，2005年随Solaris 10推出，是首个开源的Solaris组件，之后移植到Mac OS X和FreeBSD，针对Linux的移植持续进行中。
+ **优势**：在Dtrace之前，系统跟踪常用静态探针，观测有限且耗时，需配置、跟踪、导出数据及后期分析。Dtrace对用户态和内核态软件都提供静态跟踪和动态跟踪，数据实时产生。

文中给出Dtrace跟踪ssh登录进程执行过程的示例，展示了Dtrace打印出的时间戳、进程名和参数等信息，还提到用D语言可编写更复杂脚本，Dtrace和动态跟踪在后续章节会深入讲解，另有关于Dtrace高阶应用的书籍 。

这部分内容主要介绍云计算对系统性能的影响，具体如下：

<h3 id="qApWH">1.8 云计算</h3>
云计算及其根基——虚拟技术的兴起，对系统性能产生影响。云计算架构可使应用程序均衡分布在数量不断增加的小型系统中，便于快速扩展 。还降低了对容量规划精确程度的要求，可便捷在云端添加容量。在某些情况下，云计算对性能分析需求更高，因其使用较少资源，能减少系统使用数目，直接节约成本，这与企业用户受支持协议限制多年不同。 

**云计算带来的难题**

云计算和虚拟化技术带来新问题，如性能隔离问题，即如何管理其他租户（performance isolation ）对性能的影响，以及如何让每个租户都能对物理系统进行观测。例如，若系统管理不善，磁盘I/O性能可能因邻近租户竞争而下降，且部分环境中租户难以观察物理磁盘真实使用情况，甄别问题困难，这些内容将在第11章介绍。

<h3 id="op1or">1.9 案例分析</h3>
这部分内容是关于系统性能的案例研究，通过虚构案例展示性能研究方式，具体如下： 

<h4 id="V7ajL">案例一：缓慢的磁盘</h4>
+ **问题背景**：Scott是中型公司系统管理员，数据库团队提交工单称数据库服务器磁盘缓慢，未说明是否由数据库引发。 
+ **问题排查过程**
    - **初步询问与数据收集**：Scott询问当前数据库性能问题衡量方式、问题出现时长、数据库变动情况及怀疑磁盘原因。数据库团队回复查询延时超1000ms，过去一周此类查询数达每小时几十个，且那段时间磁盘繁忙 。
    - **资源使用情况查看**：Scott登录AcrmeMon查看，发现磁盘使用率高（80%左右 ），其他资源使用率低。用iostat测量，发现磁盘使用率逐步上升，CPU使用率持平，AcrmeMon未提供磁盘饱和（或错误）统计数据。
    - **深入排查**：用iostat查看磁盘错误数为零，以秒和分钟为间隔观察磁盘使用率，发现波动且常达100%，增加磁盘I/O延时。利用动态跟踪脚本捕捉数据库被内核取消调度时的栈跟踪，发现数据库查询常被文件系统读操作阻塞。 
    - **分析与定位**：对负载特征归纳，计算平均I/O大小、估计访问模式，认为是磁盘高负载问题。让数据库团队将检查磁盘的命令添加到负载中，发现磁盘能正常应对负载，排除磁盘硬件问题。进一步分析发现磁盘I/O由文件系统缓存（页缓存）未命中导致，且其他服务器缓存命中率比该服务器高很多 。
+ **问题解决**：Scott让生产数据库应用程序开发团队将应用移到另一台服务器作参照，AcrmeMon上磁盘使用率下降，文件系统缓存恢复，关闭工单。

<h4 id="vLGEF">案例二：软件变更</h4>
+ **问题背景**：Pamela是小公司性能扩展工程师，负责性能相关事务。应用程序开发人员开发新核心功能，不确定引入该功能是否影响性能，在部署到生产环境前需进行性能测试 。
+ **测试准备与过程**
    - **测试方案确定**：Pamela决定对应用程序新版本执行非回归性测试（用于确认软件或硬件变更未使性能倒退 ）。使用空闲服务器和客户负载模拟器，因之前写的模拟器有局限仍决定使用，通过研究访问日志分析客户工作负载，利用公司现有工具对比不同时间段生产环境日志分析负载 。
    - **负载测试执行**：从100客户请求开始，每次以100为增量增加负载，直至达到极限，每个测试级别测试一分钟，用shell脚本执行测试并收集结果 。随负载增加，通过行动态基准测试判定限制因素，发现服务器资源和线程能承受大量空闲，客户端模拟器显示请求数稳定在约700客户请求。
+ **问题分析与解决**
    - **初步分析**：切换到新软件版本并重复测试，发现约700客户请求就不稳定。分析结果、试图找原因但无果，绘图展示请求成功率随负载的变化情况，观察软件版本扩展特性，发现两个版本性能相似但未找到限制客户数扩展的因素，怀疑问题在应用程序逻辑或客户端其他地方 。
    - **深入排查**：为了解更多CPU资源使用情况，运行更高负载并研究数据路径组件，包括网络、客户系统和客户工作负载产生器，对服务端和客户端软件向下挖掘分析 。记录检查情况（包括屏幕截图 ），执行线程状态分析发现客户端软件是单线程，100%执行时间都在一个CPU上，这是性能限制因素。
    - **问题解决与反馈**：通过在不同客户系统并行运行客户端软件进行验证实验，发现当前版本和新版本每秒服务器请求数与CPU使用率的关系一致，当前版本达每秒3500请求，新版本每秒2300请求，新软件版本性能倒退 。Pamela通知开发人员性能倒退情况，打算对CPU使用剖析找是哪条代码路径导致，指出一般性生产工作负载已测试，但多样性工作负载未测，还发现客户工作负载生成器是单线程会成为瓶颈并提交bug 。

<h2 id="i900d"><font style="color:#601BDE;">第二章 方法</font></h2>
+ **引言**：引用夏洛克·福尔摩斯“波西米亚丑闻”中柯南·道尔的话，强调在获取数据前将事情理论化是错误的，应让理论适应事实。面对性能不佳且复杂的系统环境，需明确从何处分析、收集何种数据及如何分析，第1章已介绍性能问题可能出现的位置。
+ **方法的作用**：方法对复杂系统性能分析有帮助，能告知从何处开始工作、定位和分析性能问题的步骤，还可作为检查清单，涵盖量化和确认发现的方法，以分辨重要性能问题。
+ **章节内容**：本章包括背景（介绍术语、基本模型、关键性能概念、审视问题视角 ）、方法（讨论性能分析方法如观察法和实验法、建模、容量规划 ）、指标（介绍性能统计、监控和数据可视化 ）三部分，本章方法在后续章节会更详尽讨论。

<h3 id="AeroF">2.1术语</h3>
介绍系统性能相关关键术语：

+ **IOPS**：每秒输入/输出操作次数，衡量数据传输，磁盘读写中即每秒读写次数。
+ **吞吐量**：评价工作执行速率，用于描述数据传输速度（字节/秒或比特/秒 ），数据库场景指操作速度（每秒操作数或每秒业务数 ）。 
+ **响应时间**：一次操作完成时间，含等待、服务及返回结果时间。 
+ **延时**：描述操作中等待服务时间，某些情况等同于响应时间。 
+ **使用率**：给定时间区间内，服务请求资源的繁忙程度，存储资源中，指消耗的存储容量（如内存使用率 ）。 
+ **饱和**：资源无法满足服务请求的排队工作量。 
+ **瓶颈**：限制系统性能的资源，分辨和移除系统瓶颈是系统性能重要工作。 
+ **工作负载**：系统输入或对系统施加的负载，数据库中即客户端发出的请求和命令。 
+ **缓存**：复制或缓冲数据的高速存储区域，避免直接访问较慢存储层级以提高性能，缓存区容量小于慢一级存储容量。

<h4 id="op9QV">2.2.1 受测系统</h4>
+ **模型构成**：受测系统（SUT，system under test）的性能模型如图2.1所示，包含输入（工作负载）、测试系统、扰动以及性能结果。扰动会影响性能结果，其来源包括定时执行的系统活动、系统其他用户及其他工作负载 ，在一些云环境中，从单个客户SUT视角难以观察到由其他租户引起的物理宿主系统的活动。
+ **环境特点与分析方法**：现代系统常由多个网络化组件构成，用于处理输入工作负载，如负载平衡、Web服务器、数据库服务器等。将此环境模型化，有助于发现扰动源，还可将其模型化成排队系统进行分析研究。

![](https://cdn.nlark.com/yuque/0/2025/png/45533403/1745390875528-06443622-5927-4311-abff-12992001ef72.png)

<h4 id="ZhsrX">2.2.2 排队系统</h4>
+ **模型展示**：某些组件和资源可模型化成排队系统，图2.2展示了简易排队系统，包括输入、队列、服务中心、输出以及响应时间。
+ **相关理论**：2.6节会介绍排队理论，涵盖排队系统和排队系统网络的内容。

![](https://cdn.nlark.com/yuque/0/2025/png/45533403/1745390883259-0e97cb8b-f233-47f8-b7cd-c3964a1a16d3.png)



<h3 id="CRNgs">2.3 概念</h3>
<h4 id="BL9eX">2.3.1 延时</h4>
+ **延时的重要性**：在某些环境中，延时是唯一关注的性能焦点；在其他环境中，它是除吞吐量外重要的分析要点。
+ **定义与示例**：延时是操作执行前的等待时间，以网络服务传输数据请求为例，系统在操作前等待网络连接建立的时间就是延时。响应时间包含延时和操作时间 。延时可在不同点测量，如网站载入时间由DNS延时、TCP连接延时和TCP数据传输时间组成 。
+ **量化与比较**：延时是时间指标，有多种计算方法，可用于量化和评级性能问题。通过考量可减少或移除的延时，能计算预计加速。不同性能指标可转化为延时进行比较，尽管在复杂场景（涉及网络跳数、I/O特性等 ）中选择较难，但简单数值对比时差异明显。

![](https://cdn.nlark.com/yuque/0/2025/png/45533403/1745390967807-9b5cba59-da4d-4dcf-aa79-ee6cecec5ab3.png)

![](https://cdn.nlark.com/yuque/0/2025/png/45533403/1745390984846-1fc98963-f4fb-4bf6-b832-aa908f6f5559.png)

<h4 id="HmbUp">2.3.2 时间量级</h4>
+ **比较方法**：可用数字比较时间，也能用时间长短经验判断延时源头。
+ **量级差异示例**：系统各组件操作的时间量级差异极大，表2.2从3.3GHz的CPU寄存器访问延时开始，列举了不同事件的延时及相对时间比例，如1个CPU周期延时0.3ns ，旋转磁盘I/O延时1 - 10ms ，互联网不同地点连接延时从40ms到183ms等，体现了时间量级的巨大差别 。

![](https://cdn.nlark.com/yuque/0/2025/png/45533403/1745391001352-201a6d23-3469-42c0-b1c9-3afeda9b08c4.png)

<h4 id="I6gJc">2.3.3 权衡三角</h4>
+ **概念展示**：性能存在权衡三角关系，如“好/快/便宜”择其二 。许多IT项目选择及时和便宜，将性能问题留待后续解决，但早期若选择非最优存储架构或缺乏完善性能分析工具的编程语言、操作系统，可能阻碍性能提升 。
+ **常见权衡示例**：常见性能调整权衡存在于CPU与内存之间，在CPU资源充足的现代系统中，内存可缓存数据降低CPU使用，CPU也可压缩数据降低内存使用。还列举了文件系统记录尺寸和网络缓存尺寸的可调参数示例，小记录尺寸利于随机I/O工作负载，大记录尺寸利于流工作负载；小网络缓存尺寸减小内存开销利于系统扩展，大尺寸能提高网络吞吐量 。

![](https://cdn.nlark.com/yuque/0/2025/png/45533403/1745391155494-d0c4ab62-d892-45cd-9470-b24c9386392c.png)

<h4 id="jltiz">2.3.4 调整的影响</h4>
+ **调整效果层级差异**：性能调整在越靠近工作执行的地方效果越显著。对于工作负载驱动的应用程序，应用程序层级调整（如消去或减少数据库查询 ）可获大幅性能提升（如20倍 ）；存储设备层级调整虽能精简或提高存储I/O，但性能提升有限，多为百分比量级（如20% ） 。
+ **应用程序层级调整原因**：如今很多环境注重特性和功能快速部署，应用程序开发和测试关注正确性，留给性能测量和优化时间少，所以在应用程序层级寻求性能提升有必要 。不过应用程序层级不一定是观测效果最显著层级，数据库查询缓慢需从CPU时间、文件系统和磁盘I/O方面考察 。
+ **操作系统层级要点**：在快速部署、软件频繁变更的生产环境中，操作系统调整和从该层面观测问题易被忽视，但操作系统性能分析不仅能辨别本层级问题，还能发现应用程序层级问题，且在某些情况下比从应用程序视角分析更简单 。

![](https://cdn.nlark.com/yuque/0/2025/png/45533403/1745391179625-dd3029f8-2602-49e2-821e-8f7491dd8e39.png)

<h4 id="U9upp">2.3.5 合适的层级</h4>
不同公司和环境对性能分析需求不同，大型数据中心或云环境公司可能需性能工程师团队，利用内核数据、CPU性能计数器和动态跟踪技术，建立性能模型预测增长；小型创业公司可能仅做浅层次检查和第三方监控。像股票交易所等高交易电商，性能和延时至关重要，值得投入大量资源优化，如新建跨大西洋连接的纽约和伦敦交易所光缆，以减少传输延时 。 

<h4 id="vMlSk">2.3.6 性能建议的时间点</h4>
环境性能特性随时间变化，用户、软硬件升级等都是影响因素。如网络从1Gbps升级到10Gbps，可能出现磁盘或CPU性能瓶颈 。性能推荐参数仅在特定时间内有效，网络搜索得到的参数可能不适用于自身系统或工作负载，可能只是临时应急措施。若了解参数适用场景及历史调整情况，浏览性能推荐有助于判断自身系统和工作负载中参数是否需要调整 。 

<h4 id="ngrHT">2.3.7 负载 vs. 架构</h4>
![](https://cdn.nlark.com/yuque/0/2025/png/45533403/1745391334779-69fd0eab-2236-4383-9e35-ec58ef7cb248.png)

**性能问题分类**：应用程序性能差可能源于软件配置和硬件（架构问题 ），也可能是负载过多导致排队和延时。

**示例说明**：架构问题如单线程应用程序在单个CPU上忙碌，使请求排队，其他CPU空闲，性能受架构限制；负载问题如多线程程序使所有CPU忙碌但请求仍排队，性能可能受限于CPU性能，即负载超出CPU处理范围。

<h4 id="mHQjC">2.3.8 扩展性</h4>
+ **概念与曲线**：负载增加时系统展现的性能为扩展性，图2.6展示典型系统负载下吞吐量变化曲线。一定阶段内扩展性呈线性变化，负载增加到某点，资源争夺影响性能，该点为拐点，此后吞吐量偏离线性扩展，因资源竞争加剧，完成工作减少，吞吐量下降 。

![](https://cdn.nlark.com/yuque/0/2025/png/45533403/1745391437097-b8b2a938-a525-4eb0-8bc0-3689c19a507f.png)

+ **系统示例**：以执行大量计算的应用程序为例，更多负载由线程承担，CPU接近100%使用率时，调度延时增加，性能下降。性能峰值后，100%使用率下，吞吐量因多线程增加而下降，上下文切换消耗CPU资源，实际任务量减少 。资源替换后曲线类似，性能非线性变化可用平均响应时间或延时表示（如图2.7 ）。长响应时间不利，性能“快速”下降可能因内存负载（如系统开始换页 ），“慢速”下降可能因CPU负载 。还有磁盘I/O导致性能下降情况，负载增加，I/O排队，空闲磁盘I/O响应时间从1ms增至10ms（如M/D/1模型、60%使用率情况 ）。资源不可用，应用程序可能返回错误，而非排队任务 。

![](https://cdn.nlark.com/yuque/0/2025/png/45533403/1745391537475-a06b14be-8223-4f7e-bec5-6fbd34f9ba68.png)

<h4 id="yukp0">2.3.9 已知的未知</h4>
+ **概念解释**：性能领域存在已知的已知、已知的未知、未知的未知概念。已知的已知指知道且了解其当前值的事物，如知道应检查CPU使用率及当前均值为10% ；已知的未知指知道有可检查的指标或判断方法但未做的事，如知道可用profiling检查CPU忙碌原因但没做 ；未知的未知指不知道自己不知道的事物，如不知道设备中断会消耗大量CPU资源所以未检查 。
+ **知识拓展关系**：性能领域知识特点是知道越多，意识到的未知越多，未知的未知会转化为可查看的已知的未知 。

<h4 id="pZR08">2.3.10 指标</h4>
+ **指标定义与作用**：性能指标是系统、应用程序或其他工具产生的，用于度量感兴趣活动的统计数据，用于性能分析和监控，可由命令行提供数据或可视化工具提供图表 。
+ **常见指标列举**：常见系统性能指标有IOPS（每秒I/O操作数 ）、吞吐量（每秒数量或操作量 ）、使用率、延时 。吞吐量含义依上下文不同，数据库中常衡量每秒查询或请求数，网络中是每秒比特或字节数；IOPS度量吞吐量但只针对I/O操作 。
+ **指标开销与问题**：性能指标收集和保存需消耗CPU周期，存在开销，会对目标性能产生负面影响，即观察者效应。此外，软件商提供的指标可能存在混淆、复杂、不可靠、不精确甚至错误（因bug ）的情况，且软件版本未及时更新时，指标可能无法反映代码和路径 。

<h4 id="kJh1z">2.3.11 使用率</h4>
+ **基于时间的使用率**：常用于操作系统描述设备使用情况（如CPU和磁盘 ），基于时间的使用率用排队理论正式定义，公式为(U = B/T)，其中(U)是使用率，(B)是(T)时间内系统繁忙时间，(T)是观测周期 。以磁盘监控工具iostat调用的%b（即忙碌百分比 ）为例，能体现指标本质。使用率指标可评估组件是否成为系统瓶颈，部分组件在100%使用率下性能未必下降 。
+ **基于容量的使用率**：在容量规划中由IT专业人员使用，指系统或组件能提供一定数量的吞吐，不论处于何种级别，都在其容量（如某一比例 ）上使用 。基于容量的使用率与基于时间的使用率不同，100%容量使用率表示磁盘无法承受更多工作，而100%时间使用率不意味着100%容量使用 。
+ **非空闲时间（新术语）**：开发云监控项目时定义的术语，用于更精确表述使用率相关概念，但目前使用尚不普遍 。

<h4 id="vsLaV">2.3.12 饱和度</h4>
+ **定义**：饱和度指随着工作量增加，对资源的请求超过资源所能处理的程度 。当使用率达到100%（基于容量 ）时，多出工作无法处理开始排队，此时出现饱和度 。
+ **图示与特性**：图2.8展示了使用率与饱和度的关系，随着负载上升，饱和度在超过100%使用率标记后线性增长 。任何程度的饱和度都会因等待（产生延时 ）形成性能问题。对于基于时间的使用率（忙碌百分比 ），排队和饱和度不一定在100%使用率时出现，取决于资源处理任务的并行能力 。

![](https://cdn.nlark.com/yuque/0/2025/png/45533403/1745391793648-07657689-0524-4268-8fea-644520eab5d1.png)

<h4 id="OYyrR">2.3.13 剖析</h4>
+ **概念**：剖析（profiling）原指对目标对象绘图研究理解，在计算机性能领域，是按特定时间间隔对系统状态采样并研究样本 。
+ **与指标对比**：和IOPS、吞吐量等指标不同，剖析采样对系统活动的观察相对粗糙，观察精细程度取决于采样率 。
+ **示例**：以对CPU程序计数器采样为例，按一定频率间隔进行栈回溯跟踪，收集消耗CPU资源的代码路径统计数据，以此了解CPU使用情况，该专题在第6章会有更多介绍 。

<h4 id="ePibX">2.3.14 缓存</h4>
**缓存提升性能原理**

缓存通过将较慢存储层的结果存于较快存储层来提高性能，如把磁盘块缓存在主内存（RAM ）中 。CPU常利用多层硬件（L1、L2、L3 ）作为主缓存，L1快但小，L2和L3缓存容量增加、访问延时增大，缓存级数和大小依CPU芯片空间选择以优化性能 。系统中还有许多利用主内存的软件缓存，第3章3.2.11节有系统缓存列表 。

![](https://cdn.nlark.com/yuque/0/2025/png/45533403/1745392013132-87223383-d25c-4a87-96a5-74ff8b49931d.png)

**缓存性能指标**

+ **命中率**：衡量缓存性能的重要指标，指所需数据在缓存中找到的次数（命中 ）与未找到次数（失效 ）之比，命中率 = 命中次数/(命中次数 + 失效次数) 。命中率越高，从较快介质访问数据越多，性能提升越明显，图2.9展示了缓存命中率与性能的非线性关系，存储层级速度差异越大，曲线越陡 。
+ **失效率**：指每秒钟缓存失效的次数，与每次缓存失效对性能的影响成线性关系。通过工作负载A（命中率90%，失效率200/s ）和B（命中率80%，失效率20/s ）的例子，说明结合命中率和失效率可计算运行时间（运行时间 = (命中率×命中延时)+(失效率×失效延时) ，假定工作串行 ），综合判断工作负载执行情况 。

**缓存算法**

缓存管理算法和策略决定有限缓存空间存放的数据 。“最近最常使用算法”（MRU）保留最近使用次数最多的数据；“最近最少使用算法”（LRU）在需更多空间时移除最少使用数据；还有“最常使用算法”（MFU）、“最不常使用算法”（LFU） 。“不常使用算法”（NFU）是LRU花费不高但吞吐量稍小的版本 。 

**缓存状态**

+ **冷**：缓存为空或填充无用数据，命中率为0或接近0 。
+ **热**：缓存填充常用数据，命中率高（如超99% ）。 
+ **温**：缓存填充有用数据，但命中率未达预期高度 。
+ **热度**：指缓存热度或冷度，提高热度即提高命中率 。缓存初始化后先冷，随时间变暖，缓存大或下一级存储慢时，变暖需较长时间，如某存储服务器不同缓存从冷到温、暖所需时间不同 。这部分内容介绍了性能分析的两个常用视角：工作负载分析和资源分析，具体如下：

<h3 id="zX8jH">2.4 视角</h3>
性能分析常用视角为工作负载分析和资源分析，可分别理解为对系统软件栈自上而下和自底向上的分析 ，2.5节会论述实施策略及详细内容。

![](https://cdn.nlark.com/yuque/0/2025/png/45533403/1745393518440-71f5c259-b78f-48b6-b7f8-4468b4e09986.png)

<h4 id="SxyJG">2.4.1 资源分析</h4>
+ **执行人员与目的**：通常由系统管理员执行，以分析系统资源（CPU、内存、磁盘、网卡、总线及互联 ）为起点，进行性能问题研究（判断是否特定资源导致问题 ）和容量规划（为新系统设计提供信息或预测资源耗尽时间 ） 。
+ **分析重点与指标**：着重使用率分析，判断资源是否达极限。部分资源（如CPU ）有现成使用率指标，其他资源可通过相关指标计算，如网卡使用率可通过吞吐量与最大带宽比较估算 。适合资源分析的指标有IOPS、吞吐量、使用率、饱和度 ，这些指标衡量资源工作情况及使用程度，延时等指标也会用于衡量资源对工作负载的响应 。
+ **工具文档**：资源分析是常用手段，有大量相关文档，如针对操作系统的“统计”工具vmstat(1)、iostat(1)、mpstat(1) ，但要知道这只是一种视角 。

<h4 id="zfjSZ">2.4.2 工作负载分析</h4>
![](https://cdn.nlark.com/yuque/0/2025/png/45533403/1745393548065-54ee53c7-7c6f-49df-a6ba-d753fd13ae8d.png)

**执行人员与对象**：由应用开发人员和技术支持人员执行，检查应用程序性能，分析对象包括请求（施加的工作负载 ）、延时（应用程序响应时间 ）、完成度（查找错误 ） 。

**分析任务与指标**：任务包括辨别并确认问题（如查找超阈值延时定位原因 ），分析起始点是应用程序，研究延时需深入应用程序、程序库乃至操作系统（内核 ） 。适合的指标有吞吐量（每秒业务处理量 ）、延时 ，分别衡量请求量大小和系统性能 。研究工作负载请求需检查并归纳负载属性（如客户端机器、数据库名等 ），有助于识别不必要工作或不均衡工作，找到减少或消除负载的方法 。应用程序性能体现中，延时是重要指标，不同场景下含义有别（如MySQL数据库中查询延时 ） 。系统错误（如错误码 ）会导致请求重试增加延时 。

<h3 id="ivDAN">2.5 方法</h3>
本节讲述针对系统性能分析和调整的方法步骤，涵盖新方法（如USE方法 ）及一些反方法（anti - methodologies ）。为便于总结，这些方法被归类为观测分析、实验分析、假设分析等不同类型 ，具体见表2.4。

+ **观测分析类**：包括街灯散步方法、问题陈述法、科学法、诊断循环、工具法、工作负载特征归纳、向下挖掘分析、延时分析、R方法、事件跟踪、基础线统计、性能监控、静态性能调整、缓存调优、微基准测试等。这些方法用于观察系统性能相关情况，收集信息、分析问题、进行容量规划等 。
+ **实验分析类**：如随机变动方法，通过对系统进行变动实验来分析性能 。
+ **假设分析类**：例如责怪他人讹方法，通过假设和分析来处理性能问题 。
+ **观测与实验结合类**：像Ad Hoc核对清单法 。
+ **统计分析与容量规划类**：排队论用于统计分析和容量规划；容量规划方法专门用于规划系统容量 。

性能监测、排队理论及容量规划在后续部分有更详细阐述，后续章节会介绍这些方法在不同环境中的应用，特殊性能分析领域会使用特定方法 ，并建议在使用其他方法前优先尝试问题陈述法。

**<u><font style="background-color:#FBDE28;">反方法</font></u>**

+ **街灯讹方法**：用户选用熟悉的观测工具随意分析性能，像警察在街灯下帮醉汉找钥匙（只因灯光亮 ），可能命中问题也可能忽视问题。此方法能以试错方式摸索，熟悉可调参数，但当工具和调整与问题不相关时进展缓慢，适用于初步探索，可快速排除“误报” 。
+ **随机变动讹方法**：属于实验性讹方法，用户随机猜测问题位置并改动，通过选择指标（如应用程序运行时间等 ）判断性能提升。步骤为任选项目改动、朝某方向修改、测量性能、换方向修改、再测量，若结果好于基准值则保留修改并重复。该方法耗时，调整可能仅适用于被测负载且难以长期有效，生产负载高峰还可能引发更严重问题，需准备回退方案 。
+ **责怪他人讹方法**：步骤为找到非自身负责的系统组件、假定问题与之相关、将问题扔给负责团队，若证明错了就返回重新找。此方法缺乏数据支撑，是无端臆想，会浪费其他团队资源，使用时需谨慎，附上工具运行和输出情况截图及向他人征求意见 。
+ **Ad Hoc核对清单法**：技术支持人员检查调试系统时使用，基于经验和以往问题创建核对清单，如运行iostat -x 1检查磁盘负载 。清单需频繁更新以反映当前状态，主要处理易修复问题（如设置可调参数 ），能在短时间内提供价值，确保团队知道如何检查和修复问题 。

**<u><font style="background-color:#FBDE28;">性能分析方法</font></u>**

+ **问题陈述法**：支持人员询问客户相关问题来明确性能问题，包括存在什么性能问题、系统之前运行状况、近期软硬件或负载改动、问题能否用延时或运行时间表述、问题影响范围、环境及所用软件版本和配置 。该方法能指向问题根源和解决方案，遇到新问题应优先使用 。
+ **科学法**：通过假设和试验研究未知问题，步骤为陈述问题、做出假设、进行预测、开展试验、分析数据 。可采用观测性或实验性试验，观测性试验如对比不同系统缓存失效率，实验性试验如改变缓存大小测试性能 。若问题未解决，可重新假设并试验 。
+ **诊断循环**：与科学法相似，流程为假设→仪器检验→数据→假设 ，通过收集数据验证假设，循环强调用数据快速引发假设、验证和改良，如同医生看病，根据每次检验结果修正假设，使不好的理论尽早被识别和摒弃 。
+ **工具法**：列出可用性能工具（包括可选、已安装或可购买的 ），针对每个工具列出提供的指标，针对每个指标列出阐释指标的规则，能帮助了解可用工具、可读指标及指标阐释，但仅依赖工具可能无法全面审视系统，需与街灯讹方法结合使用 。
+ **USE方法**：用于性能研究识别系统瓶颈，核心是查看所有资源的使用率、饱和度和错误 。资源包括服务器物理元件及部分软件资源；使用率指资源服务工作的时间百分比；饱和度指资源不能再服务额外工作的程度；错误指资源产生的错误 。该方法能引导分析关键指标，排查系统资源问题，若未找到问题再考虑其他方法 。

![](https://cdn.nlark.com/yuque/0/2025/png/45533403/1745394013843-839d069b-5e8d-4f22-ae8e-20a43f4afb97.png)

**指标表述**

+ **使用率**：一定时间间隔内资源用于服务工作的时间百分比，如“单个CPU运行在90%的使用率上” 。需注意监控工具汇报的均值可能掩盖短时间高使用率情况，如CPU使用率瞬间可剧烈变动 。
+ **饱和度**：用等待队列长度衡量，如“CPU的平均运行队列长度是4” ，表示资源无法立即处理任务，请求需排队 。
+ **错误**：指资源报告出的错误数目，如“这个网络接口发生了50次滞后冲突” ，错误会损害性能，需调查 。



**资源列表**

USE方法第一步是建立完整资源列表，常见服务器资源包括CPU（插槽、核、硬件线程 ）、内存（DRAM ）、网络接口（以太端口 ）、存储设备（磁盘 ）、控制器（存储、网络 ）、互联（CPU、内存、I/O ） 。有些组件兼具多种资源类型，如存储设备既是I/O资源也是容量资源 。制定列表时要考虑所有可能成为性能瓶颈的组件，I/O资源可当作排队系统研究 。硬件缓存等物理资源可能不在清单中，USE方法对处理高使用率或饱和状态下性能下降的资源有效，不确定资源是否列入清单时可先包含，查看实际指标 。 

**原理框图**

通过找到或绘制系统原理框图，可显示组件关系，有助于寻找数据流中的瓶颈，如CPU、内存、I/O互联和总线常被忽视，但一般不是常见瓶颈，必要时可升级主板或减小负载 。 

![](https://cdn.nlark.com/yuque/0/2025/png/45533403/1745394283066-5a8efc0b-f26e-44ea-8dce-35d91288b6d4.png)

**指标示例**

表2.5列举了一般性操作系统下，不同资源（CPU、内存、网络接口、存储设备I/O等 ）对应的使用率、饱和度、错误指标，如CPU使用率为单CPU使用率或系统级均值，存储设备I/O饱和度为等待队列长度等 。表2.6给出一些进阶指标示例，如CPU错误包括可修正的缓存ECC事件等，网络饱和度涉及相关网络接口或操作系统错误 。部分指标需用动态跟踪或CPU性能计数器获取 。附录A、B分别提供Linux和Solaris系统的USE方法核对清单，涵盖硬件资源和观测工具组合，还含软件资源 。 

![](https://cdn.nlark.com/yuque/0/2025/png/45533403/1745394318876-3d4cb6e2-253b-4269-af68-939fca7870e7.png)![](https://cdn.nlark.com/yuque/0/2025/png/45533403/1745394326173-1756b6db-eb93-4fd0-8ebd-bef1f49b9c1d.png)

![](https://cdn.nlark.com/yuque/0/2025/png/45533403/1745394344666-0ed96d07-33ab-4ef4-af07-b0e9e71c97ac.png)

**软件资源**

一些软件组件（如互斥锁、线程池、进程/线程容量、文件描述符容量 ）也可按USE方法检测，使用率指资源使用时间，饱和度指排队情况，错误指分配失败等情况 ，指标适用则采用，否则用其他方法 。 

**使用建议**

+ **使用率**：使用率超100%通常是瓶颈信号，需检查确认。时间间隔均值可能掩盖短期100%使用率爆发，且硬盘等资源操作期间不能中断，即使使用率超60%仍可工作，但随使用率上升，排队延时更频繁明显 。
+ **饱和度**：任何程度的饱和度（非零 ）都是问题，可用排队长度或时间衡量 。
+ **错误**：错误值得研究，尤其增加会致性能变差的错误 。低使用率、无饱和、无错误的反例研究有助于缩小范围、快速定位问题，判断非某一资源问题 。

云计算

云计算环境中，软件资源通过OS虚拟技术（如SmartOS Zones ）限制或为多租户设定阈值，内存、CPU、存储I/O的限定都可用USE方法检验，类似检查物理资源 ，如“内存容量使用率”为租户内存使用率与容量比值，“内存容量饱和”与匿名分页活动相关 。这部分内容介绍了工作负载特征归纳和向下挖掘分析两种性能分析方法，具体如下：

工作负载特征归纳

+ **方法作用**：是辨别由施加负载导致问题的简单高效方法，关注系统输入，即便系统架构和配置无问题，负载超出承受范围也会引发问题 。
+ **特征归纳方式**：通过回答负载产生者（进程ID、用户ID、远端IP地址 ）、被调用原因（代码路径、堆栈跟踪 ）、特征（IOPS、吞吐量、方向类型、变动情况 ）、随时间变化情况（是否有日常模式 ）等问题进行 。
+ **应用场景**：能识别因负载产生的问题，如数据库性能问题可能源于异常负载（如遭受DoS攻击 ），或因应用程序异常运行、错误配置产生不必要工作 。归纳特征后可维护和重新配置解决问题，也可用系统资源控制限制不必要工作 。还可作为仿真基准设计输入，除均值外，收集分布和变化细节对仿真负载多样性很重要 。工作负载分析能区分负载和架构问题，执行该方法所用工具和指标依目标而定，部分应用程序记录的客户活动信息及用户使用报告可作为分析数据来源 。

**向下挖掘分析**

+ **分析流程**：从高级别检查开始，依据发现缩小关注范围，深入挖掘相关部分，可能探究到软件栈深层甚至硬件层找问题根源 。
+ **分析阶段**：在《Solaris性能与工具》中，针对系统性能的深度分析方法分三个阶段，一是监测，持续记录高层统计数据，发现问题辨别报警；二是识别，对给定问题缩小范围找瓶颈；三是分析，确定系统部分做进一步检查，发现根源并量化问题 。
+ **监测方法**：在公司范围执行，聚合服务器和云数据，传统用SNMP监控支持协议的网络设备，数据展示长期变化，通过短时间内运行命令行工具获取数据，发现怀疑问题多数监测方案会报警并进入下一阶段 。
+ **问题识别**：在服务器上交互执行，用标准观测工具（如vmstat(1)、iostat(1)、mpstat(1) ）检查系统组件，也有新工具通过GUI支持实时性能分析 。
+ **深入分析**：部分分析工具具备tracing或profiling功能，对可疑区域做更深层次检查，按需分离软件栈层次找问题根源，执行工具包括strace(1)、truss(1)、perf和DTrace 。
+ **五个Why技巧**：分析阶段可用“五个Why”技巧，通过连续问“为什么”深入探究问题实质，如数据库性能变差案例，经多次追问找到内存分配器存在碎片问题这一根源 。这部分内容介绍了延时分析、R方法、事件跟踪三种性能分析方法，具体如下：

**延时分析**

+ **分析原理**：检查操作完成时间，将时间细分，对最大延时时间段再次划分，定位并量化问题根源，会深入软件栈各层找延时原因 。
+ **分析示例**：以MySQL请求延时分析为例，通过依次询问是否存在请求延时、请求时间是否大量花在CPU上、不在CPU上的时间在等待什么等问题，将延时逐步细分，采用类似二分搜索法，对较大可能部分进一步分析，直至找到问题根源 。

![](https://cdn.nlark.com/yuque/0/2025/png/45533403/1745395167743-e7c99b27-dfc1-4b33-ba61-b802632d0c83.png)

**R方法**

是针对Oracle数据库开发的性能分析方法，基于Oracle的trace events ，旨在找到延时根源，被描述为“基于时间的响应性能提升方法，可得到对业务的最大经济收益” ，着重识别和量化查询过程消耗时间，虽用于数据库研究领域，但方法思想可用于所有系统作为研究手段 。 

**事件跟踪**

+ **事件范畴**：系统操作由离散事件构成，如CPU指令、磁盘I/O、磁盘命令、网络包、系统调用、函数库调用、应用程序事件、数据库查询等 。性能分析常研究这些事件汇总数据（如每秒操作数、字节数、延时均值 ），但重要细节信息可能被忽略，需逐个检查事件 。
+ **工具及信息获取**：网络排错用tcpdump(1) ，可按需输出各类信息；存储设备层用iosnoop(1M)（基于DTrace ）跟踪；系统调用跟踪用Linux的strace(1)和Solaris的truss(1) ，这些工具可打印时间戳 。执行事件跟踪需获取输入（事件请求属性 ）、时间（起始、终止、延时 ）、结果（错误状态、事件结果 ）等信息 。事件时间戳利于延时分析，研究之前发生的事件也能提供信息，可辨别如延时离群值等特殊情况是由之前事件导致 。

**基础线统计**

+ **对比分析作用**：将当前性能指标与之前数值比较，有助于分析问题，能观察负载和资源使用变化，追溯问题发生时间 。一些基于内核计数器的观测工具可显示启动以来信息统计，虽比较粗略但有帮助 。
+ **基础线统计方式**：进行大范围系统观测并保存数据作参考，与启动以来信息统计不同，基础线统计涵盖每秒数据，能体现变化 。在系统或应用程序变化前后都可进行，可不定期执行并记录，作为性能监测部分也可按固定间隔执行 。

**静态性能调整**

+ **调整时机与内容**：处理架构配置问题，在系统空闲未施加负载时执行，与关注负载施加后性能的动态性能分析不同 。需对系统组件逐一确认是否必要、配置是否针对预期工作负载、自动配置是否最优、有无错误或处于降级状态 。
+ **常见问题示例**：可能发现的问题包括网络接口协商速率低、建立RAID池失败、操作系统和应用程序版本旧、文件系统记录尺寸与工作负载I/O尺寸不一致、服务器意外配置路由、使用远端数据中心资源而非本地等 。

**缓存调优**

+ **多层缓存设置**：从应用程序到磁盘，应用程序和操作系统部署多层缓存提高I/O性能，具体内容见第3章3.2.11节 。
+ **整体调优策略**：包括缓存大小尽量靠近工作执行地方，减少命中缓存资源开销；确认缓存开启且正常工作；确认缓存命中/失效比例和失效率；缓存大小动态时确认当前尺寸；针对工作负载调整缓存及可调参数；针对缓存调整负载，减少不必要消耗 。调优时要小心避免二次缓存，考虑每层缓存调优的整体性能收益，如调整CPU的L1、L3缓存可获得性能提升 。

**微基准测试**

+ **测试用途**：测量施加简单人造工作负载时的性能，可用于支持科学法（验证假设和预测 ），也是容量规划一部分 。与对真实自然工作负载的基准测试不同，微基准测试执行和理解相对简单 。
+ **测试方法**：因涉及因素少，执行和理解较简单，可用微基准测试工具施加工作负载并测量性能，也可用负载生成器产生负载，用标准系统工具测量，还可结合两者确认性能数据 。
+ **测试示例**：如针对系统调用（fork()、exec()等 ）、文件系统读取（从1B到1MB变化 ）、网络吞吐量（不同socket缓冲区尺寸测试TCP端到端传输 ）等进行测试 。微基准测试在目标系统执行，测量大量操作时间并计算均值（平均时间 = 运行时间/操作次数 ） 。这部分内容围绕建模展开，介绍了建模在性能分析中的用途、企业与云环境下的建模特点、可视化识别性能曲线类型等内容，具体如下：

<h3 id="mQmtu">2.6 建模</h3>
+ **性能分析手段**：建立系统分析模型对可扩展性分析很有用，能研究负载或资源扩展时性能的变化，资源可以是硬件（如CPU核 ）或软件（如进程、线程 ） 。除测量和实验性测试外，分析建模是第三种性能评估方法，分析建模与仿真、测量相结合，能让性能研究更透彻 。
+ **分析流程**：对现有系统分析可从测量开始，归纳负载特征和测量性能；无生产环境负载或工作负载不可见时，可用工作负载仿真测试；分析建模基于测试和仿真结果进行性能预测 。
+ **可扩展性分析意义**：可扩展性分析能找出性能因资源限制停止线性增长的拐点，帮助在遇到性能问题前修复，相关内容可参考2.5.10节和2.5.18节 。

<h4 id="JITUq">2.6.1 企业 vs. 云</h4>
+ **大型环境建模难点**：建模可对大型企业系统仿真，但大型环境性能复杂，难以精确建模 。
+ **云计算优势**：利用云计算技术，环境可短期租用用于基准测试，无需建立模型预测性能，可在不同规模云上进行工作负载特征归纳、仿真和测试 。虽一些发现（如拐点 ）相似，但基于真实环境测试能发现模型未涵盖的性能制约点 。

<h4 id="JNHsq">2.6.2 可视化识别</h4>
+ **数据可视化作用**：实验收集足够数据后，绘制成性能随规模变化曲线可揭示规律 。
+ **示例分析**：图2.15展示某应用程序随线程数增加的吞吐量变化，8个线程处疑似存在拐点，可研究该点附近应用程序和系统配置信息 。图2.16展示一系列性能扩展曲线类型，包括线性扩展（性能随资源扩展成比例增加，可能是其他扩展早期阶段 ）、竞争（架构共享组件串行使用，竞争减少扩展效益 ）、一致性（维持数据一致性代价超扩展好处 ）、拐点（某因素达扩展制约点改变扩展曲线 ）、扩展上限（达到硬件极限如设备瓶颈或软件设置限制 ） 。可视化识别曲线简单有效，数学模型能更深入了解系统扩展性，后续会介绍Amdahl定律和排队理论 。

![](https://cdn.nlark.com/yuque/0/2025/png/45533403/1745395449568-995901bd-d389-448d-83a1-9119bbf3ccd7.png)![](https://cdn.nlark.com/yuque/0/2025/png/45533403/1745396267178-c8eb2969-9393-4bf5-b984-41b05d4b284a.png)

<h4 id="dkGlk">2.6.3 Amdahl扩展定律</h4>
1. **定义与命名**：由计算机架构师Gene Amdahl命名，对系统扩展性建模，针对串行构成、不能并行执行的工作负载，可用于CPU、线程、工作负载等扩展性研究 。
2. **公式及参数含义**：![image](https://cdn.nlark.com/yuque/__latex/b165178ec18f0e2baf007c31bbc017e3.svg) 。其中![image](https://cdn.nlark.com/yuque/__latex/98322a142e04c886a2bfe63e89167e74.svg)是容量，![image](https://cdn.nlark.com/yuque/__latex/459f3c80a50b7be28751b0869ef5386a.svg)是扩展维度（如CPU数目、用户负载 ），![image](https://cdn.nlark.com/yuque/__latex/18d25ca4f77a9bbed9812e2bb0b350a5.svg)（![image](https://cdn.nlark.com/yuque/__latex/0d722db83b85f76915c9d24814136a6d.svg) ）代表串行程度，即偏离线性扩展程度。
3. **应用步骤**
    - 收集![image](https://cdn.nlark.com/yuque/__latex/459f3c80a50b7be28751b0869ef5386a.svg)范围内数据，可通过观测现有系统、微基准测试、负载生成器等方式。
    - 用统计软件（如gnuplot或R ）执行回归分析，判断Amdahl系数![image](https://cdn.nlark.com/yuque/__latex/18d25ca4f77a9bbed9812e2bb0b350a5.svg)的值。 
    - 用gnuplot或R将收集数据点与预测扩展性的模型函数绘图，分析数据与模型差异。
4. **示例**：给出R语言实现Amdahl扩展定律回归分析代码示例（具体代码略）。

<h4 id="ZOp5J">2.6.4 通用扩展定律</h4>
1. **定义与命名**：即Universal Scalability Law（USL），之前称超串行模型，由Neil Gunther开发，引入系数处理一致性延时，用于描述一致性扩展曲线，考虑竞争影响 。
2. **公式及参数含义**：![image](https://cdn.nlark.com/yuque/__latex/4c1f29c4cde63d25f44dc5e725d45fcc.svg) 。![image](https://cdn.nlark.com/yuque/__latex/98322a142e04c886a2bfe63e89167e74.svg)、![image](https://cdn.nlark.com/yuque/__latex/459f3c80a50b7be28751b0869ef5386a.svg)、![image](https://cdn.nlark.com/yuque/__latex/18d25ca4f77a9bbed9812e2bb0b350a5.svg)与Amdahl扩展定律含义相关，![image](https://cdn.nlark.com/yuque/__latex/6100158802e722a88c15efc101fc275b.svg)是一致性系数，![image](https://cdn.nlark.com/yuque/__latex/04277e3f043e754cdcb67352b7971871.svg)时，定律变为Amdahl扩展定律。
3. **特点与应用**：输入数据量大会给扩展曲线形状判断带来困难。模型起始一组十个数据点用圆圈标记，额外一组十个数据点用叉形标记，可检验模型预测能力。更多分析参考[Gunther 97]和[Gunther 07]。

<h4 id="C8hMn">2.6.5 排队理论</h4>
1. **定义与应用**
    - 用数学方法研究带队列系统，可分析队列长度、等待时间（延时）和使用率（基于时间）。在计算领域，硬件和软件组件常建模为队列系统，多条队列系统模型称队列网络。
    - 建立在概率、随机过程等数学统计领域之上，包含Erlang的C公式（Ager Krarup Erlang创立）和Little's定律（![image](https://cdn.nlark.com/yuque/__latex/14a807e03d1714feb7c391773e194111.svg) ，系统请求平均数![image](https://cdn.nlark.com/yuque/__latex/c895173d3be4872abf206be4268a58cb.svg)由平均到达率![image](https://cdn.nlark.com/yuque/__latex/e520c061a407db472027709bf3f73290.svg)乘以平均服务时间![image](https://cdn.nlark.com/yuque/__latex/a36915ecf0b5605493f5aeaf1480a9ac.svg)得到 ）。
    - 可用于回答如负载增加一倍平均响应时间变化、增加处理器对平均响应时间影响、负载增加一倍时系统90%响应时间能否在100ms以下等问题，还研究使用率、队列长度、任务数目等因素。

![](https://cdn.nlark.com/yuque/0/2025/png/45533403/1745396495310-d7cc388f-e2fe-44e8-bd95-5f59585ad6cb.png)

2. **排队系统要素与Kendall标记法**
    - **要素**：到达过程（请求到达系统间隔时间，随机或固定，如泊松过程 ）、服务时间分布（服务中心服务时间分布类型，如确定性、指数型 ）、服务中心数目。
    - **Kendall标记法**：格式为![image](https://cdn.nlark.com/yuque/__latex/3983069b292b4f111b48e49b6de6239a.svg) ，![image](https://cdn.nlark.com/yuque/__latex/de951302f41d4707b9d80ca1af34dd0f.svg)为到达过程，![image](https://cdn.nlark.com/yuque/__latex/55fc237afbe535f7d8434985b848a6a7.svg)为服务时间分布，![image](https://cdn.nlark.com/yuque/__latex/4760e2f007e23d820825ba241c47ce3b.svg)为服务中心数目，还有扩展格式涵盖更多要素。
    - **常见排队系统**：M/M/1（马尔可夫到达、马尔可夫服务时间、一个服务中心 ）；M/M/c（马尔可夫到达、马尔可夫服务时间、多个服务中心 ）；M/G/1（马尔可夫到达、一般服务时间分布、一个服务中心 ）；M/D/1（马尔可夫到达、确定性服务时间、一个服务中心 ，常用于研究旋转物理硬盘性能 ） 。

![](https://cdn.nlark.com/yuque/0/2025/png/45533403/1745396509002-4470245c-9cd1-4b16-a8cb-1a70e1aeb28e.png)

3. **M/D/1和60%使用率示例**
    - 假定磁盘响应工作负载时间固定（M/D/1模型 ），研究使用率增加时磁盘响应时间变化。![image](https://cdn.nlark.com/yuque/__latex/57c7ac8efc43abe4abd1975551e061ec.svg) ，![image](https://cdn.nlark.com/yuque/__latex/72cb3a229067770aeb6caa625a65a1a1.svg)为响应时间，由服务时间![image](https://cdn.nlark.com/yuque/__latex/79ce3c7a71877c2ff01695e38ade43ca.svg)和使用率![image](https://cdn.nlark.com/yuque/__latex/d4cd21d60552e207f237e82def9029b6.svg)决定 。
    - 1ms服务时间下，使用率60%时平均响应时间变为两倍，80%时变为三倍。磁盘I/O延时通常是应用程序瓶颈，因需增加应用程序带宽，CPU资源与之不同，高优先级任务可抢占CPU 。
    - 用R统计软件绘制平均响应时间与使用率关系图，代码中大部分是在定义绘图的边缘、线条等属性并传到`plot()`函数。

![](https://cdn.nlark.com/yuque/0/2025/png/45533403/1745396528731-a7a58e43-b692-4c7b-82bd-5ee4698db07b.png)

<h3 id="suv0o">2.7 容量规划</h3>
容量规划用于检查系统处理负载情况及随负载增加的扩展能力。方法包括研究资源极限和因素分析，扩展解决方案有负载均衡器和分片。更多内容见《The Art of Capacity Planning》[Allspaw 08] 。针对应用程序做容量规划，有助于制定量化性能目标，相关内容见第5章前半部分。

<h4 id="GyKx5">2.7.1 资源极限</h4>
通过研究负载下成为系统瓶颈的资源来进行容量规划，步骤如下：

1. 测量服务器请求频率并监视其随时间变化。
2. 测量硬、软件使用情况并监视使用率随时间变化。
3. 用资源使用表示服务器请求情况。
4. 根据资源推断服务器请求极限（或实验确定）。开始需识别服务器及服务的请求种类，如Web服务器服务HTTP请求等。判断请求消耗的系统资源，对现有系统，测量当前请求率，推断先达100%使用率的资源及对应请求率；对未来系统，用微基准测试或负载生成工具模拟请求并测量资源使用。  
需监视的资源：
+ **硬件**：CPU使用率、内存使用、磁盘IOPS、磁盘吞吐量、磁盘容量（使用率）、网络吞吐量。
+ **软件**：虚拟内存使用、进程/任务/线程、文件描述符。   
举例：系统当前每秒执行1000个请求，16个CPU平均使用率40%，预测CPU达100%时成为瓶颈。计算得出每个请求消耗0.64% CPU，每秒请求最大值为2500个。要判断此性能是否足够，需了解工作负载峰值，对既有系统监视一段时间可了解峰值情况。

![](https://cdn.nlark.com/yuque/0/2025/png/45533403/1745396651428-a19ec9bb-ae49-4251-8b3f-4c465d398d2a.png)

<h4 id="uW7Ry">2.7.2 因素分析</h4>
购买和部署新系统时，需调整磁盘、CPU数量等因素实现理想性能，以最小成本达所需性能。对因素组合全测试会失控，解决方法如下：

1. 测试所有因素设为最大时的性能。
2. 逐一改变因素并测试性能（因素改变会致性能下降）。
3. 统计因素变化引起性能下降百分比及节省成本。
4. 选最高性能（和成本）为起点，选能节省成本且性能下降仍满足需求的因素组合。 
5. 重新测试配置，确认性能。   
举例：新存储系统需特定性能指标，不同组件改变对性能有不同影响，考虑性能下降和成本节省，预估双处理器和单网卡配置能满足吞吐量需求，且测试该配置是明智之举。

<h4 id="gtLMV">2.7.3 系统扩展策略</h4>
1. **垂直扩展（vertical scaling）**：为满足更高性能需求，建立更大系统的策略。
2. **水平扩展（horizontal scaling）**：将负载分散到多个系统，并在这些系统前放置负载均衡器（load balancer），使其看起来像一个系统 。
3. **云计算扩展**：在许多较小的虚拟化系统之上构建，比水平扩展更进一步。用户购买云计算处理所需负载时，能提供更优颗粒度，支持系统小规模且有效的扩展，与企业大型机相比，无需一开始就进行大规模申购，项目早期也无需严格容量规划。 
4. **数据库分片（sharding）**：常见的数据库扩展策略，将数据分成一个个逻辑组件，由各自数据库（或冗余数据库组）管理。例如用户数据库可按顾客名字字母范围划分。 扩展设计取决于需处理的负载和选用的应用程序，更多内容可参考《Scalable Internet Architectures》[Schlossnagle 06]。



<h3 id="W3ijV">2.8 统计</h3>
理解统计的使用及局限性十分重要。本章通过统计（指标）量化性能问题，涉及统计类型有平均值、标准差、百分位数等。

<h4 id="FfPHd">2.8.1 量化性能</h4>
+ **基于观测**：通过选择可靠指标、估计解决问题带来的性能收益来量化性能问题。如观测到应用程序请求需10ms，其中9ms为磁盘I/O ，若将I/O缓存到内存（预期DRAM延时10μs ），估计性能收益约9倍。测量延时需确保作为应用程序请求同步组件计时，异步事件（如后台磁盘I/O ）不直接影响应用性能。
+ **基于实验**：实施修复，用可靠指标做前后对比。如观测应用程序事务平均延时10ms ，增加线程数后观测到平均延时2ms ，性能增益5倍。但生产环境实验代价高时不适用。

<h4 id="dtOA0">2.8.2 平均值</h4>
+ **算术平均值**：数据总值除以个数，是最常见平均值类型。
+ **几何平均值**：数值乘积的n次方根（n为数值个数），适用于网络性能分析等场景，各层性能提升有“相乘”效果时适用。
+ **调和平均值**：数值个数除以所有数值倒数之和，适用于利用速率求均值，如计算数据传输平均速率。
+ **随时间变化的平均值**：性能度量中很多指标是随时间变化的平均值，涉及平均值时需确认时间间隔长度，如监测工具显示的CPU使用率平均值可能掩盖短时间内的高负载情况。
+ **衰退均值**：在系统性能中使用，最近时间权重高于之前时间，减小短期波动对平均值的影响 。

<h4 id="K6w1F">2.8.3 标准差、百分位数、中位数</h4>
+ **标准差**：衡量数据离散程度，数值越大表示数据偏离均值程度越大。
+ **百分位数**：如第99百分位数表示分布上包含99%数值的点。第99、90、95和99.9百分位数可在请求延时性能监测及服务水平协议（SLA）中使用，量化请求分布的最慢部分，衡量大多数用户可接受性能。 
+ **中位数**：即第50百分位数，显示数据大部分所在位置。

<h4 id="GoiBe">2.8.4 变异系数</h4>
标准差相对平均值的比例称为变异系数（CV），用于表示数据变异程度。单独标准差提供信息有限，结合平均值更有意义，较小CV意味着数据变异小。

<h4 id="A20F5">2.8.5 多重模态分布</h4>
平均值、标准差、百分位数等常针对正态分布或单模态分布。但系统性能可能出现双模态等情况，如代码路径快慢不同、缓存命中与否导致延时呈现多模态。以磁盘I/O延时为例，读写混合负载下，直方图显示缓存命中时延时小于1ms，缓存失效时在7ms处有峰值 ，此时平均值无法准确反映中心趋势，会产生误导。看到性能指标平均值（尤其是平均延时）时，需关注分布情况。

<h4 id="ZUH8p">2.8.6 异常值</h4>
统计中异常值是少量极高或极低、不符合预期分布的值。如磁盘I/O延时大多在0 - 10ms ，但偶尔出现超1000ms的情况，会严重影响性能。正态分布中异常值可能使平均值偏移，但对中位数影响小。用标准差和第99百分位数可更好识别异常值，了解多重模态分布、异常值等复杂情况，可用直方图等方法审视数据整体情况。

<h3 id="DuVip">2.9 监视</h3>
系统性能监视记录一段时间序列的性能统计数据，便于找出基于时间的使用规律，对容量规划、量化增长、显示峰值使用情况有帮助，历史数据能为理解性能指标当前值提供背景。

<h4 id="yo0r3">2.9.1 基于时间的规律</h4>
通过云计算服务器文件系统不同时间跨度（一天、五天、三十天）操作数目读取曲线示例，展现基于时间的规律：

+ **每小时**：应用程序环境存在活动周期，如监视和报告任务常以每5或10分钟为周期执行。
+ **每天**：使用规律常与工作时间（早9:00 - 晚5:00 ）相关，互联网服务则与全球用户活动时间有关，还可能有晚间日志回滚、备份等事务。
+ **每周**：存在基于工作日和周末的规律。 
+ **每季度**：如财务报告按季度完成。负载非规律性增长可能源于网站发布新内容等因素。

<h4 id="GryYO">2.9.2 监测产品</h4>
+ 第三方性能监测产品功能多样，包括数据存档、数据图表网页交互显示、可配置警报系统 。部分通过在系统上运行代理软件收集统计数据，代理软件可调用操作系统监测工具（可能低效或引发性能问题）或直接链接操作系统库和接口读取数据。
+ 采用SNMP协议监测，系统支持该协议时，无需在系统上运行客户端程序。随着系统分布式发展和云计算兴起，集中化监测系统可通过一个界面监测大量系统，部分公司会开发自有监测系统以适配自身客户环境和需求。

<h4 id="tkB3D">2.9.3 启动以来的信息统计</h4>
若未执行监测，可检查系统自启动以来自带的信息统计，用于与当前值比对。

<h3 id="GAFLy">2.10 可视化</h3>
可视化通过直观方式检查数据，便于识别和匹配规律，确定不同指标源间相关性。

<h4 id="Z5uQI">2.10.1 线图</h4>
常见的数据可视化方法，用于检查一段时间的性能指标，X轴为时间。如展示20秒内磁盘I/O延时平均值的线图，可看出读取延时约4ms，高于预期。还可绘制多条线展示相关数据，如同时呈现同一时段磁盘I/O的中位数、标准差和百分位数，揭示平均值比预期大是因存在高延时I/O，如第99百分位数线显示1%的I/O超过20ms，中位数线显示预期I/O延时约1ms。

![](https://cdn.nlark.com/yuque/0/2025/png/45533403/1745398123891-9a872737-af98-477f-ab87-110efd81ca30.png)

![](https://cdn.nlark.com/yuque/0/2025/png/45533403/1745398134374-6670a737-0df6-4053-9c52-aea6529d614f.png)

<h4 id="kKpUo">2.10.2 散点图</h4>
将每次磁盘I/O按完成时间（X轴）和延时值（Y轴）标记为点。同一时段磁盘I/O散点图可展示所有数据，能看出延时平均值高于预期，大量I/O延时达10ms、20ms甚至超50ms，显示出异常值存在。但数据多时，点易重叠难分辨，且需收集处理对应坐标数据。

![](https://cdn.nlark.com/yuque/0/2025/png/45533403/1745398144900-48f1278d-9b2f-4ed6-aa4a-82996250dcee.png)

<h4 id="w33W1">2.10.3 热图</h4>
将X和Y轴区域分组量化成“桶”并涂色，颜色依区域内事件数目而定，解决散点图扩展问题。如展示磁盘I/O情况的热图，可呈现高延时异常值，大量数据按规律分布，部分I/O返回零延时（磁盘缓存命中），部分延时略小于1ms（磁盘缓存失效）。不过热图不如线图直观，需用户有一定了解才能有效使用。

![](https://cdn.nlark.com/yuque/0/2025/png/45533403/1745398160690-dd47ea7d-d51c-4583-adce-578bbf2eae56.png)

<h4 id="wEQHK">2.10.4 表面图</h4>
三维表示方式，呈现三维平面，三维值变化不频繁剧烈时效果最佳。如展示数据中心300台物理服务器和5312颗CPU在60秒内每秒使用率的线框形表面图，以16颗CPU为行，每秒使用率测量值为列，表面高度或颜色反映使用率，可识别出高使用率和低使用率区域 。



![](https://cdn.nlark.com/yuque/0/2025/png/45533403/1745398199054-ba302939-750d-475f-ad4a-fa333f72c1d6.png)

<h3 id="lYstP">第三章 操作系统</h3>


了解操作系统及其内核对于系统性能分析至关重要。本章为后续章节做知识储备，分为背景知识（介绍术语和操作系统基础 ）、内核（总结Linux和基于Solaris的内核 ）两部分。与性能相关事项在后续章节详述。

<h4 id="ktgOI">3.1术语</h4>
+ **操作系统**：安装在系统上的软件和文件，包含内核、管理工具、系统库，用于启动和运行程序。
+ **内核**：管理系统硬件设备、内存、CPU调度，运行于CPU特权模式，可直接访问硬件。
+ **进程**：执行程序的环境，通常在用户模式运行，通过系统调用或自陷入内核模式。 
+ **线程**：可在CPU上调度执行的上下文，内核有多个线程，一个进程含一个或多个线程。 
+ **任务**：Linux可运行实体，可为进程、进程中的线程或内核线程。 
+ **内核空间**：内核的内存地址空间。 
+ **用户空间**：进程的内存地址空间，存放用户级程序和库。 
+ **上下文切换**：内核程序切换CPU操作的地址空间。 
+ **系统调用**：用户程序请求内核执行特权操作的协议。 
+ **处理器**：含一颗或多颗CPU的物理芯片。 
+ **自陷**：信号发至内核请求执行系统程序，包括系统调用、处理器异常、中断。 
+ **中断**：物理设备向内核发送请求I/O服务的信号，是自陷的一种。

<h3 id="afK03">3.2 背景</h3>
<h4 id="mvhKv">3.2.1 内核</h4>
**内核管理范畴**：内核负责管理CPU调度、内存、文件系统、网络协议以及系统设备(如磁盘、网络接口等)，并通过系统调用为设备访问和内核服务提供机制。

![](https://cdn.nlark.com/yuque/0/2025/png/45533403/1745398776234-ae352b6c-c45e-4fde-a57c-8261ff57c492.png)

1. **内核角色图解读**
    - 从图中可知系统库、系统调用、内核、硬件、应用程序存在层次关系。系统库提供的编程接口比单纯系统调用更丰富简便。应用程序涵盖数据库、Web服务器等用户级软件。
    - 系统库环有缺口，意味着应用程序在系统允许时可直接进行系统调用。传统模型中该环封闭，特权从内核向外逐层降低，此模型源于Multics，是UNIX前身。
2. **内核执行特点**
    - 内核代码量庞大，通常有几十万行。其执行按需进行，如响应用户程序的系统调用或设备中断。部分内核线程异步执行系统维护任务，像内核时钟程序和内存管理任务，且资源占用少。
    - I/O密集型负载（如Web服务器 ）常在内核上下文执行；计算密集型负载尽量不干扰内核，但在CPU竞争等场景下，内核调度会影响性能。内核需决定线程执行顺序，还会根据情况选择合适CPU以提升性能。
3. **时钟机制**
    - 经典UNIX内核核心组件clock()例程，由计时器中断触发，历史上每秒执行60、100或1000次，每次为一个tick，功能包括更新系统时间等。
    - 存在性能问题及改进：
        * **tick延时**：100Hz时钟下，处理等待下一个tick可能延时达10ms，高精度实时中断已解决此问题。
        * **tick开销**：现代处理器有动态电源功能，Linux采用动态tick，系统空闲时clock例程不启动，降低功耗。现代内核将很多功能从clock例程移到按需中断，以打造无tick内核 ，如Linux的clock例程除更新系统时钟和jiffies计数器外，执行任务减少 。
4. **内核态**
    - 内核运行在特殊CPU模式即内核态，在此状态下可访问设备、执行特权指令。内核控制设备访问以支持多任务处理，进程和用户间数据默认无法互访。
    - 用户程序（进程）运行在用户态，请求内核特权操作（如I/O ）通过系统调用传递。执行系统操作时，会进行上下文切换，从用户态到内核态，以更高特权级别执行。用户态和内核态都有各自软件执行上下文，包括栈和寄存器，在用户态执行特权指令会引发异常，由内核处理。上下文切换在I/O等场景增加开销，也会在不同进程（如CPU调度 ）间发生。

![](https://cdn.nlark.com/yuque/0/2025/png/45533403/1745398797884-85c0913a-f9e1-4597-90f6-5c73f97381c4.png)

<h4 id="U2gfF"><font style="background-color:#FBDE28;">3.2.2 栈</font></h4>
1. **栈的作用**：栈通过函数和寄存器记录线程执行历史，CPU利用栈高效处理函数执行。函数调用时，CPU将寄存器（含CPU状态 ）存入栈，函数返回时，栈按顺序恢复寄存器状态。栈检查可用于调试和性能分析。
2. **如何读栈**：以Linux内核栈示例展示TCP传输调用路径，栈顶在第一行，通过函数名及调试器信息可了解调用历史，能自上而下或自下而上阅读跟踪执行路径，但栈内函数多无文档，因源于源代码。
3. **用户栈和内核栈**
    - 执行系统调用时，进程的线程有用户级栈和内核级栈。线程被阻塞时，用户级栈在系统调用期间不变，执行内核上下文时，线程使用单独内核级栈，不过信号处理程序依配置可借用用户级栈。

![](https://cdn.nlark.com/yuque/0/2025/png/45533403/1745398819082-bba1fb93-24d5-4f85-81bb-31a514e8cc6d.png)

<h4 id="aatky"><font style="background-color:#FBDE28;">3.2.3 中断和中断线程</font></h4>
+ **中断概念**：内核除响应系统调用外，还需响应设备服务请求，即中断，它会中断当前执行。
+ **中断服务程序**：通过注册处理设备中断，设计要点是运行尽可能快，减少对活动线程中断的影响。若工作多且可能阻塞，最好用中断线程处理。不同内核版本实施方式不同，如Linux设备驱动分上下半部，上半部快速处理中断，下半部在禁止模式后处理，可作为tasklet或工作队列，由内核线程调度；Solaris系统把中断工作放中断线程。
+ **中断延时**：从中断开始到中断服务完成的时间叫中断延时，有专门学科研究实时或低延时系统。

![](https://cdn.nlark.com/yuque/0/2025/png/45533403/1745398910557-aa9c6c6c-e640-4a07-bb93-254d29073784.png)

<h4 id="C5Qyf"><font style="background-color:#FBDE28;">3.2.4 中断优先级</font></h4>
+ **定义**：用中断优先级（IPL）表示当前活跃中断服务程序的优先级，在中断信号发出时从处理器读取。若读到级别高于当前执行中断，该中断成功；否则排队等待，避免高优先级工作被低优先级工作打断。
+ **范围示例**：以内核服务作为中断线程的IPL，范围为1 - 10 ，如串行I/O中断优先级高，因硬件缓冲小，需快速服务避免溢出。

![](https://cdn.nlark.com/yuque/0/2025/png/45533403/1745398924372-205173a4-6391-4fe7-97f3-25f4b48bfba4.png)

<h4 id="lf8kV"><font style="background-color:#FBDE28;">3.2.5 进程</font></h4>
+ **进程概念**：是执行用户级程序的环境，包含内存地址空间、文件描述符、线程栈和寄存器等，类似早期电脑虚拟化，可让内核进行多任务处理，系统中可执行上千个进程，用进程ID（PID）识别，每个PID唯一 。一个进程含一个或多个线程，线程在进程地址空间操作，共享文件描述符，多线程进程可在多个CPU上并发执行。
+ **进程创建**：正常通过系统调用fork()创建，fork()用自身进程号创建副本，再调用exec()执行不同程序。fork()采用写时拷贝（COW）策略提升性能，添加原有地址空间引用，有进程修改被引用内存时，针对修改建立独立副本，减少内存和CPU使用。 

![](https://cdn.nlark.com/yuque/0/2025/png/45533403/1745398937194-84c96179-a27a-4b29-81ab-4cf2edac1ef1.png)

+ **进程生命周期**：包括创建、idle（空闲 ）、ready - to - run（就绪 ，在CPU运行队列等待 ）、on - proc（运行在CPU上 ）、sleep（因I/O阻塞 ，I/O完成或被唤醒 ）、zombie（进程终止，等待父进程读取状态或被内核清除 ）等状态。 

![](https://cdn.nlark.com/yuque/0/2025/png/45533403/1745398945339-1f7af417-5654-4c6c-a055-5264d2b5b46b.png)

+ **进程环境**：包括进程地址空间内数据和内核元数据（上下文 ）。内核上下文含进程ID、所有者用户ID等属性和统计信息，还有文件描述符；用户地址空间包含可执行段、库、堆等内存段。

![](https://cdn.nlark.com/yuque/0/2025/png/45533403/1745398967766-3ca0df2a-3891-4135-84a7-488768c136c1.png)

<h4 id="PpTJm"><font style="background-color:#FBDE28;">3.2.6 系统调用</font></h4>
+ **概念及设计理念**：系统调用用于请求内核执行特权系统例程。UNIX理念倡导尽量减少系统调用数量，以保持内核简洁，更复杂接口应构建在用户空间，便于开发和维护。
+ **关键系统调用列举**
    - **文件操作类**：read()读取字节、write()写入字节、open()打开文件 、close()关闭文件、stat()获取文件统计信息。
    - **进程管理类**：fork()创建新进程、exec()执行新程序。
    - **网络通信类**：connect()连接到网络主机、accept()接受网络连接。
    - **其他功能类**：ioctl()设置I/O属性等、mmap()把文件映射到内存地址空间、brk()扩展堆指针 。
+ **系统调用特性**：都有良好文档，一般随操作系统附带Man手册。接口简单一致，设置有特殊变量errno，用于出错时指示错误及其类型。
+ **部分系统调用用法说明**
    - **ioctl()**：用途广泛但用法多样，例如Linux的perf工具执行特权指令协调性能监测点时，通过调用ioctl() （如perf_event_open()会用ioctl()返回文件描述符 ），以不同参数执行不同行为，开发人员可对参数进行操作修改。
    - **mmap()**：常将可执行文件、库及内存映射文件映射到进程地址空间，提升性能，内存映射管理会做权衡。
    - **brk()**：由系统内存分配库执行，定义进程工作内存大小，有时会替代基于brk()的malloc()为进程分配内存，减少系统调用频率。

<h4 id="Mbu2Z"><font style="background-color:#FBDE28;">3.2.7 虚拟内存</font></h4>
+ **概念与功能**：虚拟内存是主存的抽象，为进程和内核提供近乎无穷且私有的主存视角。支持多任务处理，让进程和内核在私有地址空间操作，无需担心竞争。还支持主存超额使用，必要时可在主存（一级存储，即RAM ）和二级存储（磁盘 ）间映射虚拟内存。
+ **实现原理**：依赖处理器和操作系统支持，并非真实内存。多数操作系统仅在内存首次填充（写入 ）时，将虚拟内存映射到真实内存。

![](https://cdn.nlark.com/yuque/0/2025/png/45533403/1745399562018-4b6cdeed-01c6-4b79-94e3-50f03f6f3252.png)

<h4 id="l1MJR"><font style="background-color:#FBDE28;">3.2.8 内存管理</font></h4>
+ **管理目标**：当虚拟内存借助二级存储扩展主存时，内核致力于将最活跃数据保留在主存中。
+ **内核例程方式**
    - **交换（swapping）**：使整个进程在主存和二级存储间移动，是原始UNIX方法，会造成严重性能损耗。
    - **换页（paging）**：移动小内存单元（如4KB的页 ），是更高效的方法，被引入到BSD。两种方法都将最近最少使用或最近未使用的内存移至二级存储，按需再移回主存 。在Linux中，术语swapping用于指代paging，且Linux内核不支持老UNIX风格的整体线程和进程交换。

<h4 id="zpG6j"><font style="background-color:#FBDE28;">3.2.9 调度器</font></h4>
**基本概念**：UNIX及其衍生系统是分时系统，通过划分执行时间实现多进程同时运行。调度器是操作系统内核关键组件，负责进程在处理器和CPU间的调度，在Linux中操作对象是任务（task，即线程 ），并将其映射到CPU上。 

**调度原理**

+ 调度器把CPU时间划分给活跃进程和线程，维护优先级机制，以便重要工作优先执行。它跟踪所有处于ready - to - run状态的线程，传统上每个优先级队列叫运行队列，现代内核为每个CPU实现队列，也可用其他数据结构跟踪线程。当待运行线程数多于可用CPU数时，低优先级线程等待。多数内核线程优先级高于用户级线程。 
+ 调度器可动态修改进程优先级以优化特定工作负载性能。工作负载分为：
        * **CPU密集型**：如科学和数学分析程序，执行大量计算，运行时间长，受CPU资源限制。
        * **I/O密集型**：如Web服务器、文件服务器、交互shell，执行I/O操作多，计算少，需低延时响应，负载增加时受存储I/O或网络资源限制。
+ 调度器识别CPU密集型进程并降低其优先级，让I/O密集型负载优先运行，通过计算进程最近在CPU上执行时间与真实时间比例，选择比例低的进程降低优先级，可优化性能。
+ **现代内核调度特点**：现代内核支持多类别调度，采用不同算法管理优先级和可运行线程，包括实时调度类别（优先级高于非关键工作 ），还支持抢占（后续会详述 ），实时调度类别为系统提供低延时调度。 关于内核调度详细内容可参考第6章。

![](https://cdn.nlark.com/yuque/0/2025/png/45533403/1745399652827-3ef11a50-d3f9-4d88-bf5c-166b0e3f7f09.png)

<h4 id="pW2gE">3.2.10 文件系统</h4>
+ **文件系统定义与作用**：文件系统是文件和目录的数据组织形式，通常基于POSIX标准的接口访问。内核可支持多种文件系统类型和实例，提供文件系统支持是操作系统重要功能之一 。
+ **文件系统结构**：操作系统提供以根目录（“/” ）为起点的全局文件命名空间，呈自上而下拓扑结构。通过挂载（mounting）可添加文件系统分支到挂载点，用户遍历文件命名空间时无需考虑底层文件系统类型。顶层目录有不同用途，如etc放系统配置文件，usr是系统提供的用户级程序和库，dev是设备文件，var包括系统日志等文件，tmp是零时文件，home是用户主目录 。

![](https://cdn.nlark.com/yuque/0/2025/png/45533403/1745399757094-ed134b09-4f6b-472a-a408-755614f14d18.png)

+ **文件系统存储**：多数文件系统用磁盘等存储设备存放内容，部分如/proc和/dev由内核动态生成。
+ **虚拟文件系统（VFS）**
    - **起源与目的**：VFS是对文件系统类型做抽象的内核界面，由Sun Microsystems公司提出，初衷是让UNIX文件系统（UFS）和NFS更易共存。
    - **作用**：使内核添加新文件系统更简便，支持全局文件命名空间，让用户程序和应用程序能透明访问各类文件系统。
+ **I/O栈**：基于存储设备的文件系统，从用户级软件到存储设备的路径称为I/O栈，是软件栈的子集 。第8章将详细介绍文件系统及其性能，第9章介绍相关存储设备内容。

![](https://cdn.nlark.com/yuque/0/2025/png/45533403/1745399736704-452064dc-a75c-476b-9a8b-b50676ba6c39.png)

![](https://cdn.nlark.com/yuque/0/2025/png/45533403/1745399771524-a49058f7-258f-410c-bb25-2b2ba2f4a9c0.png)

<h4 id="v5UvW"><font style="background-color:#FBDE28;">3.2.11 缓存</font></h4>
+ **缓存目的**：因磁盘I/O延时较长，软件栈的很多层级通过缓存读取和缓存写入来避免这种情况。
+ **缓存类型及实例**
    - **应用程序缓存**：无具体实例列举。
    - **服务器缓存**：如Apache缓存。
    - **缓存服务器**：例如memcached。
    - **数据库缓存**：像MySQL缓冲区高速缓存。 
    - **目录缓存**：如DNSLC。
    - **文件元数据缓存**：inode缓存。 
    - **操作系统缓冲区高速缓存**：如segvn 。
    - **文件系统主缓存**：ZFS ARC 。
    - **文件系统次缓存**：ZFS L2ARC 。
    - **设备缓存**：ZFS vdev 。
    - **块缓存**：缓冲区高速缓存，是主存的一块区域，用于存放最近使用的磁盘块，若请求的磁盘块在其中，可避免高延时的磁盘I/O 。
    - **磁盘控制器缓存**：如RAID卡缓存。
    - **存储阵列缓存**：无具体实例。
    - **磁盘内置缓存**：无具体实例。
+ **缓存特点**：基于不同的系统和环境，缓存的类型会有较大差异。

<h4 id="FLXBu"><font style="background-color:#FBDE28;">3.2.12 网络</font></h4>
+ **网络协议**：现代内核提供内置网络协议，使系统能通过网络通信，融入分布式系统环境，典型的是TCP/IP栈，涵盖TCP和IP协议，用户级应用程序通过套接字进行网络端点通信。
+ **网络接口**：连接网络的物理设备是网络接口卡（NIC），系统管理员需将IP地址关联到网络接口才能实现网络通信。
+ **协议变化**：网络协议基本结构相对稳定，但协议增强和选项会改变，如TCP新选项、新的TCP阻塞控制算法等，这些变化及对不同网络接口卡的支持都依赖内核。关于网络和网络性能更多内容参考第10章。

<h4 id="wBuGH"><font style="background-color:#FBDE28;">3.2.13 设备驱动</font></h4>
+ **功能作用**：内核需与各种物理设备通信，通过设备驱动实现，设备驱动是用于设备管理和设备I/O的内核软件，常由硬件设备厂商提供，部分内核支持“可插拔”设备驱动，无需系统重启就能装卸设备驱动。
+ **设备类型**
    - **字符设备**：提供无缓冲的设备顺序访问，访问尺寸依设备而定，如键盘、串行口、纸带和打印机等，是最早UNIX就有的设备类型。 
    - **块设备**：以块为单位访问，块的偏移值可随机访问，偏移值在设备头部从0开始计数，最早UNIX的块设备接口还为设备缓冲区提供缓存（缓冲区高速缓存 ）以提升性能。

<h4 id="iKmum"><font style="background-color:#FBDE28;">3.2.14 多处理器</font></h4>
+ **对称多处理结构（SMP）**：支持多处理器让操作系统能用多个CPU并行工作，常实现为SMP，所有CPU平等对待，但实现时因并行线程间共享内存和CPU会遇到调度、线程同步及内存访问架构等问题，相关细节参考第6、7章 。
+ **CPU交叉调用**：多处理器系统中会出现CPU需协调情况，如内存翻译条目缓存一致性失效时，CPU可通过交叉调用请求其他CPU执行工作，交叉调用设计为快速执行的处理器中断，以减少对其他线程的影响，抢占也可使用交叉调用。

<h4 id="LLhyt"><font style="background-color:#FBDE28;">3.2.15 抢占</font></h4>
+ **概念及作用**：支持内核抢占使高优先级用户级线程可中断内核执行，让实时系统成为可能，这类系统对响应时间要求严格。完全可抢占内核支持抢占，但仍有少量关键代码路径不能中断 。
+ **Linux的做法**：Linux通过自愿内核抢占实现，在内核代码逻辑停止点进行检查并执行抢占，避免完全抢占式内核复杂性，为常见工作负载提供低延时抢占。

<h4 id="BtJsK"><font style="background-color:#FBDE28;">3.2.16 资源管理</font></h4>
+ **管理方式**：操作系统提供可配置控制，用于精细管理CPU、内存、磁盘、网络等系统资源，可针对不同应用程序或用户环境设定资源使用限制，或采用灵活方法共享剩余资源 。
+ **不同系统情况**
    - **UNIX和BSD**：早期版本基于每个进程进行资源控制，如用nice(1)调整CPU优先级，ulimit(1)做资源限定。
    - **Solaris**：从Solaris 9（2002 ）起提供先进资源管理，文档见resources_controls(5)的Man手册。 
    - **Linux**：开发了控制组（control groups, cgroups ）并整合进2.6.24（2008 ）版本，相关记录在Documentation/cgroups中，后续章节会讲述具体资源控制，第11章有管理OS虚拟化租户性能用例。

<h4 id="TCd0O"><font style="background-color:#FBDE28;">3.2.17 观测性</font></h4>
+ **观测工具构成**：操作系统由内核、库和程序组成，其中包含观测系统活动和性能分析的工具，通常安装在/usr/bin和/usr/sbin目录下 。
+ **工具来源**：用户可安装第三方工具增加观测手段，关于观测工具及基于操作系统组件构建的观测工具将在下一章介绍。

<h3 id="uW4rq">3.3 内核</h3>
+ **概述**：本节介绍基于Solaris系统的内核和Linux的内核，涉及其历史、特点，以性能为重点探讨差异，UNIX作为背景知识也会介绍。现代内核在文件系统、观测框架、系统调用界面、网络栈架构、实时支持、I/O调度等方面存在区别。通过对比近期内核版本及系统调用数目（Linux系统调用数目呈增加趋势，Solaris呈减少趋势 ）可看出差异。

<h4 id="B6Qw2"><font style="background-color:#FBDE28;">3.3.1 UNIX</font></h4>
+ **起源**：由Ken Thompson、Dennis Ritchie等人于1969年及之后在AT&T贝尔实验室开发。开发人员基于Multics操作系统，打造轻量多任务操作系统和内核，命名自UNICS 。
+ **内核特点**：内核代码不能因用户喜好替换，做事方法是所有方法的最小公约数。早期内核小，但提供高性能功能，如进程有调度优先级，磁盘I/O用大磁盘块，支持多任务处理。后为支持网络等功能，内核不断增长，衍生出BSD、SunOS（Solaris ）、Linux等 。

<h4 id="XCc21"><font style="background-color:#FBDE28;">3.3.2 基于Solaris</font></h4>
+ **内核渊源**：Solaris内核衍生自UNIX，部分代码直接来自最初UNIX内核，始于Sun Microsystems公司1982年的SunOS，基于BSD，后发展出SunOS 5.0（即Solaris 2.0 ）等 。
+ **与性能相关开发**
    - **文件与存储相关**
        * **NFS**：允许文件通过网络共享并挂载，版本3和4有性能提升。
        * **VFS**：抽象界面让多种文件系统共存，便于NFS和UFS共存。
        * **页缓存（ZFS ARC ）**：缓存虚拟内存页，多数操作系统文件系统缓存首选。
        * **内存映射文件**：减少文件I/O开销，重写虚拟内存时引入。
    - **通信相关**
        * **RPC**：远程过程调用接口。
        * **NIS**：网络信息服务，用于网络共享信息，现渐被LDAP取代。 
        * **CacheFS**：缓存文件系统，提升NFS服务器性能，后因NFS性能提升不再广泛使用。
    - **内核特性相关**
        * **完全抢占内核**：保证高优先级工作低延时。
        * **调度器**：多个调度器级别可调整不同类型工作负载性能，如分时、交互、实时等。
        * **多处理器支持**：20世纪90年代投入多处理器操作研究，开发出ASMP和SMP 。
        * **slab分配器**：替代buddy分配器，让每个CPU缓存预分配缓冲区，提高性能。 
        * **crash分析**：成熟内核crash dump分析框架，含modular debugger 。
        * **M:N线程调度**：实现线程和进程间对象，用于高效调度线程，后相关实现有变化但术语和数据结构部分保留。
    - **网络相关**
        * **STREAMS网络栈**：搭建在AT&T STREAMS接口上的TCP/IP网络栈，后多数管道在Solaris 10前被移除。 
        * **64位支持**：Solaris 7内核提供对64位处理器支持。
        * **锁统计**：Solaris 7引入锁性能统计。 
        * **MPSS**：支持多种页面尺寸，提升内存操作效率。 
        * **MPO**：内存位置优化，提高内存访问性能。 
        * **FireEngine**：增强Solaris 10高性能TCP/IP栈，提升本地性能、分散CPU负载。 
        * **DTrace**：动态跟踪框架，可观测软件栈，已移植到其他操作系统。 
        * **Zones**：虚拟化技术，创建共享主内核操作系统实例，类似FreeBSD的jails 。
        * **Crossbow**：提供高性能虚拟网络接口和网络带宽资源控制。 
        * **ZFS**：高性能企业级文件系统，开源，成为许多文件服务器基础文件系统 。
+ **开源情况**：Sun于2005年以OpenSolaris项目开源Solaris，2010年Oracle收购Sun后代码更新发布停止，OpenSolaris最后版本是Solaris 11开发版镜像，基于illumos内核的操作系统如Joyent公司的SmartOS在书中相关例子有应用。

<h4 id="QbBSq"><font style="background-color:#FBDE28;">3.3.3 基于Linux</font></h4>
+ **起源**：1991年由Linus Torvalds开发，是针对英特尔个人电脑的免费、自由操作系统。项目灵感部分源于MINIX（针对小型计算机的小型UNIX版本 ），当时BSD试图提供免费UNIX版本存在法律问题 。
+ **开发借鉴思路**
    - **Unix（和Multics）**：借鉴操作系统层级、系统调用、多任务处理、进程相关属性、虚拟内存、全局文件系统、文件系统权限、设备文件、缓冲区高速缓存等。
    - **BSD**：参考换页虚拟内存、按需换页、快文件系统（FFS）、TCP/IP网络栈、套接字。 
    - **Solaris**：引入VFS、NFS、页缓存、统一页缓存、slab分配器，以及在进程中使用ZFS和DTrace 。
    - **Plan 9**：采用资源forks（rfork），为进程间和线程（任务 ）间共享设置不同级别。
+ **与性能相关功能**
    - **CPU调度**：开发多种先进调度算法，如调度域（2.6.7 ），利于非一致存储访问架构（NUMA）决策。
    - **I/O调度**：有不同块I/O调度算法，如deadline（2.5.39 ）、anticipatory（2.5.75 ）、完全公平队列（CFQ ）（2.6.6 ） 。
    - **TCP增强**：支持新TCP拥塞算法，可按需选择。
    - **内存管理**：Overcommit机制含out-of-memory（OOM）killer；Futex（2.5.7 ）提供高性能用户级同步原语；巨型页（2.5.36 ）由内核和内存管理单元支持大型内存预分配 。
    - **性能分析工具**：Oprofile（2.5.43 ）用于研究CPU使用等；RCU（2.5.43 ）提供只读更新同步机制；epoll（2.5.46 ）高效处理文件描述符I/O等待 。
    - **I/O调度扩展**：模块I/O调度（2.6.10 ）提供可插拔调度算法 。
    - **调试工具**：DebugFS（2.6.11 ）提供内核性能数据接口 。
    - **进程管理**：Cpusets（2.6.12 ）将进程独占CPU分组；自愿内核抢占（2.6.13 ）实现低延时调度 。
    - **文件系统与I/O监控**：inotify（2.6.13 ）监控文件系统事件；blktrace（2.6.17 ）跟踪块I/O事件 。
    - **数据传输**：splice（2.6.17 ）在文件描述符和管道间快速移动数据，不经过用户空间 。
    - **统计监测**：延时审计（2.6.18 ）跟踪任务延时状态；IO统计（2.6.20 ）测量进程存储I/O统计 。
    - **节能机制**：DynTicks（2.6.21 ）实现动态tick，节省CPU资源和电力 。
    - **内存分配**：SLUB（2.6.22 ）是新的slab内存分配器简化版本 。
    - **调度算法**：CFS（2.6.23 ）为完全公平调度算法 。
    - **资源控制**：cgroups（2.6.24 ）测量并限制进程组资源使用 。
    - **延时观测**：latencytop（2.6.25 ）观察操作系统延时来源 。
    - **内核跟踪**：Tracepoints（2.6.28 ）作为静态内核跟踪点，用于跟踪工具 。
    - **性能工具**：perf（2.6.31 ）是性能观测工具，涵盖CPU性能计数器剖析等 。
    - **内存优化**：透明巨型页（2.6.38 ）简化巨型内存页面使用 。
    - **动态跟踪**：Uprobes（3.5 ）是用户级软件动态跟踪基础设施 。
    - **虚拟化**：KVM（基于内核的虚拟机 ）由Qumranet公司为Linux开发（后被Red Hat收购 ），可创建虚拟操作系统实例并运行虚拟机内核 。部分功能如epoll和KVM已移植或重新实现在基于Solaris的系统上 。Linux因广泛设备驱动支持，推动其他操作系统开源 。

<h4 id="xkvDA"><font style="background-color:#FBDE28;">3.3.4 差异</font></h4>
+ **内核差异概述**：Linux和基于Solaris的内核虽都是UNIX后代且有相同操作系统理念，但存在诸多差异，难以简洁概括。
+ **差异具体表现**
    - **系统优势来源**：Linux优势多来自内核和操作系统本身，如对应用程序包、设备驱动的支持，且多数基于Linux的系统是开源的；基于Solaris的系统优势源于应用程序包（如ZFS作为企业级文件系统、DTrace作为强大观测工具 ），多数基于Solaris的系统也是开源的（Oracle Solaris除外 ），但驱动支持不如Linux广泛 。
    - **观测工具差异**：基于Solaris的系统在性能观测工具方面有优势，如ZFS和DTrace，且较为成熟可用，DTrace自2003年移植到Linux后，相关框架未默认安装或广泛启用 。
    - **性能优化差异**：两个内核在性能优化上有许多细微不同，如POSIX的fadvise()调用在Linux中实现，应用程序可用其告知内核不缓存与文件描述符相关数据，提高Linux内核缓存效率和性能，而基于Solaris的系统中没有（但可能后续修正 ）。
+ **差异对性能影响**：两个系统性能因工作负载会有小差异，最大差异在于性能观测工具（特别是动态跟踪支持 ），若一个内核的观测工具能带来大幅性能提升，之前较小的性能差异就相对不那么重要了，后续章节将介绍观测工具。

<h2 id="wp8fO">第4章 观测工具</h2>
+ **引言**：历史上操作系统提供众多工具观测系统软硬件，但实际与全面观测存在差距，系统性能专家需借助推论和间接工具了解系统活动。追踪框架引入，尤其是动态跟踪，极大拓展了观测能力，催生大量新观测工具。本章介绍操作系统观测工具类型、关键示例及基础框架，重点是 /proc、kstat、/sys、DTrace 和 SystemTap 等框架 ，后续章节会介绍更多相关工具如Linux性能事务（LPE）。

<h3 id="x6iAL">4.1工具类型</h3>
+ **分类方式**：性能观测工具按系统级别和进程级别分类，多数基于计数器或跟踪，部分工具属性不唯一，如top有系统级视图，DTrace也有进程级能力，还有基于剖析（profiling）的性能工具。

<h4 id="m5uiK">4.1.1计数器</h4>
+ **原理**：内核维护的统计数据，为无符号整型，事件发生时递增，默认开启由内核维护，仅用户空间读取时开销极小。
+ **系统级别工具**
    - vmstat：虚拟内存和物理内存统计。
    - mpstat：每个CPU的使用情况。 
    - iostat：每个磁盘I/O的情况，由块设备接口报告。 
    - netstat：网络接口、TCP/IP栈及每个连接的统计信息。 
    - sar：各种统计，能归档历史数据。这些工具系统全体用户可见，统计数据常被监控软件绘图，使用时可选时间间隔和次数，如vmstat (8) 以秒为间隔输出。
+ **进程级别工具**
    - ps：显示进程状态及各种统计信息，如内存和CPU使用情况。 
    - top：按CPU使用等统计数据排序，显示排名靠前进程，Solaris 对应工具是prstat (1M)。 
    - pmap：将进程内存使用统计一起列出，从 /proc 文件系统读取统计信息。

<h4 id="jZTE8">4.1.2 跟踪</h4>
+ **原理**：跟踪收集每个事件的数据以供分析，跟踪框架默认不启用，因其捕获数据会占用CPU时间和存储空间，这些开销会拖累跟踪对象，在评估测量时间时需考虑。日志是默认开启的低频率跟踪，通常针对偶发事件，如错误和警告，且日志包含每个事件数据。
+ **系统级别跟踪工具**
    - **利用内核跟踪设施**
        * tcpdump：基于libpcap库的网络包跟踪工具。
        * snoop：基于Solaris系统打造的网络包跟踪工具。 
        * blktrace：Linux系统的块I/O跟踪工具。 
        * iosnoop：基于DTrace的块I/O跟踪工具。 
        * execsnoop：基于DTrace跟踪新进程的工具。 
        * dtruss：基于DTrace的系统级别系统调用缓冲跟踪工具。
        * DTrace：可跟踪内核内部活动和所有资源使用情况（不限于网络和块I/O ），支持静态和动态跟踪。 
        * SystemTap：跟踪内核内部活动和资源使用，支持静态和动态跟踪，与DTrace一样是可编程环境，可构建系统级别跟踪工具。 
        * perf：Linux性能事件跟踪工具，可跟踪静态和动态探针。
+ **进程级别跟踪工具**
    - strace：基于Linux系统的系统调用跟踪工具。 
    - truss：基于Solaris系统的系统调用跟踪工具。 
    - gdb：源自代码级调试器，可扩展应用于Linux系统。 
    - mdb：Solaris系统的一个具有可扩展性的调试器。 调试器能检查每个事件数据，但操作时需停止目标程序执行再重新启动。DTrace、SystemTap和perf虽更适合归为系统级别工具，但也支持对单个进程检查。

<h4 id="vgt7Y">4.1.3 剖析</h4>
+ **原理**：通过对目标收集采样或快照归纳目标特征，常见如对CPU使用率，对程序计数器采样或跟踪栈找消耗CPU周期代码路径，采样频率固定（如100Hz或1000Hz ） 。剖析工具可能会微调采样频率避免与目标活动同步致计算偏差。剖析也可基于非计时硬件事件，如CPU硬件缓存未命中、总线活动等，能找出对系统资源使用有责任的代码路径，助力开发人员优化代码。
+ **系统和进程级别工具**
    - oprofile：Linux系统剖析工具。
    - perf：Linux性能工具集，含剖析子命令。 
    - DTrace：程序剖析基于时间用profile provider，基于硬件事件用cpu provider。 
    - SystemTap：程序剖析基于时间用timer tapset，基于硬件事件用自身perf tapset。 
    - cachegrind：源自valgrind工具集，可剖析硬件缓存使用，也能用kcachegrind做数据可视化。 
    - Intel VTune Amplifier XE：适用于Linux和Windows的剖析工具，有包括源代码浏览的图形界面。 
    - Oracle Solaris Studio：自带性能分析器，可对Solaris和Linux剖析，有含源代码浏览的图形界面。编程语言通常有专用剖析器，剖析工具更多内容见第6章。

<h4 id="JWfuY">4.1.4 监视（sar）</h4>
+ **原理及功能**：第2章介绍过监视，sar(1)是广泛用于监视单一操作系统的工具，源于AT&T的UNIX，基于计数器，在预定时间（通过cron ）执行记录系统计数器状态，支持命令行查看数据。默认读系统统计归档数据（若开启）打印历史统计信息，也可指定时间间隔和次数检查当前活动。
+ **局限性与替代方案**：sar(1)虽能报告大量统计数据，但无法覆盖所有需求且可能有误差（尤其在Solaris系统 ） 。替代工具有System Data Recorder和Collectl等。在Linux中，sar(1)通过sysstat包提供，第三方监视产品常基于sar(1) 。sar(1)具体使用见第6、7、8、10章，附录C是其选项总结。

<h3 id="N3ZaV">4.2 观测来源</h3>
介绍为观测工具提供统计数据的接口和框架，系统性能统计主要来源是/proc、/sys和kstat ，还会介绍延时核算、微状态核算等来源，以及DTrace和SystemTap工具，它们构建于相关框架之上。

<h4 id="kX0xz">4.2.1 /proc</h4>
+ **概述**：提供内核统计信息的文件系统接口，包含众多目录，以进程ID命名的目录代表对应进程，其下文件含进程信息和统计数据，由内核数据结构映射而来。Linux中，/proc还有提供系统级别统计数据的文件。多数文件只读，为观测工具提供统计数据，部分文件可写，用于控制内核和进程行为。该文件系统直观，将内核统计数据以目录树形式暴露给用户空间，编程接口是POSIX文件系统调用（open()、read()、close() ） ，通过访问权限保证用户级别安全。
+ **Linux 下 /proc**
    - **进程级统计文件**：limits（资源限制）、maps（映射内存区域）、sched（CPU调度信息）等与进程性能观测相关。
    - **系统级统计文件和目录**：cpuinfo（处理器信息）、diskstats（磁盘I/O统计）、interrupts（中断计数器）等与系统性能观测相关。可通过strace跟踪工具读取，文件多为文本格式，可用shell脚本命令读取。某些文件条目取决于内核版本和CONFIG选项，如开启schedstats需CONFIG_SCHEDSTATS ，开启sched debug需CONFIG_SCHED_DEBUG 。
+ **Solaris 下 /proc**
    - 仅含进程状态统计，系统级别观测主要靠kstat 。进程目录文件如map（虚拟地址空间映射）、psinfo（进程信息含CPU和内存使用）等与性能观测相关。truss输出可显示prstat读取进程状态过程，psinfo文件数据结构为psinfo_t ，用户空间可直接读取该变量及成员变量值，C编程处理更合适。/proc文档在proc(4)的Man手册页和头文件sys/procfs.h中，若内核开源，研究源代码有助于了解统计来源和工具使用方式。
+ **lxproc**：基于Solaris系统，为实现类似Linux的/proc ，因两个/proc不同，移植Linux观测工具（如htop ）困难。lxproc文件系统提供Linux大体兼容的/proc ，可与procfs标准的/proc同时挂载，需类似Linux /proc的应用程序可修改为从/lxproc加载进程信息，lxproc尚不完善，为Linux /proc用户提供便利接口。

<h4 id="bV8vW">4.2.2 /sys</h4>
+ **概述**：Linux的sysfs文件系统，挂载在/sys ，2.6内核引入，为内核统计提供目录结构。与/proc不同，/sys经发展将各种系统信息置于顶层目录，最初设计用于提供设备驱动统计数据，现扩展到提供所有统计类型。
+ **示例**：以CPU 0的/sys文件列表部分内容为例，许多文件与CPU硬件缓存相关，内容可通过命令查看。/sys文件多数只读，也有很多可写文件用于调整内核状态，如通过对“online”文件写入“1”或“0”控制CPU上线和下线 ，状态设置可通过文本字符完成，无需二进制接口。

<h4 id="IrpKH">4.2.3 kstat</h4>
+ **概述**：基于Solaris的系统中，为系统级别观测工具所用的内核统计框架，包含CPU、磁盘、网络接口、内存及内核许多软件组件等大量资源统计，典型系统中有上万可用统计。与/proc和/sys不同，kstat无伪文件系统，通过ioctl从/dev/kstat读取，常用libkstat库（虽一些优化版本取消对该Perl库支持 ） ，命令行工具kstat(1M)也能提供统计数据并用于shell脚本。
+ **结构**：kstat是四元组结构（module:instance:name:statistic ） 。module指创建统计数据的内核模块，如sd指SCSI磁盘驱动，zfs指ZFS文件系统；instance指模块以多实例形式存在时的枚举值，如每个SCSI磁盘对应一个sd模块实例；name是统计数据的名字；statistic是单个统计值名。
+ **使用示例**：如用kstat(1M)指定四元组名称读取rproc统计值（当前运行进程数目 ） ，通过特定命令可匹配并打印system_misc组下所有统计值。很多kstat统计值是累计的，如freemem统计值每秒累加空闲页面数目，可按时间间隔计算平均值，也有提供即时值的版本缓解累计版本缺点。不指定统计值名称时，kstat(1M)会列出所有统计数据，可结合grep、wc等工具搜索和计算相关统计数目。
+ **文档情况**：kstat统计无正式文档，因被认为是不稳定接口，内核改变时可能变化。理解统计值计算方式需研究源代码，如累计的freemem统计值源于特定内核代码，内核的clock()例程每秒执行一次使freemem统计值累计，可通过检查修改freemem的代码处了解其工作原理，现有系统工具源代码也可作为使用kstat的研究示例。

<h4 id="PpsNC">4.2.4 延时核算</h4>
+ **原理**：开启CONFIG_TASK_DELAY_ACCT选项的Linux系统，会跟踪每个任务在不同状态下的时间，包括调度器延时（等待轮到上CPU ） 、块I/O（等待块I/O完成）、交换（等待换页，即内存压力时 ） 、内存回收（等待内存回收例程 ） 。调度器延时统计数据虽源自schedstat（在/proc中 ） ，但与其他延时核算数据一起放在sched_info结构中（而非task_delay_info结构 ） 。
+ **数据获取方式**：用户空间工具通过taskstats获取这些统计数据，taskstats是基于网络连接的接口。内核源码的Documentation/accounting目录下，有delay - accounting.txt文档，还有示例代码getdelays.c 。
+ **示例**：以getdelays工具查看进程ID为17451的任务延时情况为例，展示了CPU、I/O、SWAP、RECLAIM等状态下的count（统计次数 ） 、real total（实际总时间 ） 、virtual total（虚拟总时间 ） 、delay total（延迟总时间 ） 、delay average（平均延迟 ） 等信息，时间单位通常是ns 。此例来自高CPU负载系统，显示出系统调度器延时严重。

<h4 id="wWikP">4.2.5 微状态核算</h4>
基于Solaris的系统具备线程级别和CPU级别的微状态核算功能，可针对预先设定状态记录高精度时间。相较于基于tick的指标，其精确度大幅提升，还提供额外状态用于性能分析。

+ **指标暴露方式**：CPU级别的指标经kstat ，供用户空间工具获取；进程级别的指标则通过/proc获取。
+ **显示示例**：CPU微状态在mpstat(1M)中体现为usr、sys和idl列 ，在内核代码中对应CMS_USER、CMS_SYSTEM和CMS_IDLE ；线程微状态在prstat -m输出结果中呈现为USR、SYS等列 ，第6章的6.6.7节有相关总结。

<h4 id="ZVZwF">4.2.6 其他的观测源</h4>
+ **CPU性能计数器**：可编程硬件寄存器，提供底层性能信息，如CPU周期计数、指令数、停滞周期等。Linux通过perf_events接口或系统调用perf_event_open()访问 ，相关工具如perf可利用其进行性能分析；Solaris通过lbrpc或cputat(1M)访问 ，具体内容见第6章。
+ **进程级别跟踪**：跟踪内核活动和函数调用，执行代价高，会拖累目标。Linux用ptrace控制进程跟踪，strace用于跟踪系统调用，还有uprobes实现用户级别的动态跟踪；Solaris用truss跟踪系统调用，DTrace进行动态跟踪。
+ **内核跟踪**：Linux的tracepoints提供静态内核探针（曾称内核标记kernel markers ） ，ftrace、perf、DTrace和SystemTap可利用；Solaris中静态和动态探针由dtrace内核模块提供，DTrace和SystemTap会在后续章节介绍并解释静态和动态探针。
+ **网络嗅探**：从网络设备抓包，分析数据包和协议性能。Linux通过libpcap库和/proc/net/dev ，命令行工具如tcpdump(8)实现；Solaris通过libdlpi库和/dev/net ，命令行工具为snoop(1M) ，移植的libpcap库和tcpdump(8)也可用，嗅探存在存储和性能开销，进程网络嗅探见第10章。
+ **进程核算**：基于进程执行和运行时间计费，Linux和Solaris都有相关形式，如Solaris的swapctl()用进程核算捕捉短暂存活进程信息，/proc快照可能无法察觉。
+ **系统调用**：部分系统调用和库函数调用能提供性能指标，如getrusage()可获取进程资源使用统计，包括用户时间、系统时间等，Solaris的swapctl()用于swap设备管理和统计（Linux对应/proc/swap ） 。
+ **更多观测源**：取决于内核版本和开启选项，Linux有IO核算、blktrace、timer_stats、lockstat、debugfs等；Solaris有扩展核算、流核算、Solaris审计等。寻找观测源可阅读内核代码相关部分，了解统计和跟踪点设置。若内核无所需统计数据，除动态跟踪外，还可使用调试器（如gdb(1)、mdb(1) ） ，极端情况可通过/dev/mem或/dev/kmem读取内核内存。自2003年起，Solaris内核中的DTrace致力于整合跟踪框架，简化跟踪，期望未来内核观测框架更少更强大。

<h3 id="FENrY">4.3 DTrace</h3>
DTrace是涵盖编程语言和工具的观测框架，本节介绍其基础知识，包括动态和静态跟踪、探针、provider、D语言、action、变量、单行语句和脚本等，可用于Solaris和Linux系统性能观测。

+ **工作原理**：通过探针观察用户级和内核级代码，探针命中时执行D语言编写的action，action可执行计数、记录时间戳、计算、打印及数据可视化等操作。
+ **动态跟踪示例**：以动态跟踪内核ZFS文件系统的spa_sync()函数为例，可显示完成时间和持续时间（单位ns ） 。该函数将写入数据回写到ZFS存储设备，会引发磁盘I/O，利用DTrace可研究其调用频率和持续时间，也可用于研究其他内核函数。
+ **优势**：与其他跟踪框架相比，DTrace设计为生产环境安全，开销小。利用CPU内核缓冲区，提升内存本地性，减少缓存一致性开销，移除对同步锁依赖，缓冲区还以温和速度（默认每秒一次）向用户空间传递数据，减少上下文切换，且提供操作对内核数据总结和过滤，降低数据负载。
+ **静态和动态跟踪**
    - **原理**：理解静态和动态跟踪可查看涉及的源和CPU指令。以illumos内核块设备接口代码中的biodone()函数为例，宏DTRACE_IOI是编译前加入代码的静态探针，动态探针在编译后软件运行时加入。当用动态跟踪探测biodone()入口时，第一个指令会变为引发软中断的指令，执行动态跟踪的action ，动态跟踪禁用时指令恢复原状，这是内核地址空间的现场修改，技术因处理器类型而异。
    - **特点**：不启用时DTrace零开销，启用时开销与探针触发频率（跟踪事件频率和执行的action ）成比例。DTrace能动态跟踪函数入口、返回及用户空间指令，在CPU指令上动态建立探针，接口不稳定，跟踪软件更新时可能变化，因此是单行命令和脚本的理想选择。

<h4 id="Eqh5V">4.3.2 探针</h4>
DTrace探针以四元组命名：provider:module:function:name 。provider是相关探针集合，类似软件库概念；module和function动态产生，标记探针指示的代码位置；name是探针名字。指定时可使用通配符“_” ，留空字段相当于通配符“_:*” ，指定探针时留空字段可省略，如“io:::start”是io provider的start探针，匹配所有位置上的start探针。 

<h4 id="W60Is">4.3.3 provider</h4>
可用的DTrace provider取决于DTrace和操作系统版本，常见的有：

+ **系统相关**：syscall（系统调用自陷表）、vminfo（虚拟内存统计）、sysinfo（系统统计）等。
+ **调度与进程**：profile（任意频率采样）、sched（内核调度事件）、proc（进程级别事件，如创建、执行、退出 ） 。
+ **设备与协议**：io（块设备接口跟踪，即磁盘I/O ） 、pid（用户级别动态跟踪）、tcp（TCP协议事件，如连接、发送和接收 ） 、ip（IP协议事件，发送和接收 ） 。
+ **内核与语言**：fbt（内核级别动态跟踪），还有为Java、JavaScript、Node.js等高级语言使用的provider 。很多provider用静态跟踪实现，接口稳定，使用它们可使脚本适配目标软件不同版本，虽监测视野受限，但能降低维护和文档负担。

<h4 id="IlzGx">4.3.4 参数</h4>
探针通过参数变量提供数据，参数使用取决于provider。如系统调用的provider为每个系统调用设置入口和返回探针，入口参数为arg0 - argN（系统调用参数），返回参数为arg0或arg1（返回值 ） ，errno也会设置。fbt和pid的provider参数类似。要了解各provider参数，可参考文档，或用dtrace(1)的 -lv选项打印概要。 

<h4 id="wCVhB">4.3.5 D语言</h4>
D语言与awk类似，可用于单行命令和脚本编写。DTrace语句形式为：probe_description /predicate/ { action } 。action是触发探针时执行的以分号间隔的语句系列；predicate是可选过滤表达式。例如“proc:::exec-success /execname == "httpd"/ { trace(pid); }” ，当进程名是“httpd”时，跟踪proc provider中的exec - success探针并执行trace(pid)操作，exec - success探针常用于跟踪新进程创建和系统调用exec()执行，当前进程名通过内置变量execname检索，进程ID通过pid检索。 

<h4 id="YjP7q">4.3.6 内置变量</h4>
内置变量用于计算和判断，可通过action（如trace()和printf() ）打印。常用内置变量包括：

+ **进程信息**：execname（进程名字符串）、uid（用户ID ） 、pid（进程ID ） 。
+ **时间信息**：timestamp（当前时间，自启动以来纳秒数）、vtimestamp（CPU上线程时间，单位纳秒）。
+ **探针参数**：arg0..N（探针参数，类型为uint64_t ） 、args[0]..[N]（类型化的探针参数 ） 。
+ **其他**：curthread（指向当前线程内核结构指针）、probefunc（探针描述的函数组件）、probename（探针描述的命名组件）、curpsinfo（当前进程信息）。

<h4 id="Fy1Qh">4.3.7 action</h4>
action是DTrace中探针触发时执行的操作，常用action如下：

| action | 描述 |
| --- | --- |
| trace(arg) | 打印arg |
| printf(format, arg, …) | 打印格式化的字符串 |
| stringof(addr) | 返回来自内核空间的字符串 |
| copyinstr(addr) | 返回来自用户空间地址的字符串（需内核执行从用户空间到内核空间的复制操作） |
| stack(count) | 打印内核级别的栈跟踪，若有count，按count截断 |
| ustack(count) | 打印用户级别的栈跟踪，若有count，按count截断 |
| func(pc) | 从内核程序计数器（pc），返回内核函数名 |
| ufunc(pc) | 从用户程序计数器（pc），返回用户函数名 |
| exit(status) | 退出DTrace并返回状态 |
| trunc(@agg, count) | 截断聚合变量，或全部删除（删除所有键），或按指定键数目（count）截断 |
| clear(@agg) | 删除聚合变量的值（键保留） |
| printa(format, @agg) | 格式化地打印聚合变量 |


后三个action使用聚合型（aggregation）变量类型。

<h4 id="jc0cc">4.3.8 变量类型</h4>
变量类型相关信息如下：

| 类型 | 前缀 | 作用域 | 开销 | 多CPU安全 | 赋值示例 |
| --- | --- | --- | --- | --- | --- |
| 聚合变量 | @ | 全局 | 低 | 是 | @x = count(); |
| 带键的聚合变量 | @[] | 全局 | 低 | 是 | @x[pid] = count(); |
| 从句局部变量 | this-> | 从句实例 | 非常低 | 是 | this->x = 1; |
| 线程局部变量 | self-> | 线程内 | 中等 | 是 | self->x = 1; |
| 标量 | 无 | 全局 | 中下 | 否 | x = 1; |
| 关联数组 | 无 | 全局 | 中上 | 否 | x[y] = 1; |


+ 线程局部变量作用域在线程内，像时间戳等数据易与线程关联。
+ 子句局部变量用于中间计算，只在针对同一探针描述的action子句中有效。
+ 多个CPU同时对同一个标量写会损坏变量状态。
+ 聚合变量（aggregation）由CPU单独计算汇总后传递到用户空间，开销最低，是数据汇总的一种方法。用于填充聚合变量的action如下：

| 聚合Action | 描述 |
| --- | --- |
| count() | 发生计数 |
| sum(value) | 对value求和 |
| min(value) | 记录value的最小值 |
| max(value) | 记录value的最大值 |
| quantize(value) | 用2的幂次方直方图记录value |
| lquantize(value, min, max, step) | 用给定最小值、最大值和步进值做线性直方图记录value |
| llquantize(value, factor, min_magnitude, max_magnitude, steps) | 用混合对数/线性直方图记录value |


<h4 id="xo8IO">4.3.9 单行命令</h4>
DTrace可编写简洁强大的单行命令，示例如下：

+ 跟踪系统调用open()，打印进程名和文件路径：`dtrace -n 'syscall::open*entry { printf("%s %s", execname, copyinstr(arg0)); }'`  ，Oracle Solaris 11对系统调用自陷表有修改，需注意。
+ 按进程名归纳所有CPU的交叉调用：`dtrace -n 'sysinfo:::xcalls { @[execname] = count(); }'` 
+ 按99Hz采样内核栈：`dtrace -n 'profile:::profile-99 { @[stack()] = count(); }'`  ，本书许多DTrace单行命令列于附录D。

<h4 id="Epgh3">4.3.10 脚本</h4>
可将DTrace语句保存到文件执行，编写长DTrace程序。如脚本bitesize.d根据进程显示请求的磁盘I/O大小：

```plain
#!/usr/bin/dtrace -s
#pragma D option quiet
dtrace:::BEGIN
{
    printf("Tracing... Hit Ctrl-C to end.\n");
}
io:::start
{
    this->size = arg0->b_bcount;
    @[pid, curpsinfo->pr_psargs] = quantize(this->size);
}
dtrace:::END
{
    printf("%-6s %-16s\n", "PID", "CMD");
    printa("%-6d %-16s\n", @);
}
```

文件开头是解释器行（#! ） ，可执行且能从命令行运行。`#pragma D`设置安静模式，压缩默认输出。`dtrace:::BEGIN`探针开始时打印信息，`io:::start`探针观测磁盘I/O开始并记录大小，`dtrace:::END`探针结束时打印输出。示例输出展示了跟踪时多数磁盘I/O由tar命令请求及相应I/O大小，该脚本来自DTraceToolkit脚本集合，可在网上找到。 

<h4 id="lkF4N">4.3.11 开销</h4>
DTrace利用每个CPU内核缓冲区和内核聚合总结减小跟踪开销，默认每秒一次从内核空间向用户空间传递数据。还有减小开销和提高安全性功能，如系统无响应时终止跟踪。跟踪执行开销与跟踪频率和执行的action相关，跟踪磁盘I/O频率不高（1000 I/O每秒或更少）时开销可忽略，跟踪网络I/O包速率达每秒百万次时开销显著。action有代价，如对所有CPU按99Hz采样内核栈（用stack() ）无明显开销，对用户级栈采样（用ustack() ）通常需将频率降至97Hz 。数据存入变量（特别是关联数组）也有开销，使用DTrace通常无明显开销，但需谨慎。 

<h4 id="yAygo">4.3.12 文档和资源</h4>
+ **参考资料**：DTrace的action、内置变量、标准provider等在《Dynamic Tracing Guide》有介绍，可在网上免费获取；动态跟踪背景知识、解决问题及DTrace演化历史，参见[Cantrill 04]和[Cantrill 06]。
+ **单行命令**：附录D列出便利的DTrace单行命令，可作为学习DTrace调用的参考。
+ **脚本和策略**：《DTrace: Dynamic Tracing in Oracle Solaris, Mac OS X and FreeBSD》介绍相关内容，书中脚本可在线获得。
+ **脚本集合**：DTraceToolkit包含200多个脚本，多数用shell或Perl封装提供命令行选项，像execsnoop工具。
+ **相关GUI**：Oracle的ZFS Appliance Analytics和Joyent公司的Cloud Analytics提供基于DTrace的GUI。

<h3 id="vVkIS">4.4 SystemTap</h3>
SystemTap由Red Hat、IBM和Intel团队专为Linux打造，提供用户级和内核级代码的静态与动态跟踪功能，在DTrace未移植到Linux时开发。使用时可在探针执行action，包括事件计数、记录时间戳等，跟踪开启时action实时执行，能在命令行执行单行命令或运行脚本。

+ **跟踪来源**：静态探针用tracepoints，动态探针用kprobes（内核级）、uprobes（用户级） ，这些来源也被其他工具（如perf、LTTng ）使用。
+ **发展现状与问题**：多年发展后在功能上追赶DTrace且有超越，但稳定性欠佳，存在启动慢、错误信息难懂、隐式功能无文档、语言精炼度不够等问题。同时有将DTrace移植到Linux的项目，如Oracle Enterprise Linux和Paul Fox主导的项目，但新移植项目可能导致内核崩溃。若需使用SystemTap，可参考附录E将本书多数DTrace脚本转换。

<h3 id="WoX9K">4.4.1 探针</h3>
探针定义由句号分隔，部分有内置选项（放括号中），示例如下：

+ begin（程序开始）、end（程序结束） 。
+ syscall.read（系统调用read()开始）、syscall.read.return（系统调用read()结束） 。
+ kernel.function("sys_read")（内核函数sys_read()开始）、kernel.function("sys_read").return（内核函数sys_read()结束） 。
+ socket.send（发送包）、timer.ms(100)（单CPU每100ms触发一次）、timer.profile（按内核时钟频率对所有CPU触发，用于采样/剖析） 。
+ process("a.out").statement("*@main.c:100")（跟踪目标进程a.out中main.c第100行代码执行） 。许多探针将相关数据作为内置变量，如syscall.read探针把请求大小存于$count 。

<h4 id="W0GnJ">4.4.2 tapset</h4>
一组相关探针称为tapset，很多探针名字以tapset名字开头，示例有：

+ syscall（系统调用）、block（块设备接口和I/O调度器） 。
+ scheduler（内核CPU调度器事件）、memory（进程和虚拟内存使用） 。
+ scsi（SCSI目标事件）、networking（网络设备事件，含接收和传输） 。
+ tcp（TCP协议事件，含发送和接收）、socket（套接字事件） 。tapset还可附带执行action。

<h4 id="uGwfL">4.4.3 action和内置变量</h4>
SystemTap提供众多action和内置变量，如execname()获取进程名，pid()获取当前进程ID，print_backtrace()打印内核栈回溯信息，更多内容参考附录E。

<h4 id="gZp7O">4.4.4 示例</h4>
以跟踪系统调用read() ，将返回读取大小结果存成2次方直方图的单行命令为例：

```plain
# stap -vm 'global stats; probe syscall.read.return { stats <<< $return; }
probe end { print("nValue(bytes)n"); print(@hist_log(stats)); }'
```

选项-vm会打印编译阶段详细信息，告知用户跟踪开启。不选该选项时SystemTap默认不打印，过早按Ctrl+C不仅会终止跟踪，还可能因编译中断打印混乱错误消息。单行命令以声明全局变量stats开始，用probe关键字定义匹配系统调用read()返回的探针，action用stats记录返回值![image](data:image/svg+xml;utf8,%3Csvg%20xmlns%3Axlink%3D%22http%3A%2F%2Fwww.w3.org%2F1999%2Fxlink%22%20width%3D%22101.326ex%22%20height%3D%223.343ex%22%20style%3D%22vertical-align%3A%20-1.171ex%3B%22%20viewBox%3D%220%20-934.9%2043626.3%201439.2%22%20role%3D%22img%22%20focusable%3D%22false%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%20aria-labelledby%3D%22MathJax-SVG-1-Title%22%3E%0A%3Ctitle%20id%3D%22MathJax-SVG-1-Title%22%3EEquation%3C%2Ftitle%3E%0A%3Cdefs%20aria-hidden%3D%22true%22%3E%0A%3Cpath%20stroke-width%3D%221%22%20id%3D%22E1-MJMATHI-72%22%20d%3D%22M21%20287Q22%20290%2023%20295T28%20317T38%20348T53%20381T73%20411T99%20433T132%20442Q161%20442%20183%20430T214%20408T225%20388Q227%20382%20228%20382T236%20389Q284%20441%20347%20441H350Q398%20441%20422%20400Q430%20381%20430%20363Q430%20333%20417%20315T391%20292T366%20288Q346%20288%20334%20299T322%20328Q322%20376%20378%20392Q356%20405%20342%20405Q286%20405%20239%20331Q229%20315%20224%20298T190%20165Q156%2025%20151%2016Q138%20-11%20108%20-11Q95%20-11%2087%20-5T76%207T74%2017Q74%2030%20114%20189T154%20366Q154%20405%20128%20405Q107%20405%2092%20377T68%20316T57%20280Q55%20278%2041%20278H27Q21%20284%2021%20287Z%22%3E%3C%2Fpath%3E%0A%3Cpath%20stroke-width%3D%221%22%20id%3D%22E1-MJMATHI-65%22%20d%3D%22M39%20168Q39%20225%2058%20272T107%20350T174%20402T244%20433T307%20442H310Q355%20442%20388%20420T421%20355Q421%20265%20310%20237Q261%20224%20176%20223Q139%20223%20138%20221Q138%20219%20132%20186T125%20128Q125%2081%20146%2054T209%2026T302%2045T394%20111Q403%20121%20406%20121Q410%20121%20419%20112T429%2098T420%2082T390%2055T344%2024T281%20-1T205%20-11Q126%20-11%2083%2042T39%20168ZM373%20353Q367%20405%20305%20405Q272%20405%20244%20391T199%20357T170%20316T154%20280T149%20261Q149%20260%20169%20260Q282%20260%20327%20284T373%20353Z%22%3E%3C%2Fpath%3E%0A%3Cpath%20stroke-width%3D%221%22%20id%3D%22E1-MJMATHI-74%22%20d%3D%22M26%20385Q19%20392%2019%20395Q19%20399%2022%20411T27%20425Q29%20430%2036%20430T87%20431H140L159%20511Q162%20522%20166%20540T173%20566T179%20586T187%20603T197%20615T211%20624T229%20626Q247%20625%20254%20615T261%20596Q261%20589%20252%20549T232%20470L222%20433Q222%20431%20272%20431H323Q330%20424%20330%20420Q330%20398%20317%20385H210L174%20240Q135%2080%20135%2068Q135%2026%20162%2026Q197%2026%20230%2060T283%20144Q285%20150%20288%20151T303%20153H307Q322%20153%20322%20145Q322%20142%20319%20133Q314%20117%20301%2095T267%2048T216%206T155%20-11Q125%20-11%2098%204T59%2056Q57%2064%2057%2083V101L92%20241Q127%20382%20128%20383Q128%20385%2077%20385H26Z%22%3E%3C%2Fpath%3E%0A%3Cpath%20stroke-width%3D%221%22%20id%3D%22E1-MJMATHI-75%22%20d%3D%22M21%20287Q21%20295%2030%20318T55%20370T99%20420T158%20442Q204%20442%20227%20417T250%20358Q250%20340%20216%20246T182%20105Q182%2062%20196%2045T238%2027T291%2044T328%2078L339%2095Q341%2099%20377%20247Q407%20367%20413%20387T427%20416Q444%20431%20463%20431Q480%20431%20488%20421T496%20402L420%2084Q419%2079%20419%2068Q419%2043%20426%2035T447%2026Q469%2029%20482%2057T512%20145Q514%20153%20532%20153Q551%20153%20551%20144Q550%20139%20549%20130T540%2098T523%2055T498%2017T462%20-8Q454%20-10%20438%20-10Q372%20-10%20347%2046Q345%2045%20336%2036T318%2021T296%206T267%20-6T233%20-11Q189%20-11%20155%207Q103%2038%20103%20113Q103%20170%20138%20262T173%20379Q173%20380%20173%20381Q173%20390%20173%20393T169%20400T158%20404H154Q131%20404%20112%20385T82%20344T65%20302T57%20280Q55%20278%2041%20278H27Q21%20284%2021%20287Z%22%3E%3C%2Fpath%3E%0A%3Cpath%20stroke-width%3D%221%22%20id%3D%22E1-MJMATHI-6E%22%20d%3D%22M21%20287Q22%20293%2024%20303T36%20341T56%20388T89%20425T135%20442Q171%20442%20195%20424T225%20390T231%20369Q231%20367%20232%20367L243%20378Q304%20442%20382%20442Q436%20442%20469%20415T503%20336T465%20179T427%2052Q427%2026%20444%2026Q450%2026%20453%2027Q482%2032%20505%2065T540%20145Q542%20153%20560%20153Q580%20153%20580%20145Q580%20144%20576%20130Q568%20101%20554%2073T508%2017T439%20-10Q392%20-10%20371%2017T350%2073Q350%2092%20386%20193T423%20345Q423%20404%20379%20404H374Q288%20404%20229%20303L222%20291L189%20157Q156%2026%20151%2016Q138%20-11%20108%20-11Q95%20-11%2087%20-5T76%207T74%2017Q74%2030%20112%20180T152%20343Q153%20348%20153%20366Q153%20405%20129%20405Q91%20405%2066%20305Q60%20285%2060%20284Q58%20278%2041%20278H27Q21%20284%2021%20287Z%22%3E%3C%2Fpath%3E%0A%3Cpath%20stroke-width%3D%221%22%20id%3D%22E1-MJMATHI-64%22%20d%3D%22M366%20683Q367%20683%20438%20688T511%20694Q523%20694%20523%20686Q523%20679%20450%20384T375%2083T374%2068Q374%2026%20402%2026Q411%2027%20422%2035Q443%2055%20463%20131Q469%20151%20473%20152Q475%20153%20483%20153H487H491Q506%20153%20506%20145Q506%20140%20503%20129Q490%2079%20473%2048T445%208T417%20-8Q409%20-10%20393%20-10Q359%20-10%20336%205T306%2036L300%2051Q299%2052%20296%2050Q294%2048%20292%2046Q233%20-10%20172%20-10Q117%20-10%2075%2030T33%20157Q33%20205%2053%20255T101%20341Q148%20398%20195%20420T280%20442Q336%20442%20364%20400Q369%20394%20369%20396Q370%20400%20396%20505T424%20616Q424%20629%20417%20632T378%20637H357Q351%20643%20351%20645T353%20664Q358%20683%20366%20683ZM352%20326Q329%20405%20277%20405Q242%20405%20210%20374T160%20293Q131%20214%20119%20129Q119%20126%20119%20118T118%20106Q118%2061%20136%2044T179%2026Q233%2026%20290%2098L298%20109L352%20326Z%22%3E%3C%2Fpath%3E%0A%3Cpath%20stroke-width%3D%221%22%20id%3D%22E1-MJMATHI-61%22%20d%3D%22M33%20157Q33%20258%20109%20349T280%20441Q331%20441%20370%20392Q386%20422%20416%20422Q429%20422%20439%20414T449%20394Q449%20381%20412%20234T374%2068Q374%2043%20381%2035T402%2026Q411%2027%20422%2035Q443%2055%20463%20131Q469%20151%20473%20152Q475%20153%20483%20153H487Q506%20153%20506%20144Q506%20138%20501%20117T481%2063T449%2013Q436%200%20417%20-8Q409%20-10%20393%20-10Q359%20-10%20336%205T306%2036L300%2051Q299%2052%20296%2050Q294%2048%20292%2046Q233%20-10%20172%20-10Q117%20-10%2075%2030T33%20157ZM351%20328Q351%20334%20346%20350T323%20385T277%20405Q242%20405%20210%20374T160%20293Q131%20214%20119%20129Q119%20126%20119%20118T118%20106Q118%2061%20136%2044T179%2026Q217%2026%20254%2059T298%20110Q300%20114%20325%20217T351%20328Z%22%3E%3C%2Fpath%3E%0A%3Cpath%20stroke-width%3D%221%22%20id%3D%22E1-MJMAIN-28%22%20d%3D%22M94%20250Q94%20319%20104%20381T127%20488T164%20576T202%20643T244%20695T277%20729T302%20750H315H319Q333%20750%20333%20741Q333%20738%20316%20720T275%20667T226%20581T184%20443T167%20250T184%2058T225%20-81T274%20-167T316%20-220T333%20-241Q333%20-250%20318%20-250H315H302L274%20-226Q180%20-141%20137%20-14T94%20250Z%22%3E%3C%2Fpath%3E%0A%3Cpath%20stroke-width%3D%221%22%20id%3D%22E1-MJMAIN-29%22%20d%3D%22M60%20749L64%20750Q69%20750%2074%20750H86L114%20726Q208%20641%20251%20514T294%20250Q294%20182%20284%20119T261%2012T224%20-76T186%20-143T145%20-194T113%20-227T90%20-246Q87%20-249%2086%20-250H74Q66%20-250%2063%20-250T58%20-247T55%20-238Q56%20-237%2066%20-225Q221%20-64%20221%20250T66%20725Q56%20737%2055%20738Q55%20746%2060%20749Z%22%3E%3C%2Fpath%3E%0A%3C%2Fdefs%3E%0A%3Cg%20stroke%3D%22currentColor%22%20fill%3D%22currentColor%22%20stroke-width%3D%220%22%20transform%3D%22matrix(1%200%200%20-1%200%200)%22%20aria-hidden%3D%22true%22%3E%0A%20%3Cuse%20xlink%3Ahref%3D%22%23E1-MJMATHI-72%22%20x%3D%220%22%20y%3D%220%22%3E%3C%2Fuse%3E%0A%20%3Cuse%20xlink%3Ahref%3D%22%23E1-MJMATHI-65%22%20x%3D%22451%22%20y%3D%220%22%3E%3C%2Fuse%3E%0A%20%3Cuse%20xlink%3Ahref%3D%22%23E1-MJMATHI-74%22%20x%3D%22918%22%20y%3D%220%22%3E%3C%2Fuse%3E%0A%20%3Cuse%20xlink%3Ahref%3D%22%23E1-MJMATHI-75%22%20x%3D%221279%22%20y%3D%220%22%3E%3C%2Fuse%3E%0A%20%3Cuse%20xlink%3Ahref%3D%22%23E1-MJMATHI-72%22%20x%3D%221852%22%20y%3D%220%22%3E%3C%2Fuse%3E%0A%20%3Cuse%20xlink%3Ahref%3D%22%23E1-MJMATHI-6E%22%20x%3D%222303%22%20y%3D%220%22%3E%3C%2Fuse%3E%0A%3Cg%20transform%3D%22translate(2904%2C0)%22%3E%0A%3Ctext%20font-family%3D%22monospace%22%20stroke%3D%22none%22%20transform%3D%22scale(71.759)%20matrix(1%200%200%20-1%200%200)%22%3E%EF%BC%8C%3C%2Ftext%3E%0A%3C%2Fg%3E%0A%20%3Cuse%20xlink%3Ahref%3D%22%23E1-MJMATHI-65%22%20x%3D%223836%22%20y%3D%220%22%3E%3C%2Fuse%3E%0A%20%3Cuse%20xlink%3Ahref%3D%22%23E1-MJMATHI-6E%22%20x%3D%224303%22%20y%3D%220%22%3E%3C%2Fuse%3E%0A%20%3Cuse%20xlink%3Ahref%3D%22%23E1-MJMATHI-64%22%20x%3D%224903%22%20y%3D%220%22%3E%3C%2Fuse%3E%0A%3Cg%20transform%3D%22translate(5427%2C0)%22%3E%0A%3Ctext%20font-family%3D%22monospace%22%20stroke%3D%22none%22%20transform%3D%22scale(71.759)%20matrix(1%200%200%20-1%200%200)%22%3E%E6%8E%A2%3C%2Ftext%3E%0A%3Cg%20transform%3D%22translate(932%2C0)%22%3E%0A%3Ctext%20font-family%3D%22monospace%22%20stroke%3D%22none%22%20transform%3D%22scale(71.759)%20matrix(1%200%200%20-1%200%200)%22%3E%E9%92%88%3C%2Ftext%3E%0A%3C%2Fg%3E%0A%3Cg%20transform%3D%22translate(1865%2C0)%22%3E%0A%3Ctext%20font-family%3D%22monospace%22%20stroke%3D%22none%22%20transform%3D%22scale(71.759)%20matrix(1%200%200%20-1%200%200)%22%3E%E5%B0%86%3C%2Ftext%3E%0A%3C%2Fg%3E%0A%3Cg%20transform%3D%22translate(2798%2C0)%22%3E%0A%3Ctext%20font-family%3D%22monospace%22%20stroke%3D%22none%22%20transform%3D%22scale(71.759)%20matrix(1%200%200%20-1%200%200)%22%3E%E7%BB%9F%3C%2Ftext%3E%0A%3C%2Fg%3E%0A%3Cg%20transform%3D%22translate(3731%2C0)%22%3E%0A%3Ctext%20font-family%3D%22monospace%22%20stroke%3D%22none%22%20transform%3D%22scale(71.759)%20matrix(1%200%200%20-1%200%200)%22%3E%E8%AE%A1%3C%2Ftext%3E%0A%3C%2Fg%3E%0A%3Cg%20transform%3D%22translate(4664%2C0)%22%3E%0A%3Ctext%20font-family%3D%22monospace%22%20stroke%3D%22none%22%20transform%3D%22scale(71.759)%20matrix(1%200%200%20-1%200%200)%22%3E%E6%95%B0%3C%2Ftext%3E%0A%3C%2Fg%3E%0A%3Cg%20transform%3D%22translate(5597%2C0)%22%3E%0A%3Ctext%20font-family%3D%22monospace%22%20stroke%3D%22none%22%20transform%3D%22scale(71.759)%20matrix(1%200%200%20-1%200%200)%22%3E%E6%8D%AE%3C%2Ftext%3E%0A%3C%2Fg%3E%0A%3Cg%20transform%3D%22translate(6530%2C0)%22%3E%0A%3Ctext%20font-family%3D%22monospace%22%20stroke%3D%22none%22%20transform%3D%22scale(71.759)%20matrix(1%200%200%20-1%200%200)%22%3E%E4%BB%A5%3C%2Ftext%3E%0A%3C%2Fg%3E%0A%3Cg%20transform%3D%22translate(7462%2C0)%22%3E%0A%3Ctext%20font-family%3D%22monospace%22%20stroke%3D%22none%22%20transform%3D%22scale(71.759)%20matrix(1%200%200%20-1%200%200)%22%3E%E7%9B%B4%3C%2Ftext%3E%0A%3C%2Fg%3E%0A%3Cg%20transform%3D%22translate(8395%2C0)%22%3E%0A%3Ctext%20font-family%3D%22monospace%22%20stroke%3D%22none%22%20transform%3D%22scale(71.759)%20matrix(1%200%200%20-1%200%200)%22%3E%E6%96%B9%3C%2Ftext%3E%0A%3C%2Fg%3E%0A%3Cg%20transform%3D%22translate(9328%2C0)%22%3E%0A%3Ctext%20font-family%3D%22monospace%22%20stroke%3D%22none%22%20transform%3D%22scale(71.759)%20matrix(1%200%200%20-1%200%200)%22%3E%E5%9B%BE%3C%2Ftext%3E%0A%3C%2Fg%3E%0A%3Cg%20transform%3D%22translate(10261%2C0)%22%3E%0A%3Ctext%20font-family%3D%22monospace%22%20stroke%3D%22none%22%20transform%3D%22scale(71.759)%20matrix(1%200%200%20-1%200%200)%22%3E%E6%89%93%3C%2Ftext%3E%0A%3C%2Fg%3E%0A%3Cg%20transform%3D%22translate(11194%2C0)%22%3E%0A%3Ctext%20font-family%3D%22monospace%22%20stroke%3D%22none%22%20transform%3D%22scale(71.759)%20matrix(1%200%200%20-1%200%200)%22%3E%E5%8D%B0%3C%2Ftext%3E%0A%3C%2Fg%3E%0A%3Cg%20transform%3D%22translate(12127%2C0)%22%3E%0A%3Ctext%20font-family%3D%22monospace%22%20stroke%3D%22none%22%20transform%3D%22scale(71.759)%20matrix(1%200%200%20-1%200%200)%22%3E%EF%BC%8C%3C%2Ftext%3E%0A%3C%2Fg%3E%0A%3Cg%20transform%3D%22translate(13060%2C0)%22%3E%0A%3Ctext%20font-family%3D%22monospace%22%20stroke%3D%22none%22%20transform%3D%22scale(71.759)%20matrix(1%200%200%20-1%200%200)%22%3E%E4%B8%8D%3C%2Ftext%3E%0A%3C%2Fg%3E%0A%3Cg%20transform%3D%22translate(13993%2C0)%22%3E%0A%3Ctext%20font-family%3D%22monospace%22%20stroke%3D%22none%22%20transform%3D%22scale(71.759)%20matrix(1%200%200%20-1%200%200)%22%3E%E8%BE%93%3C%2Ftext%3E%0A%3C%2Fg%3E%0A%3Cg%20transform%3D%22translate(14925%2C0)%22%3E%0A%3Ctext%20font-family%3D%22monospace%22%20stroke%3D%22none%22%20transform%3D%22scale(71.759)%20matrix(1%200%200%20-1%200%200)%22%3E%E5%87%BA%3C%2Ftext%3E%0A%3C%2Fg%3E%0A%3Cg%20transform%3D%22translate(15858%2C0)%22%3E%0A%3Ctext%20font-family%3D%22monospace%22%20stroke%3D%22none%22%20transform%3D%22scale(71.759)%20matrix(1%200%200%20-1%200%200)%22%3E%E7%9B%B4%3C%2Ftext%3E%0A%3C%2Fg%3E%0A%3Cg%20transform%3D%22translate(16791%2C0)%22%3E%0A%3Ctext%20font-family%3D%22monospace%22%20stroke%3D%22none%22%20transform%3D%22scale(71.759)%20matrix(1%200%200%20-1%200%200)%22%3E%E6%96%B9%3C%2Ftext%3E%0A%3C%2Fg%3E%0A%3Cg%20transform%3D%22translate(17724%2C0)%22%3E%0A%3Ctext%20font-family%3D%22monospace%22%20stroke%3D%22none%22%20transform%3D%22scale(71.759)%20matrix(1%200%200%20-1%200%200)%22%3E%E5%9B%BE%3C%2Ftext%3E%0A%3C%2Fg%3E%0A%3Cg%20transform%3D%22translate(18657%2C0)%22%3E%0A%3Ctext%20font-family%3D%22monospace%22%20stroke%3D%22none%22%20transform%3D%22scale(71.759)%20matrix(1%200%200%20-1%200%200)%22%3E%E6%97%B6%3C%2Ftext%3E%0A%3C%2Fg%3E%0A%3Cg%20transform%3D%22translate(19590%2C0)%22%3E%0A%3Ctext%20font-family%3D%22monospace%22%20stroke%3D%22none%22%20transform%3D%22scale(71.759)%20matrix(1%200%200%20-1%200%200)%22%3E%E9%80%80%3C%2Ftext%3E%0A%3C%2Fg%3E%0A%3Cg%20transform%3D%22translate(20523%2C0)%22%3E%0A%3Ctext%20font-family%3D%22monospace%22%20stroke%3D%22none%22%20transform%3D%22scale(71.759)%20matrix(1%200%200%20-1%200%200)%22%3E%E5%87%BA%3C%2Ftext%3E%0A%3C%2Fg%3E%0A%3Cg%20transform%3D%22translate(21455%2C0)%22%3E%0A%3Ctext%20font-family%3D%22monospace%22%20stroke%3D%22none%22%20transform%3D%22scale(71.759)%20matrix(1%200%200%20-1%200%200)%22%3E%E4%B9%9F%3C%2Ftext%3E%0A%3C%2Fg%3E%0A%3Cg%20transform%3D%22translate(22388%2C0)%22%3E%0A%3Ctext%20font-family%3D%22monospace%22%20stroke%3D%22none%22%20transform%3D%22scale(71.759)%20matrix(1%200%200%20-1%200%200)%22%3E%E4%BC%9A%3C%2Ftext%3E%0A%3C%2Fg%3E%0A%3Cg%20transform%3D%22translate(23321%2C0)%22%3E%0A%3Ctext%20font-family%3D%22monospace%22%20stroke%3D%22none%22%20transform%3D%22scale(71.759)%20matrix(1%200%200%20-1%200%200)%22%3E%E6%89%93%3C%2Ftext%3E%0A%3C%2Fg%3E%0A%3Cg%20transform%3D%22translate(24254%2C0)%22%3E%0A%3Ctext%20font-family%3D%22monospace%22%20stroke%3D%22none%22%20transform%3D%22scale(71.759)%20matrix(1%200%200%20-1%200%200)%22%3E%E5%8D%B0%3C%2Ftext%3E%0A%3C%2Fg%3E%0A%3Cg%20transform%3D%22translate(25187%2C0)%22%3E%0A%3Ctext%20font-family%3D%22monospace%22%20stroke%3D%22none%22%20transform%3D%22scale(71.759)%20matrix(1%200%200%20-1%200%200)%22%3E%E5%9F%BA%3C%2Ftext%3E%0A%3C%2Fg%3E%0A%3Cg%20transform%3D%22translate(26120%2C0)%22%3E%0A%3Ctext%20font-family%3D%22monospace%22%20stroke%3D%22none%22%20transform%3D%22scale(71.759)%20matrix(1%200%200%20-1%200%200)%22%3E%E6%9C%AC%3C%2Ftext%3E%0A%3C%2Fg%3E%0A%3Cg%20transform%3D%22translate(27053%2C0)%22%3E%0A%3Ctext%20font-family%3D%22monospace%22%20stroke%3D%22none%22%20transform%3D%22scale(71.759)%20matrix(1%200%200%20-1%200%200)%22%3E%E6%95%B0%3C%2Ftext%3E%0A%3C%2Fg%3E%0A%3Cg%20transform%3D%22translate(27986%2C0)%22%3E%0A%3Ctext%20font-family%3D%22monospace%22%20stroke%3D%22none%22%20transform%3D%22scale(71.759)%20matrix(1%200%200%20-1%200%200)%22%3E%E5%AD%97%3C%2Ftext%3E%0A%3C%2Fg%3E%0A%3Cg%20transform%3D%22translate(28918%2C0)%22%3E%0A%3Ctext%20font-family%3D%22monospace%22%20stroke%3D%22none%22%20transform%3D%22scale(71.759)%20matrix(1%200%200%20-1%200%200)%22%3E%E6%80%BB%3C%2Ftext%3E%0A%3C%2Fg%3E%0A%3Cg%20transform%3D%22translate(29851%2C0)%22%3E%0A%3Ctext%20font-family%3D%22monospace%22%20stroke%3D%22none%22%20transform%3D%22scale(71.759)%20matrix(1%200%200%20-1%200%200)%22%3E%E7%BB%93%3C%2Ftext%3E%0A%3C%2Fg%3E%0A%3Cg%20transform%3D%22translate(30784%2C0)%22%3E%0A%3Ctext%20font-family%3D%22monospace%22%20stroke%3D%22none%22%20transform%3D%22scale(71.759)%20matrix(1%200%200%20-1%200%200)%22%3E%E3%80%82%3C%2Ftext%3E%0A%3C%2Fg%3E%0A%3Cg%20transform%3D%22translate(31717%2C0)%22%3E%0A%3Ctext%20font-family%3D%22monospace%22%20stroke%3D%22none%22%20transform%3D%22scale(71.759)%20matrix(1%200%200%20-1%200%200)%22%3E%E9%9C%80%3C%2Ftext%3E%0A%3C%2Fg%3E%0A%3Cg%20transform%3D%22translate(32650%2C0)%22%3E%0A%3Ctext%20font-family%3D%22monospace%22%20stroke%3D%22none%22%20transform%3D%22scale(71.759)%20matrix(1%200%200%20-1%200%200)%22%3E%E6%B3%A8%3C%2Ftext%3E%0A%3C%2Fg%3E%0A%3Cg%20transform%3D%22translate(33583%2C0)%22%3E%0A%3Ctext%20font-family%3D%22monospace%22%20stroke%3D%22none%22%20transform%3D%22scale(71.759)%20matrix(1%200%200%20-1%200%200)%22%3E%E6%84%8F%3C%2Ftext%3E%0A%3C%2Fg%3E%0A%3C%2Fg%3E%0A%20%3Cuse%20xlink%3Ahref%3D%22%23E1-MJMATHI-72%22%20x%3D%2239943%22%20y%3D%220%22%3E%3C%2Fuse%3E%0A%20%3Cuse%20xlink%3Ahref%3D%22%23E1-MJMATHI-65%22%20x%3D%2240394%22%20y%3D%220%22%3E%3C%2Fuse%3E%0A%20%3Cuse%20xlink%3Ahref%3D%22%23E1-MJMATHI-61%22%20x%3D%2240861%22%20y%3D%220%22%3E%3C%2Fuse%3E%0A%20%3Cuse%20xlink%3Ahref%3D%22%23E1-MJMATHI-64%22%20x%3D%2241390%22%20y%3D%220%22%3E%3C%2Fuse%3E%0A%20%3Cuse%20xlink%3Ahref%3D%22%23E1-MJMAIN-28%22%20x%3D%2241914%22%20y%3D%220%22%3E%3C%2Fuse%3E%0A%20%3Cuse%20xlink%3Ahref%3D%22%23E1-MJMAIN-29%22%20x%3D%2242303%22%20y%3D%220%22%3E%3C%2Fuse%3E%0A%3Cg%20transform%3D%22translate(42693%2C0)%22%3E%0A%3Ctext%20font-family%3D%22monospace%22%20stroke%3D%22none%22%20transform%3D%22scale(71.759)%20matrix(1%200%200%20-1%200%200)%22%3E%E7%9A%84%3C%2Ftext%3E%0A%3C%2Fg%3E%0A%3C%2Fg%3E%0A%3C%2Fsvg%3E)return值有些为负（代表错误码errno ） ，遵循内核惯例而非POSIX标准，且$return无文档说明。

<h4 id="LIPFM">4.4.5 开销</h4>
SystemTap的使用开销与DTrace类似。程序首次执行时，在编译阶段会消耗几秒CPU资源，之后会缓存程序，后续使用不会每次都产生此开销。还可在不同系统上编译程序，将缓存结果传输到目标系统。此外，其额外开销来自内核分析的内核调试信息，通常Linux发行版中不包含（大小可能为几百字节 ） 。

<h4 id="xnZ7I">4.4.6 文档和资源</h4>
+ **Man手册页**：SystemTap有大量Man手册页，每个探针的手册页都包含其中，如探针ioblock.request可通过命令`man -k ioblock.request`查看相关信息，包括探针触发时机、强弱类型、取值等。
+ **语言参考文档**：在线的《SystemTap Language Reference》是SystemTap语言的文档。
+ **新手教程和参考手册**：SystemTap文档网站有新手入门教程和tapset参考手册。
+ **功能示例与转换参考**：本书所有DTrace示例大多可体现SystemTap功能，关于两者转换可参考附录E。

<h3 id="zeGm3">4.5 perf</h3>
Linux性能事件（Linux Performance Events，LPE），简称perf ，功能不断演化，支持的性能观测范围日益宽泛。虽无DTrace和SystemTap那样的实时编程能力，但可执行静态和动态跟踪（基于instruction、kprobe和uprobe ） ，也具备剖析（profiling）功能，还能检查跟踪、局部变量和数据类型。作为Linux内核主线的一部分，若已安装，是容易使用的观测工具，其观测能力可解答相关疑问。perf(1)的部分跟踪开销与DTrace类似，典型使用场景中，DTrace常用于汇总内核数据（使用聚合变量），而perf(1)目前在这方面功能尚不完善，使用perf(1)时数据会传递到用户空间做后续处理，频繁跟踪事件时可能产生显著额外开销，更多介绍及功能演示见第6章6.6节。 

<h3 id="wnWfh">4.6 观测工具的观测</h3>
+ **工具可靠性问题**：观测工具及构建它们的统计由软件实现，软件存在缺陷，相关文档也可能有误。使用时需用质疑眼光审视新统计数据，探究其真实意义和正确性。指标可能不准确，工具、Man手册页也不总是正确，工具功能可能不完整，设计也可能存在偏差。
+ **工具交叉检查**：多个观测工具覆盖范围重叠时，可相互检查。理想情况下，基于不同框架构建的工具可用于排查问题，动态跟踪在定制化工具构建方面优势明显。
+ **验证工具表现**：通过施加已知负载，观察工具表现与预期是否相符来验证工具，可使用微基准测试技术。
+ **关注文档问题**：有时问题并非出在工具或统计上，而是包含在Man手册页等文档中的信息出错，软件更新可能导致文档未同步更新。实际使用中，要及时反复确认所用性能测量的准确性，遇到不寻常结果或与预期差异大的结果时，需排查原因，未经审视的指标可能无价值。
+ **指标完整性考量**：指标可能不完整，面对众多工具和指标，易假定其完整有效，但实际可能并非如此。程序员添加指标后进行观测工具测试，可发现覆盖不全面的问题。缺少合适指标比使用不恰当指标问题更严重，可通过研究性能分析问题发现缺失指标。

<h2 id="eSORe">第5章 应用程序</h2>
<h3 id="c8w4u">5.1 应用程序概述</h3>
    - **性能调整位置**：性能调整最好在应用程序层面进行。应用程序涵盖数据库、Web服务器、应用服务器、负载均衡器、文件服务器等。后续章节将从CPU、内存、文件系统、磁盘和网络等资源消耗角度审视应用程序。
    - **应用程序复杂度**：在分布式应用程序环境中，应用程序极其复杂。研究应用程序内部通常是开发者领域，涉及第三方工具自测。系统性能研究人员（包括系统管理员）需分析配置应用程序以实现资源最佳利用，归纳应用程序使用方式和常见问题。
    - **本章内容**：讨论应用程序基础知识、性能基础原理、编程语言和编译器，以及性能分析通用策略。

**5.1.1 应用程序基础**

    - **了解应用程序的维度**
        * **功能**：明确应用程序角色，如数据库、Web服务器等。
        * **操作**：应用服务器处理的请求或操作，如数据库服务查询和命令、Web服务器服务HTTP请求等，可用速率衡量以评估负载和容量规划。
        * **CPU模式**：判断应用程序是用户级还是内核级实现，多数以一个或多个进程形式执行，部分以内核服务形式实现（如NFS） 。
        * **配置**：检查应用程序配置及其原因，包括与性能相关的可调参数（如缓冲区、缓存大小、并发进程等）。 
        * **指标**：获取性能指标，可通过自带工具、第三方工具、API请求或操作日志，如MySQL的慢查询日志。 
        * **日志**：了解应用程序能启用的日志及可获取的性能指标，如从日志获取延迟信息。
        * **版本**：确认是否为最新版本，查看发布说明中性能修复和提升内容。
        * **Bugs**：排查应用程序是否存在性能问题，参考数据库及过往类似情况。
        * **社区**：利用论坛、博客、IRC频道等社区分享的性能优化信息。 
        * **书和会议**：阅读相关书籍，参加会议获取幻灯片和视频资料。 
        * **专家**：找到应用程序公认性能专家及相关材料。
        * **功能图**：找到应用程序内部功能图有助于理解。

<h4 id="Md2uY">5.1.1目标</h4>
+ **性能目标作用**：为性能分析指明方向，避免无目标的“钓鱼探险”。
+ **目标类型**
            + **延时**：降低应用程序响应时间。
            + **吞吐量**：提高应用程序操作率或数据传输率。
            + **资源使用率**：在给定工作负载下高效使用资源。
+ **目标量化示例**：如应用程序平均延时5ms；95%的请求延时在100ms以下；消除超过1000ms的延时异常值；每台服务器最少10000次应用请求/秒；每秒10000次请求下，平均磁盘使用率在50%以下等。确定目标后，识别阻碍目标实现的限制因素，不同目标限制因素不同（如延时可能受磁盘或网络I/O限制，吞吐量可能受CPU限制 ）。 同时要注意，针对吞吐量目标，不同操作对性能或开销影响不同，需识别操作类别。

<h4 id="EDNiP">5.1.2 常见情况的优化</h4>
+ **优化要点**：软件内部复杂，存在多种代码路径和行为。随机优化事倍功半，有效提升应用程序性能的方法是找到对应生产环境工作负载的公用代码路径并优化。
+ **针对不同类型的优化**：CPU 密集型应用程序，需关注频繁占用 CPU 的代码路径；I/O 密集型应用程序，则要查看导致频繁 I/O 的代码路径。可通过分析和剖析应用程序确定这些路径，如借助后续章节介绍的栈跟踪技术。在更高层面理解常见情况需借助应用程序观测工具。

<h4 id="tma6B">5.1.3 观测性</h4>
+ **原理**：如同操作系统通过消除不必要工作提升性能，应用程序也是如此。当以性能为选择基准时，有时性能稍低但观测工具丰富的应用程序，长期运行更具优势。因为丰富的观测工具能让人发现并消除不必要工作，更好地理解和调整运行工作，由此获得的性能收益远超初始 10%的性能差异。

<h4 id="RMWFl">5.1.4 大O标记法</h4>
+ **用途**：用于计算机科学教学，分析算法复杂度，对随着输入数据集增长时算法的执行情况建模，帮助程序员选择更高效算法。
+ **常见标记及示例**
    - **O(1)**：布尔判断。
    - **O(log n)**：顺序队列的二分搜索。 
    - **O(n)**：链表的线性搜索。 
    - **O(n log n)**：快速排序（一般情况）。 
    - **O(n²)**：冒泡排序（一般情况）。 
    - **O(2ⁿ)**：分解质因数（指数增长）。 
    - **O(n!)**：旅行商人问题的穷举法 。
+ **作用**：帮助程序员估计算法速度，判断代码中可大幅改进之处。例如，在搜索 100 项的有序数组时，线性搜索法和二分搜索法差异显著。算法分类展示其增长趋势，如 O(n²)算法在应用程序服务更多用户或处理更多数据时可能导致性能问题，开发人员需采用更高效算法或对输入进行程序切分来解决。

![](https://cdn.nlark.com/yuque/0/2025/png/45533403/1745404725192-58847306-0a78-42cc-888d-4f96ae9e7fd2.png)

<h3 id="Ef8DU">5.2应用程序性能技术</h3>
介绍了提高应用程序性能的常用技术：

1. **选择I/O尺寸**
    - **I/O开销**：执行I/O开销包括初始化缓冲区、系统调用、上下文切换等。“初始化开销”对大小I/O相近，每次I/O传输数据越多效率越高。
    - **策略及影响**：增加I/O尺寸可提高吞吐量，如一次传输128KB比128次传输1KB高效，但不匹配应用需求的大I/O尺寸会降低性能、浪费缓存空间，合适小尺寸I/O可降低I/O延时。
2. **缓存**
    - **原理**：操作系统和应用程序用缓存提高性能，将操作结果存本地缓存备用，如数据库缓冲区高速缓存存查询结果。
    - **部署要点**：确定缓存类型和尺寸，保证缓存一致性（确保查询不返回过期数据），缓存提升读操作性能。
3. **缓冲区**
    - **原理**：为提高写操作性能，数据合并放缓冲区再送下层级，会增加I/O大小，可能增加写延时。
    - **环形缓冲区**：用于组件连续数据传输，大小固定，操作异步，用头指针和尾指针实现。
4. **轮询**
    - **原理及问题**：轮询是循环检查事件状态技术，存在重复检查CPU开销高、事件检查延时高问题。
    - **替代方案**：poll()系统调用基于事件检查文件描述符状态，类似轮询但无其性能负担，接口支持多文件描述符数组，扫描复杂度为O(n) ，Linux的epoll()避免扫描，复杂度O(1) ，基于Solaris系统的事件端口（用port_get(3C) ）类似 。

<h4 id="AFIKC">5.2.5 并发和并行</h4>
1. **并发**
    - **系统层面**：分时系统（如UNIX衍生系统 ）支持程序并发，程序运行时间重叠但不一定同时在CPU执行。
    - **应用程序层面**：可通过多进程、多线程实现，也有基于事件并发（如Node.js ），但单线程/进程的事件并发受单颗CPU限制。
2. **并行**
    - 为利用多处理器优势，应用程序需在多颗CPU上同时运行，多线程（多任务）是首选，能增加CPU工作吞吐量，使I/O并发执行。
    - **多线程优势**：线程共享同一进程地址空间，可直接读写内存，配合同步原语保障数据完整性。
3. **同步原语**
    - **mutex（互斥锁）**：锁持有者操作，其他线程阻塞等待。
    - **自旋锁**：锁持有者操作，其他线程自旋等待，可能浪费CPU资源。 
    - **读写锁**：允许多个读者或一个写者，保证数据完整性。
    - **自适应mutex锁**：自旋锁和mutex锁混合，根据锁持有者位置优化，如Linux的自适应自旋mutex 。
4. **哈希表**
    - 用于优化大量数据结构锁数目。方法有设全局mutex锁（简单但并发访问竞争大 ）、为每个数据结构设mutex锁（减小竞争但有存储开销 ）、锁哈希表（固定锁数目，用哈希算法选择锁 ）。
    - 理想哈希表桶数应大于或等于CPU数目，避免伪共享（false sharing ），可通过填充无用字节解决。

<h4 id="m0Z9e">5.2.6 非阻塞I/O</h4>
1. **传统阻塞I/O问题**
    - 多路并发I/O时，每个阻塞I/O消耗一个线程，创建线程代价大。
    - 频繁短时I/O，上下文切换开销大，增加程序延时。
2. **非阻塞I/O**：异步发起I/O，不阻塞当前线程，线程可执行其他工作，是Node.js关键特性。

<h4 id="B6JlN">5.2.7 处理器绑定</h4>
1. **NUMA环境优势**：在NUMA环境下，进程或线程保持在同一颗CPU上运行有优势。线程执行I/O 后仍能在原CPU运行，提高应用程序内存本地性，减少内存I/O ，提升整体性能。操作系统设计让应用程序线程依附同一颗CPU（CPU亲和性）。
2. **应用程序绑定**：部分应用程序强制与CPU绑定，对某些系统可显著提升性能，但与其他CPU绑定（如设备中断映射 ）冲突时，会损害性能。
3. **多租户场景风险**：云计算OS虚拟化中，应用程序可见所有CPU并选择绑定。若服务器被多租户应用程序共享且都做绑定，即便其他CPU空闲，也可能因绑定的CPU被占用，引发冲突和调度器延时。

<h3 id="BMMR1">5.3 编程语言</h3>
1. **性能优化特性**：编程语言可编译、解释或通过虚拟机执行，“性能优化”常是执行语言软件的特性，如Java HotSpot虚拟机用JIT编译器动态提升性能。解释器和语言虚拟机有性能观测工具，可用于系统性能分析，如高CPU使用率可能由垃圾回收或代码路径问题导致。 
2. **编译语言**
    - **原理**：运行前将程序生成机器指令存于二进制可执行文件，如C和C++ 。编译代码性能高，执行前无需转换，操作系统内核多由C编写。
    - **性能分析**：编译时生成符号表，便于剖析和跟踪CPU执行，栈跟踪可显示代码路径上级调用。
    - **编译器优化**：以gcc编译器为例，优化区间0 - 3 ，3为最大优化。优化选项众多，如 -fomit-frame-pointer ，启用可避免保存等帧指针指令，但可能影响调试，可权衡用 -fno-omit-frame-pointer 编译。性能问题出现时，降低优化级别需谨慎，因编译器输出变化可能影响程序行为。
3. **解释语言**
    - **原理**：运行时翻译语言，增加执行开销，多用于易编程和调试场景，如shell脚本。
    - **性能分析**：缺少专门观测工具时性能分析困难，CPU剖析可展示解释器操作，但难显原始程序函数名和上下文。不过可通过解释器上下文本能跟踪分析，解释语言一般非高性能应用首选。
4. **虚拟机**
    - **原理**：模拟计算机的软件，如Java、Erlang等语言借助虚拟机执行，程序先编译成字节码再由虚拟机执行，使程序具可移植性。
    - **性能优势**：Java HotSpot虚拟机支持JIT编译，提前将字节码编译成机器码，兼具编译后代码性能优势和虚拟机可移植性。
    - **性能分析挑战**：虚拟机是语言类型中最难观测的，程序执行前有多个编译或解释阶段，原始程序信息难获取，性能分析常依赖虚拟机工具集（如DTrace探针 ）和第三方工具。

<h4 id="VNrPN">5.3.4 垃圾回收</h4>
1. **自动内存管理与问题**：部分语言采用自动内存管理，无需显式释放内存，由垃圾收集处理。但存在缺点：
    - **内存增长**：对应用程序内存使用控制不足，垃圾收集不能识别对象释放时机时，内存使用增加，达程序极限或引发系统换页，损害性能。
    - **CPU成本**：垃圾回收（GC）间歇性运行，搜索扫描内存对象，消耗CPU资源，应用程序可用CPU资源减少，内存使用增多时，GC对CPU消耗也增加，甚至可能耗尽CPU 。
    - **延时异常值**：GC执行期间应用程序可能中止，出现高延时响应，具体取决于GC类型（全停、增量、并发 ）。
2. **性能调整与问题排查**：GC是性能调整对象，可降低CPU成本、减少延时异常值。如Java虚拟机提供参数设置GC类型、线程数、堆尺寸等。若调整参数无效，可能是应用程序创建过多垃圾或存在引用泄漏，属开发人员问题。

<h3 id="rTamM">5.4 方法和分析</h3>
1. **应用程序分析调整方法**：介绍应用程序分析和调整方法及相关工具，这些方法可单独或组合使用，建议按序尝试。除通用方法外，还可针对特定应用程序和编程语言定制分析技术，需考虑应用程序逻辑行为、已知问题和性能收益。具体方法如下表：

| 方法 | 类型 |
| --- | --- |
| 线程状态分析 | 观测分析 |
| CPU剖析 | 观测分析 |
| 系统调用分析 | 观测分析 |
| I/O剖析 | 观测分析 |
| 工作负载特征归纳 | 观测分析、容量分析 |
| USE方法 | 观测分析 |
| 向下挖掘分析法 | 观测分析 |
| 锁分析 | 观测分析 |
| 静态性能调优 | 观测分析、调优 |
| 2. **线程状态分析**：目的是分辨应用程序线程时间去向，快速解决部分问题并为其他问题研究指引方向，常将应用程序时间划分为有实际意义的状态。 |  |


<h3 id="JmRAe">5.4.1 线程状态分析</h3>
1. **分析目的**：分辨应用程序线程时间去向，快速解决部分问题并为其他问题研究指引方向。
2. **线程状态划分**
    - **两种状态**
        * **on - CPU（执行）**：线程在CPU上执行。
        * **off - CPU（等待）**：线程等待下一轮上CPU，或等待I/O、锁、换页、工作等。若大量时间花在CPU上，可通过CPU剖析解释；若时间未花在CPU上，需其他方法分析。
    - **六种状态**
        * **执行**：在CPU上执行，可进一步细分用户态和内核态执行，通过剖析做CPU资源消耗分析，确定代码路径CPU消耗情况。
        * **可运行**：等待轮到上CPU，耗时意味着应用程序需更多CPU资源，需检查系统CPU负载和对应用程序的CPU限制。
        * **匿名换页**：可运行但因等待匿名页受阻，由应用程序缺少可用主存引起，需检查系统内存使用和对应用程序的内存限制。
        * **睡眠**：等待网络、块设备、数据/文本页换入等I/O，需分析阻塞应用程序的资源。
        * **锁**：等待获取同步锁（等待其他线程），需识别锁和持锁线程，确定持锁长时间的原因，一般由熟悉应用程序和锁机制的人员处理。 
        * **空闲**：等待工作。睡眠和锁状态时间常可能是空闲时间，看到大量此类时间需深入检查是否真空闲。
3. **性能提升与后续研究**：减少前五项状态时间可提升性能、增加空闲时间，降低应用程序请求延时、提高负载应对能力。确定线程在前五个状态花费时间后，可按上述各状态对应方法进一步研究。后续章节将讨论在Linux和基于Solaris系统上线程状态的测量方法及相关工具技术。



**<u><font style="color:#601BDE;">Linux系统线程状态测量方法</font></u>**

1. **执行时间**：可通过top(1)将执行时间汇报为%CPU 。
2. **可运行线程**：内核的schedstat功能跟踪可运行线程，信息显示在/proc/*/schedstat 中；perf sched工具提供了解可运行线程和等待线程耗时指标。 
3. **匿名换页时间**：若内核启用延时核算特性，可测量等待匿名换页（交换swapping ）时间，虽无常用工具显示相关状态，但内核文档示例程序getdelay.c可实现，也可用DTrace或SystemTap等跟踪工具。 
4. **睡眠时间**：可借助其他工具大致估计，如pidstat -d判断进程执行磁盘I/O还是睡眠；启用延时核算和I/O核算特性，可获取块I/O阻塞时间，iotop(1)也能观察；用DTrace或SystemTap等跟踪工具调查其他阻塞原因；应用程序可添加监测点精确跟踪I/O执行时间；若线程长时间睡眠，可用pstack(1)调查，但执行时可能使目标短暂停顿。 
5. **锁时间**：使用跟踪工具研究。

**<font style="color:#601BDE;">So</font>****<u><font style="color:#601BDE;">laris系统线程状态测量方法</font></u>**

基于Solaris系统通过第4章介绍的微状态核算统计，使用prstat(1M)查看线程状态 。示例中从USR到LAT八列是微状态核算线程状态，按线程时间划分百分比，总和为100%，对应关系如下：

+ **执行**：USR+SYS
+ **可运行**：LAT
+ **匿名换页**：DFL
+ **睡眠**：SLP
+ **锁**：LCK
+ **空闲**：SLP+LCK

线程离开CPU时，用DTrace检查栈跟踪判断等待内容以获取空闲时间；线程长时间睡眠时，可用pstack(1) ，但需注意其会使目标短暂停顿。

<h3 id="AEyRL">5.4.2 CPU剖析</h3>
1. **剖析目标与技术**：目标是判断应用程序如何消耗CPU资源。有效技术是对CPU上用户栈跟踪采样并关联结果，栈跟踪可从高层和底层揭示应用程序消耗CPU原因。
2. **数据可视化**：栈跟踪采样输出量大，可使用火焰图做可视化快速了解数据，相关内容在第6章介绍。
3. **函数采样**：除栈跟踪采样外，还可对当前运行函数单独采样，能识别应用程序使用CPU但输出少的原因。文中给出DTrace使用例子，可统计特定函数在CPU上的次数，也可研究函数调用数。 
4. **解释语言和虚拟机语言剖析难点与解决方法**：针对解释语言和虚拟机语言的CPU使用研究困难，因执行软件与原始程序无简单映射关系。解决方法依语言环境而定，可开启调试特性或用第三方工具。文中以DTrace的ustack helper为例，展示了对Java CPU栈的采样，输出显示了调用频繁的栈、JVM相关内容及Java自身栈（用粗体高亮），明确了与CPU代码路径相关的类和方法 。关于检查应用程序CPU使用的其他方法和工具在第6章介绍。

<h3 id="hjg6h">5.4.3 系统调用分析</h3>
1. **线程状态与分析目标**：基于系统调用执行研究线程状态，分为执行（CPU上用户模式）和系统调用（内核模式运行或等待，包括I/O、锁等类型 ）。目标是找出系统调用时间花费处、类型及使用原因。研究执行状态可用CPU剖析方法。
2. **断点跟踪**：传统方式是设置系统调用入口和返回断点，对系统调用频繁的应用程序性能影响大，在性能需求允许时可短时间跟踪识别当前调用类型。
3. **Linux系统工具**
    - **strace**：用strace(1)命令，如`strace -ttt -T -p 1884` ，`-ttt`打印UNIX时间戳，`-T`输出系统调用用时，`-p`跟踪指定PID进程。它将系统调用内容翻译成可读形式，`-c`选项用于系统调用统计总结，显示系统CPU时间占比、总时间、每次调用平均时间、调用次数、系统调用名等。但对高频系统调用和追踪函数调用开销高，不适合多数生产环境。通过dd(1)命令测试示例，展示了使用strace会使运行时间增加、吞吐率下降。
4. **Solaris系统工具**
    - **truss**：在Solaris系统上用truss(1)命令，如`truss -dE -p 81573` ，`-d`打印命令启动后秒数时间戳，`-E`打印系统调用用时，`-p`跟踪指定PID进程。输出将系统调用参数翻译为可读形式，时间戳解析度到0.1ms 。也可用`-c`选项做统计总结，显示系统调用CPU时间和调用次数，还能用`-u`选项对用户级别函数调用动态跟踪。
5. **缓冲跟踪与DTrace**
    - 缓冲跟踪可将监测数据缓冲在内核，与断点跟踪不同，断点跟踪会在断点中断目标程序执行。
    - DTrace提供缓冲跟踪和聚合变量方式减少开销，可编写定制程序分析系统调用。文中展示多个DTrace单命令示例：
        * 跟踪进程信号，显示信号发送源和接收者PID、进程名及信号编号。
        * 对名为“postgres”进程的系统调用计数。 
        * 度量PostgreSQL里read()系统调用执行时间（延时） 。
        * 更多表示系统调用时间的DTrace脚本示例，如dtruss（类似truss ）、execsnoop（跟踪exec() ）、opensnoop（跟踪open() ）、procsystime（统计系统调用时间） 。通过这些脚本可解决很多性能问题，属于工作负载特征归纳（应用程序的系统调用 ）。以execsnoop -v运行在云系统上的结果为例，展示高频短时进程对CPU资源的消耗及对其他应用程序的干扰。

<h3 id="nLnrf">5.4.4 I/O剖析</h3>
1. **剖析原理与方法**：与CPU剖析类似，I/O剖析用于判断I/O相关系统调用执行的原因和方式，可借助DTrace检查用户栈里系统调用的栈跟踪。 
2. **示例**：文中给出跟踪PostgreSQL的read()系统调用的DTrace单行命令示例，收集用户级栈跟踪并聚合。输出显示用户级栈及栈调用计数值，包含应用程序内部函数名，如第一个栈中的XLogRead可能与数据库日志相关，第二个栈中的PgstatCollectorMain.isra可能与监控有关，利用栈跟踪可明确执行系统调用的原因。 
3. **工作负载特征归纳属性**：研究工作负载特征归纳法的其他属性也有帮助，包括：
    - **Who**：进程ID、用户名。
    - **What**：I/O系统调用对象（如文件系统或套接字 ）、I/O尺寸、IOPS、吞吐量（B/s ）等其他属性。 
    - **How**：IOPS随时间的变化。
4. **性能结果研究**：除工作负载外，系统调用延时等性能结果也可用之前提到的方法研究。

<h3 id="oSLLl">5.4.5 工作负载特征归纳</h3>
应用程序向CPU、内存、文件系统、磁盘、网络等系统资源及操作系统施加负载，可利用第2章介绍的工作负载特征归纳法研究，后续章节也会讨论。此外，发送给应用程序的工作负载也可研究，重点关注应用程序服务操作和固有属性，这些可能是性能监视关键指标，用于容量规划。 

<h3 id="p3q20">5.4.6 USE方法</h3>
1. **原理**：如第2章所述，USE方法检查硬件资源的使用率、饱和及错误情况，找出瓶颈资源，解决应用程序性能问题。该方法也适用于软件资源（取决于应用程序 ），需对软件资源的使用率、饱和和错误指标进行考量。
2. **指标定义示例**
    - **工作线程池示例**：将处理请求的工作线程池视为资源，使用率指忙处理请求的线程平均数目占比；饱和度指请求队列平均长度；错误指请求被拒绝或失败情况。需找到测量这些指标的方法，可由应用程序提供或借助动态跟踪等工具添加测量。队列系统还可用排队论研究。
    - **文件描述符示例**：使用率是使用中的文件描述符数量与上限的百分比；饱和度取决于操作系统行为，如线程等待文件描述符分配被阻塞时，指标为被阻塞线程数目；错误指分配失败等情况。对应用程序每个组件重复此过程，忽略意义不大的指标，可在使用其他方法前制定检查清单。

<h3 id="aS18t">5.4.7 向下挖掘法</h3>
从检查应用程序服务操作入手，深入应用程序内部，查看其执行方式，对于I/O可深入系统库、系统调用甚至内核。动态跟踪工具（DTrace、SystemTap、perf(1) ）可用于研究，部分语言有自身工具集更适用。如Linux上的ltrace、Solaris上的apptrace(1)（基于DTrace ）可专门调查库调用。 

<h3 id="ZxYbd">5.4.8 锁分析</h3>
1. **分析要点**：多线程应用程序中，锁可能阻碍并行和扩展性，分析锁可通过检查竞争和过长持锁时间，识别当前是否有问题，以及识别锁名和使用锁的代码路径。
2. **分析方法**：虽有专门工具，但CPU剖析有时可解决问题，如用栈跟踪的CPU剖析可识别自旋锁、自适应mutex锁竞争情况，但线程等待时阻塞或睡眠会影响信息完整性。
3. **Solaris系统工具及示例**：基于Solaris系统有用户级锁分析工具plockstat(1M) 、内核级锁分析工具lockstat(1M) 。lockstat(1M)示例中，用五层栈（-s5 ）跟踪竞争事件（-C ），借助协同进程（sleep(1) ）提供5s执行超时，输出重定向到文件。输出包含自适应mutex锁自旋时间、竞争事件及锁名、栈跟踪时间分布图等，如zfs_range_unlock竞争次数最多。跟踪锁会增加开销，基于DTrace工具可减少开销，也可固定频率做CPU剖析确定锁问题。

<h3 id="E4mlL">5.4.9 静态性能调优</h3>
静态性能调优聚焦环境配置问题，针对应用程序性能，可从以下方面检查静态配置：

1. **应用程序版本**：运行的应用程序版本，有无更新版本，发布说明是否提及性能提升。
2. **性能问题与数据库**：应用程序已知性能问题，是否有可搜索的bug数据库。
3. **配置情况**
    - 应用程序的具体配置方式。
    - 配置或调整与默认值不同的原因（基于测量分析还是猜想 ）。
4. **缓存相关**：是否使用对象缓存及缓存大小。
5. **并发配置**：应用程序是否并发运行及并发配置（如线程池大小 ）。
6. **运行模式**：是否在特定模式（如调试模式 ）下运行，该模式对性能的影响。 
7. **系统库相关**：应用程序使用的系统库及其版本。
8. **内存分配器**：应用程序采用的内存分配器。
9. **内存堆配置**：是否使用大页面做堆。
10. **编译相关**：应用程序是否编译，编译器版本、选项、优化情况，是否为64位。
11. **错误及运行模式**：应用程序是否遇错，错误后是否在降级模式运行。
12. **系统限制与资源控制**：有无系统设置限制，对CPU、内存、文件系统、磁盘、网络等资源的控制情况（云计算中常见 ）。通过回答这些问题，可发现被忽略的配置选项。

<h2 id="nHBFZ">第六章 CPU内容整理</h2>
**章节概述**：本章为第6章，聚焦CPU。CPU是系统性能分析首要目标，现代系统多为多核，通过内核调度器共享资源，资源不足时任务排队致性能下降。可通过检查CPU用量优化性能。本章分五部分：背景、架构、方法、分析、调优，前三部分是基础，后两部分展示在Linux和Solaris系统应用，还介绍内存I/O对CPU性能影响 。

<h3 id="RiXHi">  
6.1术语</h3>
+ **处理器**：插在系统插槽或处理器板上的物理芯片，以核或硬件线程方式包含一块或多块CPU。
+ **核**：多核处理器上独立CPU实例，是处理器扩展方式，即芯片级多处理（CMP）。 
+ **硬件线程**：支持在一个核上同时执行多个线程（如Intel超线程技术）的CPU架构，每个线程是独立CPU实例，也叫多线程。 
+ **CPU指令**：源于指令集的单个CPU操作，用于算术、内存I/O及逻辑控制。 
+ **逻辑CPU**：也称虚拟处理器，是操作系统CPU实例（可调度的CPU实体），可通过硬件线程（虚拟核）、核或单核处理器实现。 
+ **调度器**：将CPU分配给线程运行的内核子系统。 
+ **运行队列**：等待CPU服务的可运行线程队列，在Solaris上常称分发器队列。 此外，本章会穿插其他术语，术语表含CPU、CPU周期和栈等基本名词，也可参考第2、3章术语部分。

<h3 id="tYlLo">6.2 模型</h3>
<h4 id="gBPyU">6.2.1 CPU架构</h4>
通过图6.1展示CPU架构示例，单个处理器内有四个核和八个硬件线程。物理架构左侧呈现，右侧是操作系统视角。每个硬件线程可按逻辑CPU寻址，此处理器看似有八块CPU 。操作系统掌握哪些CPU在同一核上的信息，有助于提升调度质量。

![](https://cdn.nlark.com/yuque/0/2025/png/45533403/1745407117575-ebbb9582-baaf-406b-8b79-c8f3c66c73e4.png)

<h4 id="gPohR">6.2.2 CPU内存缓存</h4>
为提升内存I/O性能，处理器设多种硬件缓存。图6.2呈现缓存大小关系，缓存越小越快且越靠近CPU 。缓存存在与否及位置取决于处理器类型，早期处理器缓存层次少。

![](https://cdn.nlark.com/yuque/0/2025/png/45533403/1745407127093-a148d5a8-d062-4b53-9d3f-563749b6e1d7.png)

<h4 id="vJBJt">6.2.3 CPU运行队列</h4>
图6.3展示内核调度器管理的CPU运行队列。排队和就绪运行的软件线程数量是重要性能指标，反映CPU饱和度 。等待CPU运行的时间称调度器延时。多处理器系统中，内核通常为每个CPU设运行队列，让线程尽量在同一队列，利于利用CPU缓存（热度缓存），该选择CPU的方法是CPU关联 。在NUMA系统中，可提高内存本地性、系统性能，还能避免队列操作的线程同步开销（mutex锁） ，若运行队列全局共享会影响扩展性。

![](https://cdn.nlark.com/yuque/0/2025/png/45533403/1745407139199-7601efe9-b446-4019-b527-026d23eb7208.png)

<h3 id="PQARi">6.3 概念</h3>
<h4 id="Ql4zK">6.3.1 时钟频率</h4>
时钟是驱动处理器逻辑的数字信号，CPU指令执行需一个或多个时钟周期（CPU周期） 。如5GHz的CPU每秒运行五十亿个时钟周期。部分处理器可变频，升频提性能，降频减能耗，可依操作系统请求或自动调整 。时钟频率虽常作处理器营销指标，但高频率不一定提升性能，若CPU多在等待内存访问，执行速度快也难提高指令执行效能与负载吞吐量。

<h4 id="RzM5q">6.3.2 指令</h4>
CPU执行指令集中指令，指令执行步骤由功能单元处理，包括指令预取、解码、执行、内存访问（可选）、寄存器写回（可选） 。每步至少需一个时钟周期，内存访问最慢，常需数十个时钟周期，期间指令执行停滞，产生停滞周期，凸显CPU缓存重要性。

<h4 id="laiJD">6.3.3 指令流水线</h4>
一种CPU架构，通过同时执行不同指令的不同部分，实现多指令同时执行，类似工厂组装线提高吞吐量 。无流水线时，指令步骤依次执行需多个周期，流水线可激活多个功能单元，理想情况一个周期完成一条指令执行。

<h4 id="cu5Gu">6.3.4 指令宽度</h4>
同类型功能单元可多个并存，使每个时钟周期处理更多指令，此CPU架构为超标量，常与流水线结合提高指令吞吐量 。指令宽度指同时处理的目标指令数量，现代处理器一般为宽度3或4，即每周期最多完成3 - 4个指令，实现方式因处理器而异。

<h4 id="Y7jao">6.3.5 CPI，IPC</h4>
每指令周期数（CPI）是重要高级指标，描述CPU时钟周期使用情况及理解CPU使用率本质，每周期指令数（IPC）是其倒数 。CPI高表示CPU常停滞（多在访问内存），CPI低则指令吞吐量高，是性能调优方向 。内存访问密集负载会提高CPI，即便用更快内存或CPU，若内存I/O耗时不变，也难达预期性能 。CPI与处理器及功能有关，可实验得出，如高CPI负载下CPI可达10或更高，低CPI负载下可低于1（得益于指令流水线和宽度技术） 。需注意，CPI代表指令处理效率，不代表指令本身效率，低效循环操作寄存器虽可能降低CPI，但会增加CPU使用和利用度。

<h4 id="m3G6L">6.3.6 使用率</h4>
CPU使用率是一段时间内CPU实例忙于执行工作的时间比例，以百分比表示 。可测量CPU未运行内核空闲线程的时间，期间CPU可能运行应用程序线程等 。高使用率不代表有问题，仅表明系统在工作，有人视其为ROI指示器 。内核支持优先级、抢占和分时共享，高使用率下性能不一定显著下降 。测量包含符合条件活动的时钟周期（含内存停滞周期） ，常分成内核时间和用户时间两个指标。

<h4 id="SIUZB">6.3.7 用户时间/内核时间</h4>
用户时间指CPU执行用户态应用程序代码的时间，内核时间指执行内核态代码的时间（含系统调用、内核线程和中断时间） 。用户时间和内核时间之比能揭示负载类型，计算密集型应用用户/内核时间比接近99/1 ，I/O密集型应用（如Web服务器）约为70/30 ，具体比例依多种因素而定。

<h4 id="N0PNT">6.3.8 饱和度</h4>
100%使用率的CPU处于饱和状态，线程会遇调度器延时，因需等待在CPU上运行，降低总体性能 。另一种饱和度形式与CPU资源控制有关，在云计算环境中，即使CPU未达100%使用，但达到控制上限，可运行线程也需等待 。饱和的CPU问题不如其他资源严重，因高优先级工作可抢占当前线程。

<h4 id="qjyu8">6.3.9 抢占</h4>
允许更高优先级的线程抢占当前正在运行的线程并开始执行，可节省高优先级工作的运行队列延时时间，提高性能 。

<h4 id="N83lR">6.3.10 优先级反转</h4>
低优先级线程拥有资源，阻碍高优先级线程运行的情况，降低高优先级工作性能 。Solaris内核实现优先级继承机制避免此问题，Linux从2.6.18起提供支持优先级继承的用户态mutex用于实时负载 。

<h4 id="YJ9LN">6.3.11 多进程，多线程</h4>
多数处理器支持多个CPU，应用程序利用此功能需开启不同执行线程并发运行，在多CPU上扩展技术分为多进程和多线程 。Linux可使用这两种模型，由任务实现 。多进程开发较简单（用fork() ），内存开销大（不同地址空间），CPU开销大（fork()/exit()及MMU管理开销），通信通过IPC（有CPU开销），内存使用有冗余但进程结束可返还内存；多线程使用线程API ，内存开销小，CPU开销小（API调用），通信直接（共享内存加同步原语），内存使用可能因CPU竞争和碎片化有问题 。多线程一般被认为优于多进程，但开发难度大 。重要的是创建足够进程或线程占据预期数量CPU以最大化性能，不过部分应用在更少CPU上可能因线程同步和内存本地性问题跑得更快。

<h4 id="RxGnO">6.3.12 字长</h4>
处理器按最大字长（32位或64位 ）设计，字长表示整数大小、寄存器宽度、地址空间大小和数据通路宽度 。更宽字长一般性能更好，但可能在某些数据类型下因未使用位产生额外内存开销，数据大小也会因指针增大而增加，导致更多内存I/O 。对64位x86架构，寄存器增加和有效调用约定抵消开销，64位应用比32位版本快 。处理器和操作系统支持多种字长，软件编译成较小字长可能成功运行但速度慢。

<h4 id="kSdTx">6.3.13 编译器优化</h4>
应用程序在CPU上的运行时间可通过编译器选项（含字长设置）大幅改进 。编译器不断更新以利用最新CPU指令集及其他优化，使用新编译器有时可显著提高应用程序性能，第5章有详细叙述。

<h3 id="Q3JKz">6.4 架构</h3>
<h4 id="gaMgs">6.4.1 硬件</h4>
+ **处理器**：通用双核处理器组件中，控制器（控制逻辑）是核心，负责指令预取、解码、执行管理及结果存储 。处理器还包含共享浮点单元、（可选）共享三级缓存，其他相关组件有P - cache（预取缓存，每CPU一个 ）、W - cache（写缓存，每CPU一个 ）、时钟（CPU时钟信号生成器或外部提供 ）、时间戳计数器（由时钟递增实现高精度计时 ）、微代码ROM（快速将指令转化为电路信号 ）、温度传感器（用于温度监控，部分处理器如Intel睿频加速技术会以此为单核动态超频输入 ）、网络接口（高性能芯片集成 ）。

![](https://cdn.nlark.com/yuque/0/2025/png/45533403/1745408260909-d7fd0284-348f-474d-91b8-1978f35eae00.png)

+ **CPU缓存**：多种硬件缓存可集成在处理器内或外置，通过更快内存缓存读写提升内存性能 。包括一级指令缓存（I![image](https://cdn.nlark.com/yuque/__latex/e215d20b82e4ba212677c81c600bec56.svg) ）、转译后备缓冲器（TLB ）、二级缓存（E![image](https://cdn.nlark.com/yuque/__latex/d4aebbb81979f23d2710e30c697476ab.svg)”表述 ）、三级缓存（可选 ） 。不同处理器的缓存情况依类型和型号而异，随时间推进缓存数量和大小不断增加。

![](https://cdn.nlark.com/yuque/0/2025/png/45533403/1745408273172-856001dd-83f0-4f56-9abe-f035e7f7f515.png)![](https://cdn.nlark.com/yuque/0/2025/png/45533403/1745408294965-c11c9e67-b7c1-4191-aa26-3d5f0c012643.png)

<h4 id="H7bUa">延时</h4>
多级缓存用于平衡大小和延时，一级缓存访问时间通常为几个CPU时钟周期，二级缓存约几十个时钟周期 。主存访问耗时约60ns（4GHz处理器约240个周期 ），MMU地址转译会增加额外延时 。可通过微型基准测试（如LMbench ）测出CPU缓存延时特征，图6.7展示了Intel Xeon E5620 2.4 GHz处理器在不同内存范围下的内存访问延时情况，当某级缓存被填满，访问延时会增加到下一级（更慢）缓存水平。

![](https://cdn.nlark.com/yuque/0/2025/png/45533403/1745408419265-ad1eb3f3-25bc-4a63-919f-dac8b8b0362b.png)

<h4 id="XcX7V">相联性</h4>
相联性是定位缓存新条目的特性，类型包括：

+ **全关联**：缓存可在任意位置放置新条目，如LRU算法可剔除缓存中最旧条目 。
+ **直接映射**：每个条目在缓存中只有一个有效位置，通过对内存地址哈希确定缓存地址 。 
+ **组关联**：先通过映射（如哈希）定位缓存中一组地址，再用算法（如LRU）选择，如四路组关联将地址映射到四个可能位置并挑选合适的 。CPU缓存常采用组关联，平衡全关联开销大与直接映射命中率低的问题。

<h4 id="hMtzj">缓存行</h4>
缓存行大小是CPU缓存特征，是存储和传输的字节数量单位，可提高内存吞吐量 ，x86处理器典型缓存行大小为64字节 ，编译器和程序员优化性能时会考虑此参数。

<h4 id="x68sa">缓存一致性</h4>
内存可能同时被不同处理器的多个CPU缓存，当一个CPU修改内存，需确保其他缓存知晓其拷贝失效并丢弃，以保证后续读取新修改内容，此为缓存一致性 ，是设计可扩展多处理器系统的挑战之一，因内存频繁修改。

<h4 id="JzsZ9">MMU（内存管理单元）</h4>
负责虚拟地址到物理地址转换 。图6.8展示普通MMU及相关CPU缓存类型，通过芯片上的TLB缓存地址转换 。主存（DRAM）中的转换表（页表）处理缓存未命中情况，由MMU硬件直接读取 。老处理器通过软件遍历页表处理TLB未命中，维护转译存储缓冲区（TSB）；新处理器可通过硬件响应TLB未命中，降低开销。

![](https://cdn.nlark.com/yuque/0/2025/png/45533403/1745408434854-ac23624c-68b5-48ad-87cc-c41cccfe5675.png)

<h4 id="Zygl5">硬件 - 互联部分</h4>
互联方式

在多处理器架构中，处理器通过共享系统总线（如早期Intel处理器使用的前端总线 ）或专用互联（如Intel的快速通道互联QPI、AMD的HyperTransport HT ）连接 。互联不仅连接处理器，还可连接I/O控制器等组件 。这与系统内存架构（统一内存访问UMA或非统一内存访问NUMA ）相关，第7章会讨论。 

共享系统总线问题

使用共享系统总线（前端总线）时，随着处理器数量增加，会因共享总线资源出现扩展性问题 。

![](https://cdn.nlark.com/yuque/0/2025/png/45533403/1745408539911-73276615-46dd-4d1a-85c3-78218c191a56.png)

专用互联优势

现代服务器多为多处理器且采用NUMA架构，使用专用CPU互联技术 。处理器间的私有连接（如QPI ）提供无竞争访问和更高带宽 。以Intel FSB（2007年，传输率1.6 GT/s ，宽度8字节，带宽12.8 GB/s ）和QPI（2008年，传输率6.4 GT/s ，宽度2字节，带宽25.6 GB/s ）为例，QPI速率是FSB两倍，在时钟两个边缘传输数据，使数据传输率加倍 。此外，处理器还有用于核间通信的内部互联 。互联通常设计为高带宽，避免成为系统瓶颈，若成为瓶颈，涉及互联的CPU指令（如远程内存I/O ）会停滞，表现为CPI上升，可通过CPU性能计数器分析CPU指令、周期、CPI、停滞周期和内存I/O等情况。

![](https://cdn.nlark.com/yuque/0/2025/png/45533403/1745409074531-5f2fd957-c08b-46bd-99fa-3b16ab1d8195.png)

**CPU性能计数器**

**定义与别名**：CPU性能计数器（CPC），也叫性能测量点计数器（PIC）、性能监控单元（PMU）、硬件事件和性能监控事件 ，是能计数低级CPU活动的处理器寄存器。  
**包含计数器类型**

+ **CPU周期**：含停滞周期及类型。
+ **CPU指令**：统计执行过（引退）的指令。
+ **缓存访问**：涉及一级、二级、三级缓存的命中与未命中情况。 
+ **浮点单元**：记录操作情况。
+ **内存I/O**：涵盖读、写及停滞周期。
+ **资源I/O**：包含读、写及停滞周期。   
**寄存器情况**：每个CPU通常有2 - 8个可编程记录事件的寄存器，具体取决于处理器类型和型号，可在处理器手册中查阅 。以Intel P6家族处理器为例，通过四个型号特定寄存器（MSR）提供性能寄存器，两个为只读计数器，两个用于对计数器编程（事件选择MSR ，可读可写 ），性能计数器为40b寄存器，事件选择MSR是32b 。事件选择确定计数事件类型，UMASK确定子类型或子类型组 ，OS和USR位选择计数器在内核模式（OS）或用户模式（USR）递增，CMASK设置事件阈值，达到阈值后计数器递增 。  
**事件示例**：Intel处理器手册列出可通过事件选择和UMASK值计数的众多事件，表6.4展示部分示例，如DATA_MEM_REFS记录从所有类型内存的读取和存储（不包括I/O及非内存访问 ）、DCU_MISS_OUSTANDING统计DCU未命中期间的周期权重数等 。  
**新处理器特性**：新处理器（如Intel Sandy Bridge家族 ）有更多类型计数器和寄存器，每个硬件线程有三个固定和四个可编程计数器，每个核还有八个额外可编程计数器（“通用” ），读取时为48位 。  
**标准接口**：不同厂商性能处理器不同，处理器应用程序员接口（PAPI）提供一致接口，为计数器类型取通用名字，如PAPI_tot_cyc代表总周期数。

![](https://cdn.nlark.com/yuque/0/2025/png/45533403/1745409490715-da209582-365c-4358-b81b-b242f74149f6.png)

![](https://cdn.nlark.com/yuque/0/2025/png/45533403/1745409523443-5302a147-880e-4c0b-b5a6-cf4df4bcc504.png)

![](https://cdn.nlark.com/yuque/0/2025/png/45533403/1745409532641-ef6e31db-8724-48a9-a6e2-688ceb34893e.png)

<h4 id="YI19U">6.4.2 软件</h4>
**支撑CPU的内核软件组成**：包括调度器、调度器类和空闲线程 。

![](https://cdn.nlark.com/yuque/0/2025/png/45533403/1745409695584-29987a06-bda7-4c88-8724-b18b73fca84c.png)  
**调度器功能**

+ **分时**：在可运行线程间分配任务，优先执行最高优先级任务 。
+ **抢占**：高优先级线程变为可运行状态时，能抢占当前运行线程，使其马上开始运行 。
+ **负载均衡**：将可运行线程移至空闲或较不繁忙的CPU队列 。 每个优先级有自己的运行队列，便于调度器管理CPU运行队列及线程 。  
**Linux内核调度情况**
+ **实现方式**：分时通过系统时钟中断调用scheduler_tick()实现，该函数调用调度器类管理优先级和CPU时间片到期事件 。线程状态变为可运行时触发抢占，调用调度类函数check_preempt_curr() ，线程切换由__schedule()管理，后者通过pick_next_task()选择最高优先级线程 ，负载均衡由load_balance()函数负责 。
+ **调度器类**
    - **RT**：为实时类负载提供固定高优先级，支持用户和内核级别抢占，允许RT任务短延时分发，优先级范围0 - 99（MAX_RT_PRIO-1 ） 。
    - **O(1)**：在Linux 2.6作为默认用户进程分时调度器引入，名字源于算法复杂度O(1) ，相对于CPU消耗型线程，能动态提高I/O消耗型线程优先级，降低交互和I/O负载延时 。 
    - **CFS**：Linux 2.6.23引入完全公平调度作为默认用户进程分时调度器，使用红黑树管理任务，以任务CPU时间作为键值，使CPU少量消费者更易被找到，提高交互和I/O消耗型负载性能 。
+ **调度器策略**：用户级进程可通过调用sched_setscheduler()设置调度器策略调整调度类行为 。RT类支持SCHED_RR（轮转调度，线程用完时间片移至同优先级运行队列尾部 ）和SCHED_FIFO（先进先出调度，运行队列头线程直到其自愿退出或更高优先级线程抵达 ）策略；CFS类支持SCHED_NORMAL（以前称SCHED_OTHER ，分时调度，调度器根据调度类动态调整优先级 ）和SCHED_BATCH（类似SCHED_NORMAL ，期望线程为CPU消耗型，不打断I/O消耗型交互工作 ）策略 。 
+ ![](https://cdn.nlark.com/yuque/0/2025/png/45533403/1745409710255-6b158ba8-5779-4fc6-b3ca-e822d0f39e88.png)  
**Solaris内核调度情况**
+ **实现方式**：分时由clock()驱动，调用包括ts_tick()在内的调度类函数检查时间片是否到期 。线程超分配时间，优先级降低，允许其他线程抢占 。preempt()处理用户线程抢占，kpreempt()处理内核线程抢占 。switch()函数处理线程上下文切换，并调用分发器函数寻找替代运行的最佳可运行线程 ，负载均衡调用类似函数寻找其他CPU分发器（运行队列）的空闲线程 。
+ **调度器类**：管理可运行线程行为，包括优先级、CPU时间分配及时间片长度（时间量子 ） ，还可通过调度策略对同一优先级线程进行控制 。用户线程优先级受用户定义的nice值影响，可降低不重要工作优先级 。需注意，Linux和Solaris内核优先级范围相反，Linux继承原来UNIX把较小数字用于高优先级线程的方式 。   
**空闲线程**：当没有线程可运行时，空闲任务（空闲线程）运行，直至有其他线程可运行 。

**Solaris内核调度相关内容**

**调度器类**

+ **RT（实时调度）**：为实时负载提供固定高优先级，可抢占除中断服务外的其他工作，确保应用程序响应时间可确定，适用于实时工作负载。
+ **SYS（系统调度）**：内核线程的高优先级调度类，线程有固定优先级，可按需持续执行，直至被RT或中断抢占 。
+ **TS（分时调度）**：用户进程默认调度类，依据最近CPU用量动态调整优先级和时间片 。CPU消耗型负载因使用时间片优先级降低、时间片增加，以低优先级运行；I/O消耗型负载在自愿切换上下文时以高优先级运行，性能不受长时间占用CPU任务影响，nice值也会对其产生影响 。
+ **IA（交互调度）**：与TS类似，但默认优先级稍高，现较少使用，曾用于提高图形X会话响应度 。
+ **FX（固定调度）**：设定固定优先级的进程调度类，全局优先级范围与TS相同（0 - 59 ）。
+ **FSS（公平共享调度）**：基于共享值管理一组（项目或区域）进程的CPU用量，允许组公平分享CPU资源，用量基于线程或进程数量，进程组可消耗的CPU资源大小与份额值及系统繁忙份额总数相关，常用于云计算，与TS有相似的全局优先级范围（0 - 59 ）和固定时间片 。
+ **SYSIDLE**：系统工作调度类，为特定系统线程（如ZFS事务组刷新进程 ）准备，可指定目标工作周期（CPU时间占运行时间比率 ），符合条件时调度线程，避免内核线程长时间运行，否则会反向睡眠 。
+ **中断**：调度中断线程，优先级为159 + IPL（见第3章3.2.3节 ） 。  
**调度策略**：基于Solaris的系统支持通过sched_setscheduler()设置调度策略，如SCHED_FIFO、SCHED_RR和SCHED_OTHER（分时 ） 。  
**空闲线程**：内核“空闲”线程在无其他可运行线程时运行，以最低优先级执行，常通知处理器停止CPU执行（停止指令 ）或减速以节省资源，下次硬件中断时唤醒 。

****

**NUMA分组**

+ 在Linux系统，内核感知NUMA可优化调度和内存分配，自动检测并建立本地化的CPU和内存资源组，依据NUMA架构拓扑分组，从预估内存访问开销的根开始 。
+ 在Solaris系统，本地组（lgrps ）以根组为起点，系统管理员可手动分组，将进程绑定到一个或多个CPU，也可创建CPU组并分配进程 。  
**处理器资源感知**：与NUMA不同，内核可理解CPU资源拓扑结构，在Solaris系统中由处理器组实现，用于资源管理和负载均衡，做出更好的调度决策。

<h3 id="RarEF">6.5 CPU 分析和调优方法</h3>
本节介绍 CPU 分析和调优的多种方法，这些方法可单独或结合使用，建议使用顺序为性能监控、USE 方法、剖析、微型基准测试和静态分析 ，后续 6.6 节将展示如何用操作系统工具应用这些策略。表 6.5 对主要方法分类总结：

| 方法 | 类型 |
| --- | --- |
| 工具法 | 观察分析 |
| USE 方法 | 观察分析、容量规划 |
| 负载特征归纳 | 观察分析 |
| 剖析 | 观察分析 |
| 周期分析 | 观察分析 |
| 性能监控 | 观察分析、容量规划 |
| 静态性能调优 | 观察分析、容量规划 |
| 优先级调优 | 调优 |
| 资源控制 | 调优 |
| CPU 绑定 | 调优 |
| 微型基准测试 | 实验分析 |
| 扩展 | 容量规划、调优 |


**工具法**  
通过使用各类工具检查关键指标来分析 CPU 。涉及工具及检查项目：

+ **uptime**：查看负载平均数判断 CPU 负载随时间变化趋势，负载平均数超 CPU 数量常表示 CPU 饱和。
+ **vmstat**：每秒运行检查空闲时间，低于 10% 可能存在问题。 
+ **mpstat**：检查单个繁忙 CPU ，排查线程扩展性问题。
+ **top/prstat**：找出进程和用户中 CPU 消耗大户。 
+ **pidstat/prstat**：将 CPU 消耗大户分解为用户和系统时间。 
+ **perf/dtrace/stap/oprofile**：从用户或内核时间角度剖析 CPU 使用堆栈跟踪。 
+ **perf/cpustat**：测量 CPI 。

**USE 方法**  
在性能调查早期使用，用于发现所有组件内 CPU 瓶颈和错误 。检查内容：

+ **使用率**：CPU 繁忙时间（非空闲线程中）。
+ **饱和度**：可运行线程排队等待 CPU 的程度。 
+ **错误**：CPU 错误，包括可改正错误。可优先检查错误，部分处理器和操作系统能感知可改正错误（如 ECC ），并在不可改正错误致 CPU 失效前关闭 CPU 警示，还需检查所有 CPU 是否在线。使用率可从系统工具繁忙百分比获取，用于发现扩展性问题及理解高 CPU 和核使用率原因。在如云计算环境中，CPU 使用率需关注配额限制，饱和度指标量化 CPU 过载或配额消耗程度。

**负载特征归纳**  
对施加负载进行特征归纳，是容量规划、基准测试和模拟负载重要步骤，可发现可剔除无用工作以提升性能。

+ **基本属性**：平均负载（使用率 + 饱和度）、用户时间与系统时间之比、系统调用频率、自愿上下文切换频率、中断频率 。工作目的是归纳负载特征而非性能结果，平均负载反映 CPU 请求负载，更适合归纳。频率指标理解较难，但能反映负载类型，高用户时间与内核时间比例为计算型负载，高系统调用和中断频率可能为 I/O 消耗型负载。 
+ **高级负载特征归纳/检查清单**：涵盖整个系统及单个 CPU 使用率、负载并发程度、使用 CPU 的应用程序/用户/内核线程、中断及 CPU 互联使用率、CPU 使用原因、停滞周期类型等问题 。该方法概要及测量特征相关内容参见第 2 章，后续章节会扩展剖析分析调用路径和周期分析研究停滞周期相关内容。

<h4 id="i7vRn">6.5.4 剖析Profiling</h4>
剖析是通过定期取样 CPU 状态来构建研究目标图像，以分析 CPU 用量 。

**剖析步骤**

1. 选择要采集的剖析数据及频率。
2. 按设定时间间隔开始取样。
3. 等待关注的兴趣事件发生。 
4. 停止取样并收集数据。 
5. 对收集的数据进行处理分析。部分剖析工具（如 DTrace ）支持实时处理采集数据，一边采样一边分析。同时，借助如火焰图等工具可提升处理和浏览剖析数据的能力；像 Oracle Solaris Studio 的性能分析器能自动化收集过程，方便对照目标源代码浏览剖析数据。

**剖析数据类型影响因素**

+ **级别**：涵盖用户级别、内核级别或两者兼具 。
+ **功能及偏移量**：包括基于程序计数器的仅功能、部分栈跟踪信息、全栈跟踪等情况 。选择全栈跟踪加上用户和内核级别虽能采集完整 CPU 用量信息，但会产生大量数据；仅采集用户或内核级别、部分栈（如五层栈），甚至仅执行函数名，可从较少数据推断 CPU 用量。

**剖析示例**

文中给出 DTrace 单行命令示例，以 997Hz 频率采集 10s 内用户级函数名，命令执行后通过合计函数名处理数据并按频率打印排序计数，结果展示了 CPU 上执行最频繁的用户级函数。997Hz 采样频率是为避免与 100Hz 或 1000Hz 定时运行任务合拍。通过采样全栈跟踪，能发现 CPU 使用的代码路径，从更高视角审视 CPU 用量。更多采样例子见 6.6 节，有关 CPU 剖析从栈中获取其他编程语言上下文信息细节见第 5 章 。此外，对于缓存和互联等特殊 CPU 资源，剖析可使用基于 CPC 的事件触发器，相关内容在周期分析部分介绍。

<h4 id="HXzQy">6.5.5 周期分析</h4>
利用 CPU 性能计数器（CPC）从周期级别理解 CPU 使用率，可展示一级、二级或三级缓存未命中、内存 I/O、资源 I/O 上的停滞周期，以及浮点操作等活动的周期 。通过分析这些信息，可调整编译器选项或修改代码提升性能。

+ **分析流程**：从测量 CPI 入手，CPI 较高时调查停滞周期类型；CPI 较低时寻求减少指令数量的方法。CPI 的高低因处理器而异，一般小于 1 为低，大于 10 为高，可通过已知负载（内存 I/O 密集型或指令密集型）感知其范围 。
+ **CPC 配置**：除测量计数器值，还能配置 CPC 在超出特定值时中断内核。如每 10000 次二级缓存未命中中断一次内核获取栈回溯，随时间推移，内核可建立造成二级缓存未命中的大致代码路径，避免每次未命中都测量的高开销。集成开发环境（IDE）常利用此功能标注造成内存 I/O 和停滞周期的代码位置，使用 DTrace 和 CPC provider 也能实现类似观察效果 。周期分析是高级活动，可能需借助命令行工具花费数天，也可参考 CPU 供应商处理器手册，性能分析器（如 Oracle Solaris Studio ）能辅助找到感兴趣的 CPC 。

<h4 id="Bnjum">6.5.6 性能监控</h4>
用于发现一段时间内活跃的问题和行为模式 。关键 CPU 指标包括：

+ **使用率**：即繁忙百分比。
+ **饱和度**：由系统负载推算出的运行队列长度或线程调度器延时数值 。需对每个 CPU 分别监控使用率，以发现线程扩展性问题；在有 CPU 限制或配额的环境（如云计算环境），要记录相对于限制的 CPU 用量 。监控 CPU 用量时，选择合适测量和归档时间间隔是挑战，5 分钟间隔可能忽视短时间 CPU 使用率突发，理想是每秒测量，但突发也可能在 1 秒内发生，相关信息可从饱和度获取 。

<h4 id="LWihu">6.5.7 静态性能调优</h4>
关注配置环境问题，针对 CPU 性能，检查以下静态配置：

+ CPU 数量、是核还是硬件线程。
+ CPU 架构为单处理器还是多处理器。
+ CPU 缓存大小及是否共享。
+ CPU 时钟频率（如 Intel 睿频加速和 SpeedStep 等动态特性）在 BIOS 中是否启用。 
+ BIOS 中启用或禁用的其他 CPU 相关特性。
+ 处理器是否存在性能问题（如在处理器勘误表中列出的问题）。
+ BIOS 固件版本的性能问题。
+ 是否存在软件的 CPU 使用限制（资源控制）及具体限制内容 。这些问题答案有助于发现之前忽视的配置选择，云计算环境需特别关注 CPU 使用限制问题 。

<h4 id="mnaim">6.5.8 优先级调优</h4>
UNIX 提供 nice() 系统调用，通过设置 nice 值调整进程优先级 。正值降低进程优先级（更友好），负值（仅超级用户 root 可设置）提高优先级 。nice(1) 命令指定 nice 值启动程序，renice(1M) 命令（BSD 上）调整已运行进程优先级 。在 CPU 竞争时，为高优先级工作制造调度器延时，找出低优先级工作（如监控代理程序、定期备份等），以合适 nice 值启动并分析调优效果，确保高优先级工作调度器延时低 。此外，操作系统还提供更改调度类、调度器策略或类调优等更高级进程优先级控制方式，基于 Linux 和 Solaris 的系统含实时调度类，允许进程抢占其他工作，但需注意实时应用程序若有缺陷（如线程陷入无限循环）可能导致所有进程无法使用 CPU，通常需重启系统修复 。

<h4 id="t4HB2">6.5.8 资源控制</h4>
操作系统可对进程或进程组分配 CPU 资源进行细粒度控制，包括 CPU 使用率固定限制和灵活共享方式（基于共享值消耗空闲 CPU 周期），运作原理在 6.8 节讨论 。

<h4 id="O4G4G">6.5.9 CPU 绑定</h4>
将进程和线程绑定在单个或一组 CPU 上，可增加进程 CPU 缓存热度，提升内存 I/O 性能，对 NUMA 系统可提高内存本地性 。实现方式有：

+ **进程绑定**：配置进程只在单个 CPU 或预定义 CPU 组中运行。
+ **独占 CPU 组**：划分一组 CPU 专供指定进程使用，提升 CPU 缓存效率，避免进程空闲时其他进程占用 CPU 导致缓存热度降低 。基于 Linux 系统可通过 cpuset 实现独占 CPU 组，基于 Solaris 系统称为处理器组，6.8 节有配置示例 。

<h4 id="xZqYR">6.5.10 微型基准测试</h4>
多种工具测量简单操作多次执行时间，操作基于以下元素：

+ **CPU 指令**：涵盖整数运算、浮点操作、内存加载存储、分支等指令。
+ **内存访问**：调查不同 CPU 缓存延时和主存吞吐量。 
+ **高级语言**：类似 CPU 指令测试，用高级解释或编译语言编写。 
+ **操作系统操作**：测试如 getpid()、进程创建等 CPU 消耗型系统库和系统调用函数 。早期如美国国家物理实验室 1972 年用 Algol 60 编写的 Whetstone 模拟科学计算负载，1984 年的 Dhrystone 模拟整数负载并成为流行 CPU 性能比较方法，多种 UNIX 基准测试（如进程创建、管道吞吐量等）集合在 UnixBench 中 。近期测试包含压缩速度、质数计算、加密编码等 。比较系统结果时，需明确测试对象，很多基准测试是编译优化结果而非 CPU 速度测试，且多为单线程执行，在多 CPU 系统中意义有限 ，更多信息见第 12 章 。

<h4 id="hKc4l">6.5.11 扩展</h4>
基于资源的容量规划扩展方法：

1. 确定目标用户数或应用程序请求频率。
2. 对现有系统，通过监控 CPU 用量除以用户数或请求数，转化为每用户或每请求 CPU 使用率；未投入使用系统用负载生成工具模拟用户获取 CPU 用量 。
3. 推算 CPU 资源达 100% 使用率时的用户或请求数，即系统理论上限 。对系统扩展性建模，考虑竞争和一致性延时条件，可获更实际性能预测，相关建模和扩展信息分别见第 2 章 2.6 节和 2.7 节 。

<h3 id="Rn3sp">6.6 CPU性能分析工具介绍</h3>
本节介绍基于Linux和Solaris系统的CPU性能分析工具，相关工具及对应功能如下表：

| Linux | Solaris | 描述 |
| --- | --- | --- |
| uptime | uptime | 平均负载 |
| vmstat | vmstat | 包括系统范围的CPU平均负载 |
| mpstat | mpstat | 单个CPU统计信息 |
| sar | sar | 历史统计信息 |
| ps | ps | 进程状态 |
| top | prstat | 监控每个进程/线程CPU用量 |
| pidstat | prstat | 每个进程/线程CPU用量分解 |
| time | ptime | 给一个命令计时，带CPU用量分解 |
| DTrace、perf | DTrace | CPU剖析和跟踪 |
| perf | cpustat | CPU性能计数器分析 |


这些工具从提供CPU统计信息，到深入进行代码路径剖析和CPU周期分析。完整功能说明可参考各工具文档及Man手册。建议了解不同操作系统工具，因其能从多角度反映系统信息。

<h4 id="mlWh5">6.6.1 uptime工具</h4>
uptime(1) 是用于打印系统平均负载的工具之一 。通过命令行输入`uptime`可获取系统当前时间、系统已运行天数、当前登录用户数以及1、5和15分钟内的平均负载 。例如输出`9:04pm up 268 day(s), 10:16, 2 users, load average: 7.76, 8.32, 8.60`，其中最后三个数字分别代表1、5和15分钟内平均负载 。

![](https://cdn.nlark.com/yuque/0/2025/png/45533403/1745411247492-7e3c38e9-edfe-4b5c-8574-396afda9caaf.png)

+ **平均负载含义及计算**：平均负载表示对CPU资源的需求，由正在运行的线程数（使用率）和正在排队等待运行的线程数（饱和度）汇总计算得出。新计算方法是将使用率加上线程调度器延时，可提高精度 。平均负载大于CPU数量，意味着CPU不足以服务线程，部分线程在等待；平均负载小于CPU数量，则表明系统还有余量，线程可按需在CPU上运行 。
+ **平均负载特性**：这三个平均负载值是指数衰减移动平均数，反映了1、5和15分钟以上（实际是指数移动总和中使用的常数）的负载 。早期BSD将平均负载引入UNIX，当时基于调度器平均队列长度计算，在早期操作系统（CTSS、Multics、TENEX ）中广泛使用，在[RFC 546]中有提及 。以TENEX为例，平均负载是对CPU需求的度量，是一段时间内可运行进程数的平均值。如单CPU系统一小时内平均负载为10，意味着该小时内任意时刻预期有1个进程在运行，9个就绪进程等待CPU 。现代例子中，64颗CPU的系统平均负载为128，即平均每个CPU上有一个线程运行，一个线程等待；若平均负载为10，则系统余量较大，所有CPU跑满前还可运行54个CPU消耗型线程 。

**Linux平均负载相关问题**

在Linux系统中，当前平均负载的计算包含了处于不可中断状态执行磁盘I/O的任务 。这使得平均负载不能单纯用来表示CPU余量或饱和度，无法仅依据该值推断CPU或磁盘负载情况。因为负载在CPU和磁盘间动态变化，比较1、5、15分钟这三个平均负载数值也变得困难 。

为解决包含其他资源负载的问题，一种思路是针对不同类型资源（如磁盘、内存、网络等）分别设置平均负载 。相关原型实践表明，各资源拥有独立的平均负载数值，能为非CPU资源状况提供有效概览 。

鉴于上述情况，在Linux上了解CPU负载时，不宜仅依赖平均负载，最好借助其他指标，如vmstat(1)和mpstat(1)工具提供的数据 。

<h4 id="BC1hw">6.6.2 vmstat工具</h4>
vmstat(8) 是虚拟内存统计信息命令，可打印系统全局范围的CPU平均负载，第一列还显示可运行线程数 。以Linux版本示例输出为例：

```plain
$ vmstat 1
procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----
 r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st
15  0 2852 46686812 279456 1401196    0    0     0     0    0    0  0  0 100  0  0
16  0 2852 46685192 279456 1401196    0    0     0     0 2136 36607 56 33 11  0  0
15  0 2852 46685952 279456 1401196    0    0     0    56 2150 36905 54 35 11  0  0
15  0 2852 46685960 279456 1401196    0    0     0     0 2173 36645 54 33 13  0  0
[...]
```

输出第一行是系统启动以来的总结信息（Linux 上的 r 列显示当前值除外 ）。各列含义如下：

+ **r**：运行队列长度，即所有等待加上正在运行的线程数（Linux ）；在 Solaris 上仅指在分派队列里等待的线程数 。
+ **us**：用户态时间。
+ **sy**：系统态时间（内核）。
+ **id**：空闲。
+ **wa**：等待I/O，即线程阻塞等待磁盘I/O时的CPU空闲时间。
+ **st**：偷取（虚拟化环境下CPU在其他租户上的开销，未在输出显示 ）。这些值除 r 外是所有 CPU 的系统平均数 。

<h4 id="wrcjJ">6.6.3 mpstat工具</h4>
多处理器统计信息工具mpstat可报告每个CPU的统计信息 。

+ **Linux版本**：使用`-P ALL`选项打印每个CPU报告，默认打印系统级别总结信息 。示例输出如下：

```plain
$ mpstat -P ALL 1
02:47:49 CPU    %usr   %nice    %sys %iowait    %irq   %soft  %steal  %guest   %idle
02:47:50 all    54.37    0.00   33.12    0.00    0.00    0.00    0.00    0.00   12.50
02:47:50 0      22.00    0.00   57.00    0.00    0.00    0.00    0.00    0.00   21.00
02:47:50 1      19.00    0.00   65.00    0.00    0.00    0.00    0.00    0.00   16.00
[...]
```

各列含义：  
    - **CPU**：逻辑CPU ID，`all`表示总结信息。  
    - **%usr**：用户态时间。  
    - **%nice**：以nice优先级运行的进程用户态时间。  
    - **%sys**：系统态时间（内核）。  
    - **%iowait**：I/O等待。  
    - **%irq**：硬件中断CPU用量。  
    - **%soft**：软件中断CPU用量。  
    - **%steal**：耗费在服务其他租户的时间。  
    - **%guest**：花在访客虚拟机的时间。  
    - **%idle**：空闲 。重要列有`%usr`、`%sys`和`%idle`，可显示每个CPU用量及用户态和内核态时间比例，还能找出“热”CPU 。

+ **Solaris版本**：`mpstat(1M)` 一开始输出系统启动后的统计信息，然后是每隔一段时间的总结信息 。示例输出及各列含义：

```plain
$ mpstat 1
CPU minf mjf xcal intr ithr csw icsw migr smtx  srw syscl usr sys wt  idl
[...]
0  8243  0  288 3211 1265 1682 40  236 262  0  8214 47  19  0  34
1  43708 0  1480 2753 1115 1238 58  406 1967 0  26157 17 59  0  24
[...]
```

- **CPU**：逻辑CPU ID。  
- **xcal**：CPU交叉调用。  
- **intr**：中断。  
- **ithr**：线程服务的中断（低级IPL）。  
- **csw**：上下文切换（总数）。  
- **icsw**：非自愿上下文切换。  
- **migr**：线程迁移。  
- **smtx**：在互斥锁上等待。  
- **srw**：在读/写锁上等待。  
- **syscl**：系统调用。  
- **usr**：用户态时间。  
- **sys**：系统态时间（内核）。  
- **wt**：等待I/O（已废弃，永远为0）。  
- **idl**：空闲 。关键列有`xcal`（检查是否因频率过高消耗CPU资源 ）、`smtx`（检查是否因频率过高消耗CPU资源及锁竞争情况 ）、`usr`、`sys`和`idl`（刻画每个CPU用量及用户态与内核态比例 ） 。

<h4 id="WeJGs">6.6.4 sar工具</h4>
系统活动报告器`sar(1)` ，可用于观察当前活动，还能归档和报告历史统计信息 。

+ **Linux版本选项**：
    - `-P ALL`：与`mpstat(1)`的`-P ALL`选项功能相同，可获取每个CPU的统计信息。
    - `-u`：与`mpstat(1)`默认输出类似，仅包含系统范围的平均值。
    - `-q`：包含运行队列长度列`runq-sz`（等待数加上运行数，与`vmstat`的`r`列相同）和平均负载 。
+ **Solaris版本选项**：
    - `-u`：展示系统范围内`%usr`、`%sys`、`%wio`（零）和`%idl`的平均值。
    - `-q`：包含运行队列长度列`runq-sz`（仅包括等待数），以及运行队列中有线程等待的百分比时间`%runocc`（该值在0和1之间，不够准确 ）。Solaris版本不提供单个CPU的统计信息 。

<h4 id="ynKQ5">6.6.5 ps工具</h4>
进程状态命令`ps(1)` 可列出所有进程的详细信息，包括CPU用量统计信息 。

+ **操作风格及示例**：
    - **源于BSD风格**：如`ps aux` ，其中`a`列出所有用户，`u`提供面向用户的扩展信息，`x`列出没有终端的进程 。输出示例如下：

```plain
$ ps aux
USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
root         1  0.0  0.0  23772  1948?        S    2012   0:00 init
root         2  0.0  0.0      0     0?        S    2012   0:00 [kthreadd]
root         3  0.0  0.0      0     0?        S    2012   0:26 [ksoftirqd/0]
[...]
web      11715 11.3  0.0 632700 11540 pts/0   Sl   01:36  0:27 node indexer.js
web      11721 96.5  0.1 638116 52108 pts/1  R+   01:37  3:33 node proxy.js
[...]
```

- **源于SVR4风格**：如`ps -ef` ，`-e`列出所有进程，`-f`显示完整信息 。输出示例如下：

```plain
$ ps -ef
UID        PID  PPID  C STIME TTY          TIME CMD
root         1     0  0 Nov13?        00:00:04 /sbin/init
root         2     0  0 Nov13?        00:00:00 [kthreadd]
root         3     2  0 Nov13?        00:00:00 [ksoftirqd/0]
[...]
```

+ **CPU用量相关列**：
    - **TIME列**：显示进程自创建起消耗的CPU总时间（用户态 + 系统态 ），格式为“小时:分钟:秒” 。
    - **%CPU列**：在Linux上，显示前一秒内所有CPU上的CPU用量之和，单线程CPU型进程报告100% ，双线程报告200% ；在Solaris上，会根据CPU数量正规化，如单个CPU消耗型线程在八CPU系统上显示为12.5% ，该指标显示最近的CPU用量，采用类似平均负载的衰退平均数 。此外，`ps(1)`还有`-o`等选项用于定制输出和显示列 。

<h4 id="Fuh8F">6.6.6 top工具</h4>
`top(1)` 由William LeFevre于1984年为BSD开发，灵感源自VMS命令MONITOR PROCESS/TOPCPU，该命令可显示最消耗CPU的任务及CPU消耗百分比，还有ASCII字符的条形直方图（多为整数数据 ） 。

+ **基本使用**：`top`命令以一定间隔刷新屏幕展示系统信息 。在Linux上示例输出如下：

```plain
$ top
top - 01:38:11 up 63 days, 1:17, 2 users, load average: 1.57, 1.81, 1.77
Tasks: 256 total,  2 running, 254 sleeping,  0 stopped,  0 zombie
%Cpu(s):  0.0 us,  0.0 sy,  0.0 ni,100.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st
KiB Mem :  49548744k total, 16746572k used, 32802172k free,  182900k buffers
KiB Swap: 100663292k total,        0k used, 100663292k free, 1492524k cached

   PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND
11721 web       20   0  623m  50m 4984 R  93  0.1   0:59.50 node
11715 web       20   0  619m  20m 4916 S  25  0.0   0:07.52 node
10 root      20   0      0     0     0 S   0  0.0   0:24:52.56 ksoftirqd/2
11 root      20   0      0     0     0 S   0  0.0   0:00:00.00 kworker/2:0
11724 admin     20   0  19412 1444  960 R   0  0.0   0:00.07 top
1 root      20   0  23772  1948 1296 S   0  0.0   0:04.35 init
```

顶部是系统范围统计信息，包括平均负载和CPU状态（`%us`、`%sy`、`%ni`、`%id`、`%wa`、`%hi`、`%si`、`%st` ，与`mpstat(1)`打印的相同，是所有CPU的平均值 ） 。下方是进程/任务列表，默认按CPU用量排序 。`CPU`用量用`TIME+`和`%CPU`列显示，`TIME+`列精度达百分之一秒，如“1:36.53”代表CPU已用总时间为1分36.53秒，有些版本还提供“累计时间”模式 。

+ **CPU用量统计模式**：在Linux上，`%CPU`列默认未按CPU数量正规化，即“Irix模式” 。可切换到“Solaris模式”，将CPU用量除以CPU数量，如16CPU服务器上的双线程热点进程在该模式下CPU占用百分比报告为12.5% 。
+ **自身资源占用及改进工具**：`top(1)`自身CPU用量可能较高，因使用`open()`、`read()`、`close()`等系统调用，遍历`/proc`多进程项目时开销大 。`prstat(1M)`工具也存在类似问题 。且`top(1)`对`/proc`快照，会错过短命进程，Linux上的`atop(1)`使用进程核算技术捕捉短命进程并显示 。

<h4 id="IkK5c">6.6.7 prstat工具</h4>
`prstat(1)` 命令在“基于Solaris系统上的top”里有介绍 。

+ **基本输出示例**：

```plain
$ prstat
 PID USERNAME   SIZE   RSS STATE  PRI NICE   TIME     CPU PROCESS/NLWP
214929 root     321M   304M sleep    59      0  2:57:41  0.8% node/smp/594
20721  root     345M   343M sleep    59      0  2:49:53  0.8% node/5
15354  root     172M   156M cpu3     1      0  0:31:42  0.7% node/4
21738  root     179M   143M sleep    59      0  2:37:48  0.7% node/s
21739  root     179M   143M sleep    59      0  2:37:48  0.7% node/s
23186  root     172M   149M sleep    59      0  0:10:56  0.6% node/4
16513  root     179M   148M cpu13    1      0  2:36:43  0.6% node/4
19634  root     139M   170M sleep    59      0  0:29:36  0.5% node/4
23214  root     139M   170M sleep    59      0  0:29:36  0.5% node/4
16299  root     139M   177M sleep    59      0  1:56:10  0.4% node/s
37088  root    1069M  1056M sleep    59      0 38:31:19  0.3% gem-system-x86/4
Total: 390 processes, 1758 lwps, load averages: 3.89, 3.99, 4.31
```

底部是系统信息总结，`CPU`展示最近的CPU用量（与Solaris上的`top(1)`是同一指标 ），`TIME`显示消耗时间 。`prstat(1M)` 通过在文件描述符打开时用`pread()`读取`/proc`状态，相比`top(1)`（反复用`open()`、`read()`、`close()` ）消耗更少CPU资源 。

+ **线程微状态统计输出示例**：使用`-mL`选项可按每个线程打印（每个LWP ）并持续输出（非刷新屏幕 ）。

```plain
$ prstat -mLc
 PID USERNAME   USR   SYS  TRP  TFL  DFL  LCK  SLP  LAT  PROCESS/LWPID
30210 root       8.8  1.2  0.0  0.0  0.0  0.0  90.0  0.0 node/i
42370 root      11.2  0.0  0.0  0.0  0.0  0.0  87.1  1.7 205 23 2K node/i
42328 root      11.1  0.9  0.0  0.0  0.0  0.0  87.0  1.0 205 25 2K node/i
42808 root      11.9  0.0  0.0  0.0  0.0  0.0  87.1  1.0 201 24 2K node/i
56318 root       6.8  1.4  0.0  0.0  0.0  0.0  92.0  0.1 156 23 1K node/i
55032 root       6.8  1.3  0.0  0.0  0.0  0.0  92.1  0.1 156 21 1K node/i
54445 root       6.7  1.3  0.0  0.0  0.0  0.0  92.1  0.1 156 24 1K node/i
54222 root       6.8  1.3  0.0  0.0  0.0  0.0  92.0  0.0 156 20 1K node/i
21222 103       4.1  1.0  0.0  0.0  0.0  0.0  94.0  0.0 43  0 1K beam.smp/585
21217 103       4.1  1.0  0.0  0.0  0.0  0.0  94.0  0.0 43  0 1K beam.smp/585
21222 103       4.7  1.1  0.0  0.0  0.0  0.0  93.7  0.5 985  beam.smp/590
Total: 390 processes, 1758 lwps, load averages: 3.92, 3.99, 4.31
```

高亮八列显示花在每个微状态的时间（总和为100% ），各列含义：  
    - **USR**：用户态时间。  
    - **SYS**：系统态时间（内核）。  
    - **TRP**：系统陷阱。  
    - **TFL**：文本段缺页（可执行数据段缺页）。  
    - **DFL**：数据段缺页。  
    - **LCK**：花在等待用户级锁的时间。  
    - **SLP**：花在睡眠的时间（包括在I/O上阻塞 ）。  
    - **LAT**：调度器延时（分发器队列延时 ）。  
根据线程时间分解，下一步调查方向：  
    - **USR**：剖析用户态CPU用量。  
    - **SYS**：检查使用的系统调用并剖析内核态CPU用量。  
    - **SLP**：依据睡眠事件，跟踪系统调用或代码路径获取更多细节。  
    - **LAT**：检查系统范围内的CPU用量及强制的CPU限制/配额 。许多工作可通过DTrace完成 。

<h4 id="tZwB9">6.6.8 pidstat工具</h4>
Linux上的`pidstat(1)` 工具按进程或线程打印CPU用量，包括用户态和系统态时间分解 。默认仅循环输出活动进程信息 。示例如下：

```plain
$ pidstat 1
Linux 2.6.35-32-server (dev7)  11/12/12  _x86_64_    (16 CPU)
22:24:42       PID    user   system  guest  %CPU   CPU  Command
22:24:43      7815  97.03    2.97    0.00 100.00   11  gzip
22:24:43      7815  97.03    2.97    0.00 100.00   11  gzip
23:24:43       448  0.00    1.00    0.00   1.00    0  kjournald
22:24:44      7814  0.00    2.00    0.00   2.00    3  tar
22:24:44      7816  0.00    2.00    0.00   2.00    2  pidstat
22:24:44      7816  0.00    2.00    0.00   2.00    2  pidstat
[...]
```

该示例捕捉到系统备份，`gzip(1)`内核态时间高（因其压缩代码为CPU密集型 ），`tar(1)`从文件系统读取文件，在内核中消耗更多时间 。选项`-p ALL`可打印所有进程（包括空闲进程 ），选项`-t`打印每个线程统计信息 ，其他选项在本书其他章节介绍 。

<h4 id="sd6LI">6.6.9 prstat工具使用及性能分析</h4>
`prstat(1)` 命令可用于分析系统性能，在基于Solaris系统中有重要应用 。

+ **基本进程信息查看**：执行`prstat`命令，可展示进程相关信息，示例输出如下：

```plain
$ prstat
 PID USERNAME   SIZE   RSS STATE  PRI NICE   TIME     CPU PROCESS/NLWP
214929 root     321M   304M sleep    59      0  2:57:41  0.8% node/smp/594
20721  root     345M   343M sleep    59      0  2:49:53  0.8% node/5
15354  root     172M   156M cpu3     1      0  0:31:42  0.7% node/4
[...]
Total: 390 processes, 1758 lwps, load averages: 3.89, 3.99, 4.31
```

底部系统信息总结中，`CPU`展示进程最近的CPU用量（与Solaris上的`top(1)`是同一指标 ），`TIME`显示进程消耗时间 。通过这些信息可初步了解各进程资源占用及运行时长情况 。且`prstat(1M)` 相比`top(1)`（反复使用`open()`、`read()`、`close()` ），通过在文件描述符打开时用`pread()`读取`/proc`状态，消耗更少CPU资源 。 

+ **线程微状态统计分析**：使用`-mL`选项可按每个线程打印（每个LWP ）并持续输出（非刷新屏幕 ），示例如下：

```plain
$ prstat -mLc
 PID USERNAME   USR   SYS  TRP  TFL  DFL  LCK  SLP  LAT  PROCESS/LWPID
30210 root       8.8  1.2  0.0  0.0  0.0  0.0  90.0  0.0 node/i
42370 root      11.2  0.0  0.0  0.0  0.0  0.0  87.1  1.7 205 23 2K node/i
42328 root      11.1  0.9  0.0  0.0  0.0  0.0  87.0  1.0 205 25 2K node/i
[...]
Total: 390 processes, 1758 lwps, load averages: 3.92, 3.99, 4.31
```

高亮显示的八列展示了花在每个微状态的时间（总和为100% ），各列含义如下：  
    - **USR**：用户态时间，用于剖析用户态CPU用量，了解用户程序代码执行消耗的CPU时间 。  
    - **SYS**：系统态时间（内核），可检查进程使用的系统调用并剖析内核态CPU用量，分析内核为进程提供服务所花费的时间 。  
    - **TRP**：系统陷阱，反映因系统陷阱导致的CPU资源消耗情况 。  
    - **TFL**：文本段缺页（可执行数据段缺页），体现进程在处理文本段缺页时的CPU开销 。  
    - **DFL**：数据段缺页，展示进程处理数据段缺页所占用的CPU时间 。  
    - **LCK**：花在等待用户级锁的时间，帮助分析因锁竞争导致的CPU等待时间 。  
    - **SLP**：花在睡眠的时间（包括在I/O上阻塞 ），依据睡眠事件，可跟踪系统调用或代码路径获取更多细节，判断是否因I/O操作等导致进程长时间睡眠占用CPU资源 。  
    - **LAT**：调度器延时（分发器队列延时 ），用于检查系统范围内的CPU用量及强制的CPU限制/配额 ，分析调度器对进程调度的延迟情况对CPU性能的影响 。通过对这些微状态时间的分析，能深入了解线程在CPU上的运行行为，定位性能瓶颈 。

<h4 id="QiCOy">6.6.10 DTrace工具在CPU分析中的应用</h4>
DTrace可用于剖析用户级和内核级代码的CPU用量，还能跟踪函数执行、CPU交叉调用、中断和内核调度器等，支持负载特征分析、剖析、下钻分析和延时分析等 。在Solaris和Linux系统上均可使用（部分功能有差异 ），入门介绍见第4章。

+ **内核剖析**：通过`dtrace -n 'profile-997 /arg0/ { @[stack()] = count(); }'`等单行命令，以997Hz频率采样内核栈跟踪信息，可了解内核具体操作。还可通过调整命令，如仅输出最频繁的10个、每个栈只输出5帧、采样在CPU上运行的函数或模块等，简化输出并聚焦关键信息 。
+ **用户剖析**：以类似内核剖析方法，通过`dtrace -n 'profile-97 /arg1 && execname == "mysqld"/ { @[ustack()] = count(); }'`等命令，可剖析用户态时间消耗。还可按特定条件（如指定PID、进程名等 ）采样用户栈，或调整输出格式（如仅输出前10个、仅输出5个栈帧、仅输出函数名或模块名等 ） 。
+ **函数跟踪**：利用动态跟踪（如fbt provider ），通过`dtrace -n 'fbt::zio_checksum_generate:entry { self->v = timestamp; } fbt::zio_checksum_generate:return /self->v/ { @["ns"] = quantize(timestamp - self->v); self->v = 0; }'`等命令，可测量函数的CPU时间及分布 。不过通过fbt或pid provider进行动态跟踪因函数随版本变化可能不稳定，也有静态provider提供稳定接口 。
+ **CPU交叉调用跟踪**：使用`dtrace -n 'sysinfo::xcalls { @[stack()] = count(); }'`等命令，可打印CPU交叉调用及导致这些调用的代码路径，过多交叉调用会因CPU消耗影响性能 。
+ **中断跟踪**：基于Solaris系统的intrstat(1M)命令（基于DTrace ）可总结中断CPU用量，显示中断次数及每个驱动程序在每个CPU上的CPU时间百分比。若intrstat(1M)不可用（如Linux ），可使用动态函数跟踪检查中断事件 。
+ **调度器跟踪**：调度器provider（sched ）提供对内核CPU调度器的跟踪操作，包含on - cpu（当前线程开始在CPU上执行 ）、off - cpu（当前线程马上退出CPU执行 ）等探测器 。通过`dtrace -n 'sched::on-cpu /execname == "sshd"/ { self->ts = timestamp; } sched::off-cpu /self->ts/ { @["ns"] = quantize(timestamp - self->ts); self->ts = 0; }'`等命令，可跟踪特定进程（如“sshd” ）在CPU上的运行时间 。



<h4 id="T8Edu">6.6.11 SystemTap工具</h4>
在Linux系统中，SystemTap可用于跟踪调度器时间 。若需将之前的DTrace脚本进行转换，可参考第4章的4.4节和附录E获取相关帮助信息 。

<h4 id="jjTuV">6.6.12 perf工具介绍及使用</h4>
`perf(1)` 原名Linux性能计数器（PCL），现称Linux性能事件（LPE） ，是一套用于剖析和跟踪的工具，通过子命令实现不同功能 。部分子命令及描述如下：

| 命令 | 描述 |
| --- | --- |
| annotate | 读取`perf.data`（由`perf record`创建）并显示注释过的代码 |
| diff | 读取两个`perf.data`文件并显示两份剖析信息之间的差异 |
| evlist | 列出一个`perf.data`文件里的事件名称 |
| inject | 过滤以加强事件流，在其中加入额外的信息 |
| kmem | 跟踪/测量内核内存（slab）属性的工具 |
| kvm | 跟踪/测量kvm客户机操作系统的工具 |
| list | 列出所有的符号事件类型 |
| lock | 分析锁事件 |
| probe | 定义新的动态跟踪点 |
| record | 运行一个命令，并把剖析信息记录在`perf.data`中 |
| report | 读取`perf.data`（由`perf record`创建）并显示剖析信息 |
| sched | 跟踪/测量调度器属性（延时）的工具 |
| script | 读取`perf.data`（由`perf record`创建）并显示跟踪输出 |
| stat | 运行一个命令并收集性能计数器统计信息 |
| timechart | 可视化某一个负载期间系统总体性能的工具 |
| top | 系统剖析工具 |


**系统剖析**：使用`perf record -a -g -F 997 sleep 10` 以997Hz频率对调用栈采样10s（`-a` 对所有CPU采样，`-g` 记录调用栈 ），生成`perf.data`文件，再用`perf report --stdio`查看 。输出按取样计数排序，显示CPU时间去向，如示例中72.98%时间花在空闲线程，9.43%时间花在`dd`进程 。

**进程剖析**：执行`perf record -g command` 可对单个进程剖析，创建`perf.data`文件，查看报告时需调试信息文件进行符号转译 。

**调度器延时**：`perf sched record sleep 10` 记录调度器统计信息，生成`perf.data`文件，`perf sched latency` 可报告跟踪时期平均和最大的调度器延时 。但此类跟踪因调度器事件频繁，会产生CPU和存储开销 。

**stat命令**：`perf stat gzip file1` 基于CPC为CPU周期行为提供概要总结，统计信息包括周期和指令计数、IPC（CPI倒数 ）等 。还可通过`perf list` 列出可检查的计数器，如`cpu-cycles`、`instructions` 等硬件事件，以及`LLC-load-misses`（末级缓存负载未命中 ）等缓存相关事件 。可使用`-e`选项指定事件，如`perf stat -e instructions,cycles gzip file1` 。

**软件跟踪**：`perf record -e` 配合软件性能测量点（由`perf list` 列出，如`context-switches`、`cpu-migrations` 等软件事件，`sched:sched_switch` 等跟踪点事件 ），可跟踪内核调度器活动 。如使用上下文切换软件事件跟踪应用程序离开CPU时的情况，收集调用栈信息 。还可结合动态跟踪点跟踪内核调度器函数，获取类似DTrace的数据，但可能需更多事后处理 。更多信息可参考其手册及Linux内核源代码`tools/perf/Documentation`下的相关文档 。

<h4 id="N8wQN">6.6.13 cpustat工具（Solaris系统）</h4>
在基于Solaris的系统中，`cpustat(1M)` 用于全系统CPU性能检查，`cputrack(1M)` 针对进程分析 ，它们将CPC（性能计数器 ）称为性能测量计数器（PICs ） 。通过以下命令测量CPI（需计算周期数和指令数 ）：

+ `cpustat -t cpu -e PAPI_tot_cyc,PAPI_tot_ins,sys 1` ，`cpustat(1M)` 为每个CPU生成一行输出，可进一步处理（如用awk计算CPI ） 。通过设置标志位使用`sys`令牌计算用户态和内核态周期 。
+ `cpustat -to cpu_clk_unhalted.thread_p,inst_retired.any_p,sys 1` ，通过平台相关事件名测量计数器 。运行`cpustat -h` 可获取处理器支持的计数器列表，输出常引用供应商处理器手册 。系统同时只能运行一个`cpustat(1M)` 实例，因内核不支持多路复用 。

<h4 id="IO7g4">6.6.14 其他性能分析工具</h4>
+ **Linux系统**：
    - **oprofile**：由John Levon开发的最初CPU剖析工具 。
    - **htop**：含CPU用量ASCII柱状图，交互模式比`top(1)` 更强大 。 
    - **atop**：提供更多系统级统计信息，利用进程核算统计捕捉短命进程 。
    - **/proc/cpuinfo**：可获取处理器详细信息，如时钟频率、特性标志位 。 
    - **getdelays.c**：用于延时核算观察，展示每个进程的CPU调度器延时，第4章有演示 。 
    - **valgrind**：内存调试和剖析工具组，`callgrind` 可跟踪函数调用并生成调用图（配合`kcachegrind` 可视化 ），`cachegrind` 可分析程序硬件缓存用量 。
+ **Solaris系统**：
    - **lockstat/plockstat**：用于锁分析，能分析自旋锁和自适应互斥量上的CPU消耗（详见第5章 ） 。 
    - **psrinfo**：可查看处理器状态和信息（`-vp` 选项 ）。 
    - **fmadm faulty**：检查CPU是否因可更正ECC错误增多进入预测故障模式，也可参考`fmstat(1M)` 。 
    - **isainfo -x**：列出处理器特性标志位 。 
    - **pginfo、pgstat**：提供处理器组统计信息，展示CPU拓扑及资源共享情况 。 
    - **lgrpinfo**：提供本地性组统计信息，检查使用中的lgrps（需处理器和操作系统支持 ） 。此外，还有支持Solaris和Linux的Oracle Solaris Studio等复杂CPU性能分析产品 。

<h3 id="bfh22">6.7 实验</h3>
本节介绍主动测试CPU性能的工具，背景信息参考6.5.11节 。使用时建议让mpstat(1)持续运行，用于确认CPU用量和并发度。

<h4 id="gTnSA">6.7.1 Ad Hoc</h4>
+ 虽非测量工具，但可作为已知负载确认观察工具是否正常工作。通过Bourne脚本程序（`while ;; do ;; done &` ）在后台执行无限循环，创建单线程CPU密集型负载，不需要时需终止该进程。

<h4 id="yRWY3">6.7.2 SysBench</h4>
+ SysBench系统基准测试套件含计算质数的简单CPU基准测试工具。示例命令 `sysbench --num -threads=8 --test=cpu --cpu -max -prime=100000 run` ，执行8个线程，最多计算100000个质数，运行时间30.4s 。可用于与其他系统或配置对比（需满足诸多假定，如构建软件使用相同编译器选项等，详见第12章）。

<h3 id="oUxqj">6.8 调优</h3>
对于CPU，排除不必要工作是有效的调优手段，6.5节和6.6节介绍过相关分析辨识方法。此外，还有优先级调优和CPU绑定等调优方法。调优选项和设置取决于处理器类型、操作系统版本和任务需求。

<h4 id="G4ZIw">6.8.1 编译器选项</h4>
编译器及其优化代码选项对CPU性能影响大，常见选项包括编译为64位程序、优化级别等，编译器优化在第5章有讨论。

<h4 id="K6hSu">6.8.2 调度优先级和调度类</h4>
+ **Linux**：`nice(1)`命令调整进程优先级，正`nice`值调低优先级，负值调高（负值仅超级用户可设，范围 -20 ~ +19 ，如`nice -n 19 command` ）；`renice(1)`可更改已运行进程优先级；`chrt(1)`命令可显示并设置优先级和调度策略，也可通过`setpriority()`系统调用、 `sched_setscheduler()`设置。
+ **Solaris**：`priocntl(1)`命令可直接设置调度类和优先级，如`priocntl -e -c RT -p 10 -i pid PID` ，使目标ID进程以优先级10在实时调度类下运行，操作需谨慎，避免系统锁死。

<h4 id="WKZoQ">6.8.3 调度器选项</h4>
+ **Linux**：内核提供控制调度器行为的可调参数，部分配置选项如`CONFIG_CGROUP_SCHED`（允许任务编组分配CPU时间）等（源于3.2.6版内核，附Fedora 16默认值 ），有些内核在`/proc/sys/sched`中还有额外可调参数。
+ **Solaris**：部分内核可调参数可修改调度器行为，具体参考对应操作系统版本文档（如Solaris Tunable Parameters Reference Manual ），使用需谨慎，调优可能受公司或供应商政策限制。

Linux 内核提供一些控制调度器行为的可调参数，多数情况下无需调整。在 Linux 系统（基于 3.2.6 版内核，以 Fedora 16 为例 ）中，可设置以下

| 选项 | 默认值 | 描述 |
| --- | --- | --- |
| CONFIG_CGROUP_SCHED | y | 允许任务编组，以组为单位分配 CPU 时间 |
| CONFIG_FAIR_GROUP_SCHED | y | 允许编组 CFS 任务 |
| CONFIG_RT_GROUP_SCHED | y | 允许编组实时任务 |
| CONFIG_SCHED_AUTOGROUP | y | 自动识别并创建任务组（例如，构建任务 ） |
| CONFIG_SCHED_SMT | y | 超线程支持 |
| CONFIG_SCHED_MC | y | 多核支持 |
| CONFIG_HZ | 1000 | 设置内核时钟频率（时钟中断 ） |
| CONFIG_NO_HZ | y | 无 tick 内核行为 |
| CONFIG_SCHED_HRTICK | y | 使用高精度定时器 |
| CONFIG_PREEMPT | n | 全内核抢占（除了自旋锁区域和中断 ） |
| CONFIG_PREEMPT_NONE | n | 无抢占 |
| CONFIG_PREEMPT_VOLUNTARY | y | 在自愿内核代码点进行抢占 |


此外，有些 Linux 内核在 /proc/sys/sched 中还提供额外可调参数。 

**Solaris 调度器可调参数**

在基于 Solaris 系统中，可通过以下内核可调参数修改调度器行为：

| 参数 | 默认值 | 描述 |
| --- | --- | --- |
| rechoose_interval | 3 | CPU 关联持续时间（时钟 tick ） |
| nosteal_nsec | 100 000 | 如果线程在最近时间（纳秒级 ）运行过则避免线程偷取（空闲 CPU 找活干 ） |
| hires_tick | 0 | 设为 1 代表把内核时钟频率设置到 1000Hz，而不是默认的 100Hz |


**Solaris 调度器类调优**

基于 Solaris 的系统可通过 dispadmin(1) 命令修改调度器类使用的时间片和优先级。以分时调度器类（TS）为例 ，使用 `dispadmin -c TS -g -r 1000` 命令可打印其可调参数（分发器表 ），输出包含：

+ ts_quantum：时间片（单位为毫秒，精度通过 -r 1000 设置 ）。
+ ts_tqexp：线程当前时间片过期之后的新优先级（优先级削减 ）。 
+ ts_slpret：在线程睡眠（I/O）之后醒来的新优先级（优先级提升 ）。 
+ ts_maxwait：在线程被提升至 ts_lwait 一栏的优先级前等待 CPU 的最大秒数。 
+ PRIORITY LEVEL：优先级值。

这些配置可写入文件修改后，由 dispadmin(1M) 重新载入，可先借助 DTrace 度量优先级竞争和调度器延时 。

<h4 id="Qs3m9">6.8.4 进程绑定</h4>
进程可绑定到一个或多个CPU上，借此提升缓存温度与内存本地性，进而提高性能。

+ **<u><font style="background-color:#FBDE28;">Linux系统</font></u>**<u><font style="background-color:#FBDE28;">：通过taskset(1)命令实现。</font></u>能利用CPU掩码或范围设置CPU关联性。示例中，`$ taskset -pc 7 - 10 10790`将PID为10790的进程限定在CPU 7到CPU 10间运行 。
+ **Solaris系统**：借助pbind(1)实现。如`$ pbind -b 10 11901`将PID为11901的进程限定在CPU 10上运行，该系统不能指定多个CPU，若需类似功能，需使用独占CPU组。

<h4 id="YjUQY">6.8.5 独占CPU组</h4>
+ **Linux系统**：提供CPU组功能，可编组CPU并为其分配进程，与进程绑定类似可提升性能，且独占CPU组能禁止其他进程使用，不过会减少系统其他部分可用CPU数量。示例命令创建名为“prodset”的独占组，指定CPU 7 - 10 ，并将PID 1159的进程分配其中 。
+ **Solaris系统**：可通过psrset(1M)命令创建独占CPU组。

<h4 id="e4pZR">6.8.6 资源控制</h4>
现代操作系统除实现进程与CPU关联外，还对CPU用量分配进行细粒度资源控制。

+ **Solaris系统**：将进程或进程组的资源控制机制（Solaris 9引入）称为项目。利用公平份额调度器和份额灵活控制CPU用量，还可对CPU总使用率百分比设限。 
+ **Linux系统**：通过控制组（cgroups）控制进程或进程组资源用量。使用份额控制CPU用量，CFS调度器可设置每段时间内分配CPU微秒数周期的固定上限（CPU带宽，2012年3.2版引入 ）。第11章有管理虚拟化操作系统租户CPU用量相关用例。

<h2 id="oQkUH">第七章 内存</h2>
<h3 id="U6QTA">7.1 内存概述</h3>
    - 系统主存存储应用程序、内核指令及其工作数据、文件系统缓存。二级存储（如磁盘）速度比主存低几个数量级。主存满时会与存储设备交换数据，此过程缓慢，可能影响性能甚至终止内存占用多的进程。
    - 影响系统性能因素还包括内存分配释放、复制、内存地址空间映射的 CPU 开销，多路处理器架构下内存本地性也有影响。
    - 本章分五部分：背景（内存术语和性能概念）、架构（软硬件架构）、方法（性能分析方法）、分析（分析工具）、调优（性能调优及可调参数范例）。

<h3 id="XN6ME">7.2 内存相关术语</h3>
    - **主存**：即物理内存，通常是动态随机访问内存（DRAM）。
    - **虚拟内存**：抽象概念，近乎无限且非竞争性，非真实内存。 
    - **常驻内存**：当前处于主存中的内存。 
    - **匿名内存**：无文件系统位置或路径名，存进程工作数据，如堆栈。 
    - **地址空间**：内存上下文，进程和内核有对应虚拟地址空间。 
    - **段**：特殊用途的内存区域，存可执行或可写页。 
    - **OOM**：内存耗尽，内核检测到可用内存低。 
    - **页**：操作系统和 CPU 使用的内存单位，常为 4KB 或 8KB，现代处理器支持更大页尺寸。 
    - **缺页**：无足够内存支持访问，使用按需虚拟内存时正常。 
    - **换页**：主存与存储设备间交换页。 
    - **交换**：源自 UNIX，指将整个进程从主存移到交换设备，Linux 中指页面转移到交换设备，本书用原定义（转移整个进程）。 
    - **交换（空间）**：存放换页匿名数据和交换进程的磁盘空间，也叫物理交换设备、交换文件，部分工具称交换空间，称虚拟内存易误解。



<h4 id="dhhzO">7.2.1 虚拟内存</h4>
    - **概念**：虚拟内存是抽象概念，为进程和内核提供大的、线性且私有的地址空间。它让操作系统管理物理内存分配，支持多任务，可超额订购内存（使用内存超出主存容量） 。
    - **历史背景**：历史背景可参考Denning 70。
    - **实现原理**：进程地址空间由虚拟内存子系统映射到主存和物理交换设备，内核按需在两者间移动内存页，此过程称交换。基于Solaris内核，限制为主存加上物理交换设备大小，超限时内存分配失败。Linux可配置不同行为模式，如不对内存分配做限制（称作过度提交） 。

图7.1展示了进程虚拟内存，虚拟内存以页的方式实现，图中描绘了内存页在进程地址空间、虚拟存储器、主存储器和交换设备间的关系。

![](https://cdn.nlark.com/yuque/0/2025/png/45533403/1745682268835-5d3b2ff1-cf62-45fb-9715-44e30164e50c.png)

<h4 id="p5A8z">7.2.2 换页</h4>
**操作内容**：包括页面换入和页面换出。它允许运行部分载入的程序、运行大于主存的程序、高效地在主存和存储设备间迁移 。

**换页类型：**

+ **文件系统换页**：文件系统换页由读写位于内存中映射文件页引发，使用文件内存映射（mmap()）的程序会用到。有需要时，内核调出页释放内存，若文件系统页在主存中修改过（“脏”），页面换出要写回磁盘；未修改过（“干净”），仅释放内存。
+ **匿名换页**：牵涉进程私有数据（进程栈和堆），因在操作系统中缺乏名字地址（无文件系统路径名）得名。匿名页面换出要求迁移数据到物理交换设备，Linux中叫交换（swapping） 。匿名换页被称为“坏的”换页，因应用程序访问调出页时，会读磁盘I/O阻塞，使应用程序带同步延时，不过因由内核异步执行，可能不会直接影响应用程序性能。

性能在没有匿名换页（或者交换）的情况下处于最佳状态。要做到这一点，可以通过配置应用程序常驻于内存并且监控页面扫描、内存使用率和匿名换页，来确保不存在内存短缺的迹象。

![](https://cdn.nlark.com/yuque/0/2025/png/45533403/1745682507268-be90315c-11a2-4dbe-a8c0-44c5852463ef.png)

<h4 id="fsFbC">7.2.3 按需换页</h4>
+ **概念**：支持按需换页的操作系统将虚拟内存按需映射到物理内存，把CPU创建映射开销延迟到实际需要访问时，而非初次分配内存时。访问未从虚拟映射到物理内存的虚拟内存页，会引发缺页，导致物理内存按需映射 。
+ **缺页示例**：图7.2展示了缺页示例，当访问包含数据但未映射到进程地址空间的映射文件时，可能触发读操作，若映射可由内存其他页满足，称轻微缺页；也可能在进程内存增长等情况时发生。
+ **虚拟内存状态**：<u>虚拟内存和按需换页使虚拟内存页可能处于未分配、已分配未映射、已分配映射到主存（RAM） 、已分配映射到物理交换空间（磁盘） 状态。由这些状态定义了常驻集合大小（RSS，已分配主存页大小）和虚拟内存大小（已分配区域大小） </u>。 按需换页与虚拟内存换页由BSD引入UNIX。

<h4 id="MAA9K">7.2.4 过度提交</h4>
+ **概念**：<u>Linux支持过度提交，即允许分配超过系统可存储的内存（超过物理内存与交换设备总和） 。它依赖按需换页及应用程序通常不会使用分配的大部分内存这一特性。应用程序提交内存请求（如malloc() ）会成功，否则失败，</u>**<u><font style="color:#DF2A3F;">开发者可借此慷慨分配内存并按需使用</font></u>**<u>，无需严格控制在虚拟内存限额内。</u>
+ **配置与影响**：Linux可用可调参数配置过度提交，其后果取决于内核如何管理内存，相关内容参考7.6.节，OOM终结者论述参考7.3节。

**7.2.5 交换**

    - **概念**：交换是在主存与物理交换设备或交换文件之间移动整个进程的技术，源自UNIX 。
    - **数据处理**：交换进程时，进程私有数据（线程结构、进程堆匿名数据等）需写入交换设备；源于文件系统且未修改的数据可丢弃，需用时从原位置读取。
    - **内核调度**：内核会考虑线程优先级、磁盘等待时间及进程大小等因素，决定交换哪个进程，长期等待和较小的进程更易被优先交换出。
    - **性能影响**：交换对长期运行的UNIX性能影响严重，因交换出的进程需大量磁盘I/O重新运行。基于Solaris的系统支持交换，但仅在换页无法快速释放足够内存满足应用程序需求时才进行（换页速度取决于页扫描速度，见7.3节） ，Linux系统不完全交换，仅交换必要页帧。人们说“系统在交换”通常指换页，Linux中“交换”术语特指页到交换文件或设备（匿名页）的操作 。

**7.2.6 文件系统缓存占用**

    - **原理**：系统启动后内存占用增加正常，因操作系统会将内存用于文件系统缓存以提高性能，原则是只要有可用主存就充分利用。
    - **影响**：初级用户可能因启动后可用内存减少至接近零而苦恼，但这对应用程序无影响，因应用程序需要时，内核能快速从文件系统缓存释放内存，更多不同消耗主存的文件系统缓存内容见第8章 。

**7.2.7 使用率和饱和度**

+ **使用率计算**：主内存使用率通过已占用内存除以总内存得出。文件系统缓存占用的内存可视为未使用，因其能被应用程序重新利用。
+ **主存饱和情况**：当对内存的需求超过主存容量时，称主存饱和。此时操作系统会通过换页、交换或在Linux中使用OOM终结者（后续章节介绍）来释放内存，这些操作标志着主存饱和。
+ **虚拟内存容量使用率**：若系统对允许分配的最大虚拟内存有限制（Linux过度提交无限制） ，可研究容量使用率。虚拟内存耗尽时，内核内存分配会失败，如malloc()返回ENOMEM 。有时当前系统中可用的虚拟内存也称为可用交换。

**7.2.8 分配器**

+ **功能**：虚拟内存处理多任务物理内存时，虚拟地址空间中的实际内存分配和堆放由分配器负责。用户态库或内核程序为程序员提供简单内存使用接口，如malloc() 、free() 。
+ **对性能的影响**：分配器对性能影响显著。系统常提供多个用户态分配器库供选择，分配器可利用线程级别对象缓存等技术提升性能，但分配碎片化、损耗变高时会损害性能，具体示例见7.3节。

**7.2.9 字长**

+ **支持情况**：处理器可能支持多种字长，如32位和64位，对应可运行不同应用程序。
+ **地址空间限制**：地址空间受字长寻址空间限制，32位地址空间无法容纳4GB以上（通常更小）的应用程序，这类程序需用64位或更大字长编译。
+ **对内存性能的影响**：利用更大字长可能提升内存性能，具体取决于CPU架构。当数据类型在更长字长下存在未使用位时，会浪费一小部分内存。

<h3 id="UOZLB">7.3 架构</h3>
<h4 id="KI5v8">7.3.1 硬件架构</h4>
![](https://cdn.nlark.com/yuque/0/2025/png/45533403/1745683165568-8104a29b-69e3-45a1-a463-2e7f4d4fd5be.png)

1. **内存硬件组成**：包括主存、总线、CPU 缓存和 MMU（内存管理单元）。
2. **主存**
    - **类型**：常见为动态随机存取内存（DRAM） ，是易失性内存，由电容和晶体管组成，电容需定期更新电荷。
    - **容量**：企业服务器配置 1GB - 1TB 甚至更大；云计算个体机器 512MB - 64GB。
    - **延时**：用 CAS（列地址控制器）计量，如 DDR3 约 10ns 。内存 I/O 传输、CPU 和 MMU 读取新数据会涉及延时。
    - **架构**
        * **UMA（均匀访问模型）**：通过共享系统总线，每个 CPU 访问所有内存有均匀访问延时，单操作系统实例可在所有处理器上统一运行（对称多处理器架构 SMP）。
        * **NUMA（非均匀访问模型）**：CPU 互联是内存架构一部分，对主存访问时间随相对 CPU 位置变化，本地内存访问延时低于远程内存。操作系统可依内存本地性分配和调度线程。

![](https://cdn.nlark.com/yuque/0/2025/png/45533403/1745683231284-8e99114f-ba06-4983-8f85-de7074f5066c.png)

3. **总线**
    - **共享系统总线**：单个或多个处理器通过共享系统总线、内存桥控制器及内存总线连接内存，如 UMA 示例及 Intel 前端总线示例。
    - **直连**：单个处理器通过内存总线直接连接内存。
    - **互联**：多处理器各通过内存总线连各自内存，处理器间通过 CPU 互联连接，如 NUMA 范例。
4. **DDR SDRAM**：双倍速率同步动态随机访问内存，时钟信号上升沿和下降沿都传输数据（双泵），内存时钟与 CPU 同步。不同标准有不同内存时钟、数据速率和峰值带宽，如 DDR - 200 内存时钟 100MHz，数据速率 200MT/s，峰值带宽 1,600MB/s  。2012 年 9 月发布 DDR4 接口标准，也可用“PC -”加每秒传输数据速率命名。
5. **多通道**：系统架构可并行使用多个内存总线增加带宽，常见双、三或四通道，如 Intel Core i7 处理器支持最大四通道 DDR3 - 1600，最大内存带宽 51.2GB/s。 
6. **CPU 缓存**
    - **分级**：L1 通常分指令缓存和数据缓存；L2 同时缓存指令和数据；L3 为更大一级缓存。一级缓存按虚拟内存地址空间寻址，二级及以上按物理内存地址寻址。
7. **MMU（内存管理单元）**：负责虚拟到物理地址转换，按页转换，页内偏移量直接映射。
8. **多种页大小**：现代处理器支持多种页大小，如 4KB、2MB、1GB 。Solaris 系统内核支持多种页尺寸且能动态创建更大尺寸（MPSS ）；Linux 支持超页，提前预留超页可避免内存碎片问题。
9. **TLB（转换后备缓冲器）**：MMU 的第一级地址转换缓存，分指令和数据页缓存，可按页大小分设单独缓存。如典型 Intel Core i7 处理器带四个 TLB ，指令 4K 页面尺寸 64/线程、128/核 等。
+ 企业服务器 DRAM 容量：1GB - 1TB 甚至更大
+ 云计算个体机器内存：512MB - 64GB 
+ DDR3 访问延时：约 10ns 
+ DDR SDRAM 各标准参数：

| 标准 | 内存时钟(MHz) | 数据速率(MT/s) | 峰值带宽(MB/s) |
| --- | --- | --- | --- |
| DDR - 200 | 100 | 200 | 1,600 |
| DDR - 333 | 167 | 333 | 2,667 |
| DDR2 - 667 | 167 | 667 | 5,333 |
| DDR2 - 800 | 200 | 800 | 6,400 |
| DDR3 - 1333 | 167 | 1,333 | 10,667 |
| DDR3 - 1600 | 200 | 1,600 | 12,800 |
| DDR3 - 3200 | 200 | 3,200 | 25,600 |


Intel Core i7 处理器最大内存带宽：51.2GB/s (四通道 DDR3-1600)。

现代处理器支持页大小：4KB、2MB、1GB。

Intel Core i7 处理器 TLB 信息：

| 类型 | 页面尺寸 | 条目 |
| --- | --- | --- |
| 指令 | 4K | 64/线程，128/核 |
| 指令 | 大 | 7/线程 |
| 数据 | 4K | 64 |
| 数据 | 大 | 32 |


<h4 id="tNfEZ">![](https://cdn.nlark.com/yuque/0/2025/png/45533403/1745683264796-2819da5d-f7d0-4574-8962-e2cd3cb050cc.png)7.3.2 软件架构 </h4>
**1. 内核 CPU 调度器的主要功能**  
• 核心组件：调度器、调度器类、空闲线程。  

• 线程状态流转：  

  • 休眠状态（低优先级 → 高优先级）→ 唤醒 → 可运行状态 → 分时/抢占 → 运行状态。  

  • 运行状态可能因 负载均衡 进入其他 CPU 的运行队列，或因 抢占 被更高优先级线程中断。  

• VCX（自愿上下文切换）与 ICX（非自愿上下文切换）：线程在 CPU 上的切换方式。  



![](https://cdn.nlark.com/yuque/0/2025/png/45533403/1745848927862-30ceec1f-2fbf-4c2e-8b07-a3ab004ba930.png)

**2. CPU 调度的核心机制**  
• 分时（Time-Sharing）：  

  • 多任务处理，优先执行最高优先级线程。  

  • Linux：通过系统时钟中断触发调度事件。  

  • Solaris：由 `clock()` 驱动，检查时间片是否到期。  

• 抢占（Preemption）：  

  • 高优先级线程就绪时，立即中断当前线程。  

  • Linux：`check_preempt_curr()` 检查抢占条件，`_schedule()` 执行切换。  

  • Solaris：`preempt()`（用户态）和 `kpreempt()`（内核态）处理抢占。  

• 负载均衡（Load Balancing）：  

  • 将可运行线程迁移至空闲或低负载 CPU 队列。  

  • Linux：`load_balance()` 函数负责均衡。  

  • Solaris：通过 `disp()`、`disp_rootwork()` 或 `disp_getbest()` 寻找最佳运行线程。  

**3. 调度类与优先级管理**  
• 调度类作用：  

  • 管理线程优先级、CPU 时间分片（时间量子）、调度策略。  

  • 用户线程优先级受 `nice` 值影响（Linux/Solaris 均支持）。  

![](https://cdn.nlark.com/yuque/0/2025/png/45533403/1745848918393-2f938f34-8927-4597-a293-846252c3ec46.png)

**Linux 调度类**  

| 调度器类型 | 策略 | 特点 |
| --- | --- | --- |
| RT（实时） | SCHED_RR / SCHED_FIFO | 固定高优先级（0~99），支持抢占，低延时响应。 |
| O(1) | SCHED_NORMAL | 动态时间片，优先级静态分配（高优先级分配更长分片）。 |
| CFS | SCHED_NORMAL / SCHED_BATCH | 红黑树管理任务，按 CPU 时间排序，提升交互性能。 |


**Solaris 调度类**  

| 调度器类型 | 策略 | 特点 |
| --- | --- | --- |
| RT | SCHED_FIFO | 实时负载固定高优先级（0~59），抢占所有非中断任务。 |
| SYS | SCHED_SYS | 内核线程高优先级，可一直执行（除非被 RT/中断抢占）。 |
| TS（分时） | SCHED_OTHER | 动态调整优先级和时间片，I/O 消耗型任务优先级更高。 |
| IA（交互） | SCHED_OTHER | 优先级略高于 TS（已较少使用）。 |
| FX | SCHED_FIFO | 固定优先级（0~59），类似 TS。 |


---

**4. 调度策略对比**  

| 策略 | Linux | Solaris |
| --- | --- | --- |
| FIFO | 先进先出，运行至自愿退出或更高优先级线程到达。 | 同 Linux，固定优先级队列。 |
| NORMAL | CFS 动态分片；O(1) 静态分片。 | TS/IA 动态调整优先级和时间片。 |
| BATCH | 适合 CPU 密集型任务，不打断交互任务。 | 类似 NORMAL，优化 CPU 消耗型负载。 |
| FSS | 未直接支持（需手动配置 cgroups）。 | 基于共享值公平分配 CPU（0~59 时间片）。 |
| SYSDC | 无直接对应。 | 高 CPU 消耗系统线程调度类，避免饿死。 |
| 中断 | 优先级动态调整（如 IRQ 线程）。 | 固定优先级（159 + IPL）。 |


---

**5. 关键函数与机制**  
• Linux：  

  • `sched_setscheduler()`：设置线程调度策略。  

  • `_schedule()`：执行线程切换。  

  • `pick_next_task()`：选择最高优先级线程。  

• Solaris：  

  • `clock()` → `ts_tick()`：检查时间片到期。  

  • `disp()` / `disp_getbest()`：负载均衡与线程调度。  



**6. 特殊线程状态**  
• 空闲线程（Idle Thread）：最低优先级，系统无其他任务时运行。  

• 抢占规则：  

  • Linux：用户态抢占由 `check_preempt_curr()` 触发；内核态需显式启用抢占。  

  • Solaris：`preempt()`（用户态）和 `kpreempt()`（内核态）。  



**7. 优先级数值范围**  

| 系统 | 策略/类 | 优先级范围 | 备注 |
| --- | --- | --- | --- |
| Linux | RT | 0~99 | 高优先级（数值越小优先级越高）。 |
|  | CFS / NORMAL | 100~139 | 动态调整（CFS 使用红黑树管理）。 |
|  | IDLE | 139+ | 最低优先级。 |
| Solaris | RT | 0~59 | 固定高优先级。 |
|  | TS / IA / FX | 0~59 | 动态调整（TS/IA）或固定（FX）。 |
|  | SYS | 59 | 内核线程高优先级。 |
|  | 空闲 | -60~0 | 最低优先级。 |


---

**8. 总结**  
• Linux 偏向动态调度（CFS 红黑树），支持公平共享（FSS）和实时抢占（RT）。  

• Solaris 强调多级调度类（RT/SYS/TS/IA/FX），优先级范围与 Linux 相反（小数值高优先级）。  

• 抢占与负载均衡 是跨系统的核心机制，确保高优先级任务快速响应，同时优化资源利用率。



![](https://cdn.nlark.com/yuque/0/2025/png/45533403/1746333189286-643294e3-fe8e-4c60-bffe-3741ad634744.png)

**空闲链表**

+ **起源与原理**：最初UNIX内存分配器用内存映射和首次匹配扫描，BSD引入虚拟内存换页时，空闲链表和页面换出守护进程被引入。释放内存添到表头供分配，页面换出守护进程释放的内存（可能含文件系统页缓存）加到表尾，未重用前有请求可从链表取回。
+ **在Linux和Solaris中的应用**：Linux和Solaris系统仍用此类空闲链表，由内核的slab分配器、用户空间的libc malloc等消耗，再通过分配器API暴露 。
+ **Linux的实现**：用伙伴分配器管理页，以2的幂方式为不同尺寸内存分配器提供多个空闲链表，伙伴指相邻空闲内存页可同时分配，处于内存节点pg_data_t等级结构底端，节点支持NUMA，区域有不同用途（如DMA等） ，迁移类型多样，尺寸为2的幂次方页面，节点内分配可提高内存本地性和性能。
+ **Solaris的实现**：为不同内存位置、页大小和页染色准备多个空闲链表，行为类似伙伴分配器，页染色是虚拟和物理地址映射，可利用散列等方式提高访问性能。

![](https://cdn.nlark.com/yuque/0/2025/png/45533403/1746333299336-b6ed655e-d975-4179-88ac-c8a50c63e70a.png)

**回收**

+ **通用机制**：大多从内核的slab分配器缓存释放内存，缓存含未使用内存块供重用，内存交还给系统分配。
+ **Linux**：内核模块可调用register_shrinker()注册函数回收内存。 
+ **Solaris**：主要由slab分配器通过kmem_reap()操作回收内存。

**页扫描**

+ **通用触发机制**：内核页面换出守护进程管理利用换页释放内存，主存空闲链表低于阈值时，页面换出守护进程开始页扫描，通常按需启动，频繁扫描多是内存压力预兆。
+ **Linux**：页面换出守护进程为kswapd() ，扫描非活动和活动内存的LRU列表释放页面，激活基于空闲内存和两个阈值，最低阈值vm.min_free_kbytes可调，其他阈值按比例放大。页缓存活动页和非活动页有LRU列表，kswapd先扫非活动列表，按需扫活动列表，锁定或脏页不适合释放。 
+ **Solaris**：持续循环遍历内存页找最近最少使用页移到交换设备，改进自BSD虚拟内存，有两个内存扫描指针，第一个指针设比特表示页未访问，访问后清除，第二个指针检查比特，两指针间隔可设 。扫描速度动态基于空闲内存，可用内存低于desfree、minfree时，页面换出守护进程更频繁扫描，低于desfree超30秒内核开始交换，参数在setupclock()按主存比例初始化 。 页扫描对大系统成本高，催生循环页缓存快速找页，类似Linux方式。

![](https://cdn.nlark.com/yuque/0/2025/png/45533403/1746333307789-b97a3c7f-860e-4f2c-b0a4-25ce09376d4d.png)

<h4 id="W6vys">7.3.3 进程地址空间</h4>
进程地址空间是一段虚拟页范围，由软硬件共同管理，按需映射到物理页。以32位处理器（如x86和SPARC ）为例，地址空间划分为段存放线程栈、进程可执行文件、库和堆 。不同段类型如下：

+ **可执行文本**：含进程CPU指令，由二进制应用程序文本段映射，只读且有执行权限。
+ **可执行数据**：含已初始化变量，由二进制应用程序数据段映射，有读写权限，私有标记，修改不回写磁盘。 
+ **堆**：应用程序临时工作内存，匿名内存，按需增长，用malloc()分配。 
+ **栈**：运行中线程栈，映射为读写。  
库文本段可被共享，数据段各自私有。堆不停增长不一定是内存泄漏，多数分配器free()不还内存给系统。进程缩减内存方法有Re - exec（用exec() ）和内存映射（用mmap()和mummap() ） 。

**分配器**

多种用户级和内核级分配器用于内存分配，特征包括：

+ **简单API**：如malloc()、free() 。
+ **高效内存使用**：合并未使用区域，减少碎片化。
+ **性能**：慎用锁，利用线程级或CPU级缓存提高内存本地性。 
+ **可观测性**：提供统计数据和排错模式。

**内核级分配器**

+ **slab**：管理特定大小对象缓存，避免页分配开销，适合处理固定大小结构的内核内存分配。Solaris的slab分配器有每CPU缓存（弹夹） ，还提供审计等排错分析工具。在Linux 2.2版本引入，多年为默认选项。 
+ **SLUB**：基于slab，解决其复杂性问题，移除对象队列和每CPU缓存，将NUMA优化留给页分配器，在Linux 2.6.23成为默认选项。

![](https://cdn.nlark.com/yuque/0/2025/png/45533403/1746333481231-10fe098c-a98f-4053-92e6-8883a3059d3d.png)

**用户级分配器**

+ **libc**：Solaris提供的简单通用用户级分配器，默认但不可扩展，单锁机制在多线程下性能差，基于堆会碎片化。 
+ **glibc**：GNU的libc分配器基于dlmalloc，结合多种分配策略，对不同长度分配请求有不同处理方式。 
+ **libumem**：Solaris系统中用户空间版slab分配器，设计考虑扩展性、排错和分析能力，减少时空开销。 
+ **mtmalloc**：Solaris系统高性能多线程用户级分配器，用每线程缓存做小分配，超大区域做大分配，避免锁竞争。

<h3 id="tdHWI">7.4 内存分析和调优方法</h3>
介绍了内存分析和调优的多种方法及运用，相关内容总结在表7.3“内存性能方法” 中，涵盖工具法、USE方法等，部分方法介绍如下：

<h4 id="z4pui">74.1 工具法</h4>
通过遍历可用工具，检查关键指标来分析内存问题，但可能忽略工具无法显示的问题且操作耗时。针对内存可检查以下指标：

+ **页扫描**：Linux用sar -B检查pgscan列，Solaris用vmstat (1M) 检查sr列，连续超10秒页扫描是内存压力预兆。
+ **换页**：Linux用vmstat (8) 检查si和so列，Solaris用vmstat -p按类型查看换页情况，换页是内存低的征兆。 
+ **vmstat**：每秒运行检查free列可用内存。 
+ **OOM终结者**：Linux中在/var/log/messages找“Out of memory” 相关日志。 
+ **交换**：Solaris用vmstat检查w列及用vmstat -S检查si和so看即时交换情况。 
+ **top/prstat**：查看进程和用户对物理及虚拟内存的使用情况及内存使用率。 
+ **dtrace/stap/perf**：用于内存分配栈跟踪，确认内存使用原因。

<h4 id="tjwF7">7.4.2 USE方法</h4>
在性能调查初期，用于定位瓶颈和跨组件错误，从以下系统级指标检查：

+ **使用率**：检查物理内存和虚拟内存使用及剩余情况，不同工具报告可能因对文件系统缓存页或非活动页处理不同而有差异，系统是否支持过度提交会影响虚拟内存使用率检查。
+ **饱和度**：衡量释放内存压力，通过页扫描、换页、交换和Linux OOM终结者等指标判断，持续饱和是内存问题征兆，可由vmstat (1)、sar (1)、dmesg (1) 等工具获取指标 。
+ **错误**：关注失败的内存分配，历史上由应用程序报告，现在SmartOS有相关系统错误计数器报告区域失败的brk() 调用。对于有资源限制或虚拟化环境，内存饱和及衡量方法不同。

<h4 id="Njzmo">7.4.3 使用特征归纳</h4>
在实施容量规划、基准和负载模拟时，内存使用特征分析很重要，能发现错误配置提升性能。如数据库缓存配置不当会影响命中率或引发系统换页 。需分析的内存使用特征包括：

+ **系统层面**：系统范围的物理和虚拟内存使用率；饱和度指标，如换页、交换、OOM终结者情况 ；内核和文件系统缓存使用情况 。
+ **进程层面**：每个进程的物理和虚拟内存使用情况；是否存在内存资源控制 。

示例描述系统内存属性：256GB主存，使用率1%，其中30%是文件系统缓存，用量最大进程（数据库）消耗2GB主存（RSS ） 。因缓存工作数据等原因，内存使用特征会随时间变化，还可能因软件错误出现内存泄漏。

高级使用特征分析的检查清单涵盖更细致问题，如内核内存用途、文件系统缓存活跃比例、进程内存用途及分配调用路径、哪些进程被换出、是否存在内存泄漏、NUMA系统内存分配合理性、CPU和内存停滞周期频率、内存总线平衡性、本地与远程内存I/O情况等 。 

<h4 id="gXnJf">7.4.4 周期分析</h4>
通过检查CPU性能计数器（CPC ）测定内存总线负载，可设置其计算内存停滞周期，这些指标能衡量每指令周期（CPI ），作为内存依赖的CPU负载指标 。

<h4 id="vfjUb">7.4.5 性能监测</h4>
通过监测关键内存指标发现当前问题及行为模式，关键指标包括：

+ **使用率**：以百分比呈现，根据可用内存推断。
+ **饱和度**：涵盖换页、交换、OOM终结者情况。应用内存限制或配额时，需收集强制配额统计数据。还可监测错误，按进程监测内存使用率能帮助发现内存泄漏及泄漏速度。

<h4 id="gZovE">7.4.6 泄漏检测</h4>
应用程序或内核模块无节制增长，消耗空闲链表、文件系统缓存及其他进程内存，可能引发系统换页应对内存压力。问题来源有：

+ **内存泄漏**：软件bug导致忘记释放已分配内存，可通过修改代码、打补丁或升级修复。
+ **内存增长**：软件正常消耗内存但速率远超系统允许，可通过修改软件配置或代码修复。内存增长常被误认内存泄漏，需检查配置判断是否合理。分析内存泄漏依赖软件语言类型，部分分配器排错模式可记录分配细节辅助定位。

<h4 id="EDaI2">7.4.7 静态性能调优</h4>
关注配置后的环境问题，对内存性能，从以下静态配置方面检查：

+ **硬件相关**：主存容量、速度；系统架构（NUMA、UMA ）；内存总线数量；CPU缓存数量、大小；是否支持及配置大页面；是否支持及配置过度提交。
+ **软件相关**：应用程序内存使用配置；使用的分配器；软件强制的内存限制（资源控制）；其他内存可调参数。回答这些问题可发现被忽视的配置选项。

<h4 id="cNA17">7.4.8 资源控制</h4>
操作系统可对进程或进程组内存分配进行细粒度控制，设置主存和虚拟内存固定极限，具体实现方式在7.6节讨论。 

<h4 id="bBzZC">74.9 微基准测试</h4>
用于确定主存速度和特征（如CPU缓存和缓存线长度 ），分析系统间差异。因应用程序和负载不同，内存访问速度对性能影响可能比CPU时钟速度更大，第6章6.4.1节展示了利用内存访问延时微基准测试确定CPU缓存特征的结果。

<h3 id="fddSX">7.5 分析</h3>
本节介绍基于Linux和Solaris操作系统的内存分析工具及其使用策略，相关工具如下：

| Linux | Solaris | 描述 |
| --- | --- | --- |
| vmstat | vmstat | 虚拟和物理存储器统计信息 |
| sar | sar | 历史统计信息 |
| slabtop | ::kmastat | 内核块分配统计信息 |
| ps | ps | 进程状态 |
| top | prstat | 监视每进程存储器使用率 |
| pmap | pmap | 进程地址空间统计信息 |
| DTrace | DTrace | 分配跟踪 |


这些工具从系统层面内存使用统计，深入到进程和分配跟踪。使用时可参考工具文档（如Man手册），调查文件系统内存使用相关内容可参考第8章。 即便仅关注一种系统，参考其他系统工具也有助于拓展视角。

<h4 id="Gi2Tz">7.5.1 vmstat</h4>
+ **Linux**
    - **简介**：虚拟内存统计信息命令，提供系统内存（含当前内存和换页）健康程度总览，也包含CPU统计信息。由Bill Joy和Ozalp Babaoglu于1979年引入BSD 。
    - **示例输出**：展示了vmstat 1的输出数据，默认数据单位为KB ，包括swpd（交换出的内存量）、free（空闲可用内存）、buff（缓冲缓存内存）、cache（页缓存内存）、si（换入内存）、so（换出内存）等。
    - **选项功能**： -S可修改输出单位为MB；-a可输出非活动和活动页缓存明细；-s可将内存统计信息输出成列表。
+ **Solaris**
    - **简介**：vmstat(1)命令更类似源自BSD的最初版本，有许多展示页面换出守护进程活动的列，对不熟悉内部机制的人不太友好。 
    - **示例输出**：介绍了输出数据中w（交换出的线程数量 ）、swap（可用虚拟内存 ）、free（可用内存 ）等与内存相关数据含义。还介绍了 -p选项可显示页面换入、换出和空闲明细 ，如api（匿名页面换入 ）等。
    - **原理说明**：系统启动后空闲内存用于页缓存等内核缓存是正常的，必要时可释放供应用程序使用。若si和so一直非0 ，系统存在内存压力并换页。若系统有稳定的页面扫描速率（sr）属非正常，可能是内存压力预兆，可借助其他工具排查内存去向。 部分统计信息可用kstat按CPU观测，kstat在第4章介绍。

<h4 id="qsG9v">7.5.2 sar</h4>
sar(1) 是系统活动报告工具，可用于观测当前活动，还能配置保存和报告历史统计数据，在本书多个章节被提及。

+ **Linux**

| 选项 | 统计信息 | 描述 | 单位 |
| --- | --- | --- | --- |
| -B | ppgpin/s | 页面换入 | 千字节/秒 |
| -B | ppgpout/s | 页面换出 | 千字节/秒 |
| -B | fault/s | 严重及轻微缺页 | 次数/秒 |
| -B | majflt/s | 严重缺页 | 次数/秒 |
| -B | pgfree/s | 页面加入空闲链表 | 次数/秒 |
| -B | pgscank/s | 被后台页面换出守护进程扫描的页面（kswapd ） | 次数/秒 |
| -B | pgscand/s | 直接页面扫描 | 次数/秒 |
| -B | pgsteal/s | 页面及交换高速缓存回收 | 次数/秒 |
| -B | %vmeff | 页面盗取/页面扫描比率，显示页面回收的效率 | 百分比 |
| -H | hbhugfree | 空闲巨型页面存储器（大页面尺寸 ） | 千字节 |
| -H | hbhugused | 占用的巨型页面存储器 | 千字节 |
| -r | kbmemfree | 空闲存储器 | 千字节 |
| -r | kbmemused | 占用存储器（不包括内核 ） | 千字节 |
| -r | kbbuffers | 缓冲高速缓存尺寸 | 千字节 |
| -r | kbcached | 页面高速缓存尺寸 | 千字节 |
| -r | kbcommit | 提交的主存储器：服务当前工作负载需要量的估计 | 千字节 |
| -r | %commit | 为当前工作负载提交的主存储器，估计值 | 百分比 |
| -r | kbactive | 活动列表存储器尺寸 | 千字节 |


    - **选项功能**：-B（换页统计信息 ）、-H（大页面统计信息 ）、-r（内存使用率 ）、-R（内存统计信息 ）、-S（交换空间统计信息 ）、-W（交换统计信息 ） ，涵盖内存使用、页面换出守护进程活动和大页面使用等方面，相关背景可参考7.3节。
    - **统计信息**：
+ **Solaris**

| 选项 | 统计信息 | 描述 | 单位 |
| --- | --- | --- | --- |
| -g | pgout/s | 页面换出请求 | 操作次数/秒 |
| -g | ppgout/s | 换出的页面 | 页面/秒 |
| -g | pgfree/s | 由页面换出守护进程添加到空闲链表的页面 | 页面/秒 |
| -g | pgscan/s | 页面换出守护进程扫描的页面 | 页面/秒 |
| -k | small | 用于小kmem高速缓存的存储器（对象尺寸小于256字节 ） | 字节 |
| -k | large | 用于大kmem高速缓存的存储器（对象尺寸大于256字节 ） | 字节 |
| -k | ovsz_alloc | 超大尺寸的kmem存储器（对象尺寸通常大于128KB ） | 字节 |
| -p | atch/s | 由页面高速缓存回收（附加 ） | 页面/秒 |
| -p | pgin/s | 页面换入请求 | 操作次数/秒 |
| -p | ppgin/s | 换入的页面 | 页面/秒 |
| -p | pflt/s | 源于保护或者写时复制（COW ）的缺页 | 页面/秒 |
| -p | vflt/s | 源于地址转换的缺页 | 页面/秒 |
| -p | slock/s | 源于软件锁请求磁盘I/O的缺页 | 页面/秒 |
| -r | freemem | 空闲存储器（参见单位 ） | 页面 |
| -r | freeswap | 空闲物理交换（参见单位 ） | 块（扇区 ） |
| -w | swpin/s | 交换入（进程交换 ） | 次/秒 |
| -w | swpout/s | 交换出（进程交换 ） | 次/秒 |
| -r | kbinact | 非活动列表存储器尺寸 | 千字节 |
| -R | frpg/s | 释放的存储器页面，负值表明分配 | 页面/秒 |
| -R | bufpg/s | 缓冲高速缓存增加值（增长 ） | 页面/秒 |
| -R | campg/s | 页面高速缓存增加值（增长 ） | 页面/秒 |
| -S | kbswpfree | 释放交换空间 | 千字节 |
| -S | kbswpused | 占用交换空间 | 千字节 |
| -S | kbswpcad | 高速缓存的交换空间：它同时保存在主存储器和交换设备中，因此不需要磁盘I/O就能被页面换出 | 千字节 |
| -W | pswpin/s | 页面换入（Linux换入 ） | 页面/秒 |
| -W | pswpout/s | 页面换出（Linux换出 ） | 页面/秒 |


    - **选项功能**：-g（换页统计信息 ）、-k（内核内存分配统计信息 ）、-p（换页活动 ）、-r（未使用的内存指标 ）、-w（交换统计信息 ） ，涵盖内存使用、内核分配、换页和交换等方面，背景知识参考7.3节。
    - **统计信息**：

部分统计信息名称含计量单位（pg 表示页，kb 表示KB ，% 表示百分比，/s 表示每秒 ），完整列表见Man手册。%vmeff 是衡量页回收效率的指标，高数值代表成功从非活动列表回收页（健康 ），Man手册指出100% 是高数值，少于30% 是低数值 。更多内存子系统统计信息可从kstat 读取或用DTrace 动态构建。

<h4 id="o5zjq">7.5.3 slabtop</h4>
Linux的slabtop(1)命令可通过slab分配器输出内核slab缓存使用情况，类似top(1)，能实时更新屏幕 。示例输出包含对象数量、slab使用情况、缓存大小等汇总和列表信息 。选用 -sc 选项可按缓存大小排序，slab统计信息取自 /proc/slabinfo ，也可用 vmstat -m 输出。 

<h4 id="jjaS3">7.5.4 ::kmstat</h4>
基于Solaris的系统中，mdb(1)中的::kmstat调试器命令可总结内核内存使用情况，输出分为slab分配器、缓存使用和vmem使用总览三部分 。其他与内存相关子命令有::kmem_slabs、::kmem_slabs -v 和::memstat ，但需超级用户（root）运行mdb -k才能查看。 

<h4 id="Xwb9l">7.5.5 ps</h4>
进程状态命令ps(1)可列出包括内存使用统计信息在内的所有进程细节 。以BSD方式选项输出信息中，%MEM为主存使用占总内存百分比，RSS是常驻集合大小（KB） ，VSZ为虚拟内存大小（KB） 。RSS包含系统库等映射共享段，分析共享内存使用可参考pmap(1)命令。 

<h4 id="Zo6fE">7.5.6 top</h4>
top(1)命令可监控排名靠前的运行中进程并显示内存使用统计信息 。顶部概要显示主存（Mem）及虚拟内存（Swap）总量、使用量和空闲量等 。进程列表按%MEM排序，主存百分比列（%MEM）、虚拟内存大小（VIRT）和常驻集合大小（RES）与ps(1)对应列相同。 

<h4 id="AKiR8">7.5.7 prstat</h4>
prstat(1M)命令可输出进程内存使用等信息 。示例中按RSS（-s rss）排序，最大内存占用进程排在顶部 。还能输出微状态统计信息，如文字和数据缺页计数 ，可结合vmstat -p分析进程内存相关情况。

<h4 id="ZqJM1">7.5.8 pmap</h4>
pmap(1)命令用于列出进程的内存映射，展示其大小、权限及映射对象，可用于仔细检查进程内存使用情况并量化共享内存 。以Solaris系统中PostgreSQL数据库为例，输出包含虚拟内存（kBytes）、主存（RSS）、私有匿名内存（Anon ，Linux最新版本用Dirty代替 ）和权限（Mode）等信息 。多数映射内存非匿名且多为只读，意味着可被其他进程共享，尤其系统库 。Solaris版本提供 -s 选项可显示映射页面大小。需注意，对于映射多的进程，pmap(1)输出可能很长，且会暂停进程报告内存使用，影响活跃任务性能，适合诊断分析时使用，不宜作为监视工具频繁运行。 

<h4 id="zL7U7">7.5.9 DTrace</h4>
DTrace可跟踪用户和内核级内存分配、轻微和严重缺页以及页面换出守护进程运行，支持特征归纳和深入分析 。

+ **分配跟踪**
    - **用户级分配器跟踪**：可用pid provider ，是动态跟踪provider ，无需重启和提前配置分配器。如示例中汇总了Riak数据库（PID 15041）malloc()调用请求大小，还可结合ustack()实现用户级栈跟踪 。也可观测其他用户级分配器内部活动，如列出libumem分配器入口探针。
    - **内核级分配器跟踪**：利用fbt动态provider ，类似方式跟踪。如在Solaris系统中，可跟踪slab分配器 。还列举了按进程PID汇总用户级malloc()请求长度、附调用栈，计算libumem函数调用，按堆增长计算用户栈，按缓存名称和栈跟踪内核级slab分配器等单行命令示例。
+ **缺页跟踪**：可利用fbt动态provider ，或在可用情况下使用稳定的vminfo provider 跟踪缺页 。如在Solaris系统中，跟踪“beam.smp”进程轻微缺页并计算用户栈跟踪频率，或通过vminfo:::maj_fault探针跟踪严重缺页 ，还可通过vminfo:::anonpgin跟踪匿名页面换入及相关进程 。
+ **页面换出守护进程**：可使用fbt provider跟踪其内部运行，具体因内核版本而异。

<h4 id="Y3lec">7.5.10 SystemTap</h4>
SystemTap可用于Linux系统动态跟踪文件系统事件，如需转换之前的DTrace脚本，可参考第4章4.4节和附录E 。

<h4 id="hG4np">7.5.11 其他工具</h4>
+ **Linux内存性能工具**
    - **free**：报告空闲内存，包含缓冲区高速缓存和页缓存（详见第8章）。
    - **dmesg**：检查来自OOM终结者的“Out of memory”信息。 
    - **valgrind**：包含memcheck的性能分析套件，用于分析内存使用、发现泄漏，但会造成严重系统开销，可能使目标系统慢20至30倍。 
    - **swapon**：添加和观察物理交换设备或文件。 
    - **iostat**：若交换设备是物理磁盘或块设备，可用于观测设备I/O，判断系统是否在换页。 
    - **perf**：可观察CPI、MMU/TSB事件，以及源自CPU性能计数器的内存总线停滞周期计数，还能提供缺页及一些内核内存（kmem）事件探针。 
    - **/proc/zoneinfo**：提供内存区域（NUMA节点）的统计信息。 
    - **/proc/buddyinfo**：提供内核页面伙伴分配器统计信息。
+ **Solaris内存性能工具**
    - **prtconf**：显示安装的物理内存大小，可通过grep Mem或新版选项 -m 过滤输出。 
    - **prtdiag**：（在支持的系统中）显示物理内存布局。 
    - **swap**：提供交换统计信息，可列出交换设备（-l）及汇总使用情况（-s）。 
    - **iostat**：功能同Linux下的iostat，用于观测设备I/O判断系统是否在换页。 
    - **cpustat**：观察CPI、MMU/TSB事件，以及源自CPU性能计数器的内存总线停滞周期计数。 
    - **trapstat**：输出陷阱统计信息，包括不同页面大小的TLB/TSB未命中率和CPU使用率百分比，目前仅支持SPARC处理器。 
    - **kstat**：包含有助于理解内核内存使用的更多统计信息，多数相关文档在源代码中（若存在）。
+ **应用程序和虚拟机内存分析工具**：如Java虚拟机等可能有自身内存分析工具（见第5章） 。部分分配器为便于观测会维护自身统计信息，如Solaris中libumem库可通过mdb(1)调试器命令查看，像::vmem可输出libumem占用的内部虚拟内存结构及其使用情况，::umem_malloc_info可按缓存输出分配统计信息，帮助了解不同内存长度使用模式。

<h3 id="M2402">7.6 调优</h3>
主要探讨内存调优，包括保证应用程序常驻主存、避免频繁换页和交换等。相关问题在7.4节和7.5节介绍，本节讨论内核可调参数、配置大页面等其他内存调优方式。

<h4 id="HamXy">7.6.1 可调参数</h4>
+ **Linux**：可通过Documentation/sysctl/vm.txt内核源代码文档了解多种可调参数，也能用sysctl(8)设置 。以内核版本3.2.6、默认值源自Fedora 16为例，介绍如vm.dirty_background_bytes（触发pdflush后台回写的脏存储器量，默认0 ）、vm.swappiness（相对页面高速缓存回收更倾向交换页面的程度，默认60 ）等参数，参数命名规则统一，部分参数互斥。
+ **Solaris**：关键内存可调参数可通过/etc/system设置，常含默认值。如losfree（开始页面扫描阈值，默认1/64 mem页面 ）、desfree（目标空闲存储器阈值，低于该值30秒触发交换，默认1/128 mem页面 ）等，部分情况下调整内核参数受限，大内存系统中调低部分参数或更合适。

<h4 id="l2YJl">7.6.2 多个页面大小</h4>
大页面可提升内存I/O性能，现代处理器支持多种页面大小，如4KB小页面和2MB大页面。

+ **Linux**：可参考Documentation/vm/hugetlbpage.txt设置大页面（巨页面）。创建巨页面方法包括修改相关文件参数，应用程序使用巨页面可通过共享内存段传递SHM_HUGETLB给shmget() ，还可创建基于巨页面的文件系统。
+ **Solaris**：大页面设置可通过设置应用程序环境，使用libhpmem.so.1库实现，相关设置可加入应用程序启动脚本，内核会动态创建大页面，条件不满足则用小页面。

<h4 id="Tgjmw">7.6.3 分配器</h4>
存在多种可提升多线程应用程序性能的用户级分配器，可在编译阶段或执行时借助LD_PRELOAD环境变量进行选择。例如，Solaris中的Ithnum分配器，可通过设置`export LD_PRELOAD=libthnum.so`并添加到应用程序启动脚本进行选用。

<h4 id="SqHtx">7.6.4 资源控制</h4>
+ **基础控制**：可通过ulimit(1)实现，包括设置主存限制和虚拟内存限制 。
+ **Linux的cgroup** ：
    - **memory.memsw.limit_in_bytes**：允许的最大内存和交换空间，单位为字节。
    - **memory.limit_in_bytes**：允许的最大用户内存（含文件缓存），单位字节。 
    - **memory.swappiness**：类似vm.swappiness，可针对cgroup设置。 
    - **memory.oom_control**：设为0，允许OOM终结者作用于该cgroup；设为1则禁用。
+ **Solaris**：利用prctl(1)命令按区域或按项目施加内存限制，借助预留内存页池控制内存分配极限，适用于不同目标应用程序，且不会使内存分配失败。

<h3 id="y6nJM">第八章 文件系统</h3>
<h4 id="GHCkL">章节概述</h4>
研究应用程序I/O性能时，文件系统性能比磁盘性能更关键，它通过缓存、缓冲、异步I/O等缓解磁盘延时影响。借助动态跟踪技术可分析文件系统请求，排查性能来源。本章分五部分：

1. **背景**：介绍文件系统术语、模型、原理及关键概念。
2. **架构**：阐述一般和特殊的文件系统架构。 
3. **方法**：讲述性能分析的观察法和实验法。 
4. **分析**：展示基于Linux和Solaris的文件系统性能工具（静态和动态 ）。 
5. **调优**：说明文件系统的可调参数。

<h4 id="OT8ih">8.1术语</h4>
1. **文件系统**：组织数据为文件和目录的存储方式，提供存取接口、权限控制，含特殊文件类型及时戳元数据。
2. **文件系统缓存**：主存（DRAM）区域，缓存文件系统内容及元数据。 
3. **操作**：对文件系统的请求，如read()、write() 等。 
4. **I/O**：仅指直接读写操作，不含open()和close() 。
5. **逻辑I/O**：应用程序发给文件系统的I/O。 
6. **物理I/O**：应用程序直接发给磁盘的I/O。 
7. **吞吐量**：当前程序和文件系统间数据传输率（B/s ）。
8. **inode**：含文件系统对象元数据的数据结构，有访问权限、时间戳等。 
9. **VFS**：抽象支持不同文件系统类型的内核接口，Solaris上VFS inode称vnode。 
10. **卷管理器**：管理物理存储设备，创建虚拟卷供操作系统使用。 另外涉及fsck、IOPS、操作率、POSIX等术语，可参考第2、3章术语部分。

<h4 id="dkljw">8.2 文件系统模型</h4>
<h5 id="xL9ba">8.2.1 文件系统接口</h5>
+ **基本模型**：从接口角度展示，应用程序通过逻辑操作（如read()、write()等对象操作 ）与文件系统交互，系统工具执行mount()、umount()等管理操作。文件系统再与物理存储设备（磁盘 ）进行物理操作。逻辑与物理操作区域在8.3.12节详述，研究性能时可将文件系统当作黑盒关注对象操作时延，在8.5.2节详述。

![](https://cdn.nlark.com/yuque/0/2025/png/45533403/1746334772863-d5de49d7-db43-4acf-a015-c59b9541d0e2.png)

<h5 id="nDoZN">8.2.2 文件系统缓存</h5>
+ **读操作机制**：读请求时，数据若在主存缓存中（缓存命中 ）直接返回；若不在（缓存未命中 ）则从磁盘读取，未命中的数据会存储在缓存中填充缓存（热身 ）。
+ **写操作机制**：文件系统缓存可缓冲写操作，使之延时写入（刷新 ），不同文件系统实现机制在8.4节详述。

![](https://cdn.nlark.com/yuque/0/2025/png/45533403/1746334788964-d899af3c-20f1-4546-9121-2290e7d8e048.png)

<h5 id="NcHXg">8.2.3 二级缓存</h5>
+ **缓存介质**：二级缓存可以是多种存储介质，以ZFS中出现的闪存为例，形成如RAM（一级缓存 ） - 闪存（二级缓存 ） - 高密度磁盘（文件系统存储 ）的存储层次结构。

![](https://cdn.nlark.com/yuque/0/2025/png/45533403/1746334796541-f956dc34-ad15-4b5c-a3eb-519192f52d55.png)

<h3 id="aA2VO">8.3 文件系统性能关键概念整理</h3>
<h4 id="y3QSq">8.3.1 文件系统延时</h4>
+ **定义**：文件系统逻辑请求从开始到结束的时间，涵盖文件系统、内核磁盘I/O子系统及等待磁盘设备（物理I/O ）的耗时。应用程序线程常阻塞等待请求结束，此时文件系统延时与应用程序性能直接正相关。
+ **特殊情况**：非阻塞I/O或异步线程发起I/O时，应用程序受文件系统影响小。可借助内核跟踪工具查看用户层发起文件系统逻辑I/O的调用栈来定位产生I/O的函数。文件系统曾未开放查看延时接口，提供的磁盘设备指标与应用程序关系不直接。

<h4 id="Tyd8m">8.3.2 缓存</h4>
+ **工作原理**：文件系统启动后用主存（RAM）作缓存提升性能，对应用程序透明，逻辑I/O延时因可从主存返回数据而减小。随时间推移，缓存增大、系统空闲内存减小，内核应按需从缓存释放内存。
+ **功能分类**：用缓存提高读性能，用缓冲（在缓存中 ）提高写性能。文件系统和块设备子系统使用多种缓存，如页缓存（操作系统页缓存 ）、文件系统主存（ZFS ARC ）等 。

<h4 id="G6Sa5">8.3.3 随机与顺序I/O</h4>
+ **概念区分**：按每个I/O的文件偏移量，文件系统逻辑I/O分为随机I/O和顺序I/O。顺序I/O中每个I/O始于上一个结束地址，随机I/O偏移量随机变化。文件系统在磁盘上顺序存放文件数据以减少随机I/O，未达成时会出现碎片化。
+ **性能影响**：文件系统可测量逻辑I/O访问模式识别顺序I/O，通过预取等提升性能。

![](https://cdn.nlark.com/yuque/0/2025/png/45533403/1746334948630-1440a405-2eff-45e1-9832-269b9771c4ed.png)

<h4 id="Oc9u6">8.3.4 预取</h4>
+ **应用场景**：大量顺序读I/O场景下，如文件系统备份，数据量大可能不进缓存或仅读一次，缓存命中率低致系统性能差。
+ **实现方式**：文件系统通过检查当前和上一个I/O的文件偏移量判断是否为顺序读负载并预测，在应用程序请求前向磁盘发读命令填充缓存。如应用程序调用read()，文件系统对比偏移量，若顺序则发起额外读请求，将数据存进缓存供后续读取。

![](https://cdn.nlark.com/yuque/0/2025/png/45533403/1746334956866-4cf518ea-bc4f-4fca-994a-890e7dd7f280.png)

<h4 id="W2FCN">8.3.5 预读</h4>
+ **定义**：预取也常被认为是预读。Linux采用“预读”作为系统调用readahead(2) ，允许应用程序显式预热文件系统缓存。

<h4 id="nLyIq">8.3.6 写回缓存</h4>
+ **原理**：数据写入主存后即视为写入完成并返回，之后异步将数据刷入磁盘，该过程称为刷新（flushing）。如应用程序发起write()请求，数据复制到内核空间后write()调用被内核视为结束，控制权交回应用程序，随后异步内核任务定位数据并发起磁盘写请求。
+ **风险**：牺牲可靠性，基于DRAM的主存不可靠，断电时“脏”数据丢失，应用程序认为写入已完成，但数据可能非完整写入，导致磁盘数据损坏，文件系统元数据损坏可能使文件系统无法加载，需从备份还原，若影响业务数据则冲击严重。
+ **策略**：文件系统默认采用写回缓存策略提升性能，同时提供同步写选项绕开此机制，直接将数据写入磁盘。

<h4 id="INbB2">8.3.7 同步写</h4>
+ **完成标志**：所有数据及必要文件系统元数据完整写入永久存储介质（如磁盘 ）。因包含磁盘I/O延时，比异步写（写回缓存 ）慢。部分应用程序（如数据库写日志 ）为避免异步写的数据损坏风险而采用同步写。
+ **形式**：
    - **单次同步写**：使用O_SYNC标志（或其变体如O_DSYNC、O_RSYNC ，在Linux 2.6.31中glib映射成O_SYNC ）打开文件后，写I/O为同步；部分文件系统可通过加载选项强制所有文件写I/O同步。 
    - **同步提交已写内容**：应用程序在检查点使用fsync()系统调用，同步提交之前异步写入的数据，合并写操作提高性能；关闭文件句柄或文件中未提交缓冲过多时也会提交之前写入，在NFS挂载文件系统解开打包档案时较明显。

<h4 id="bK3ZD">8.3.8 裸I/O和直接I/O</h4>
+ **裸I/O**：绕过整个文件系统，直接给磁盘发地址，常用于数据库等应用，能更好地缓存自己的数据，但难以管理，无法使用常规文件系统工具进行备份、恢复和监控 。
+ **直接I/O**：允许应用程序绕过缓存使用文件系统，类似同步写但缺少O_SYNC选项的保证，读操作也可用。它不会重新调整大小适配文件系统磁盘块大小。直接I/O可用于备份程序防止数据污染缓存，也适用于自建缓存的应用程序，避免双重缓存问题，预取可能因文件系统实现而失效。

<h4 id="VAS5u">8.3.9 非阻塞I/O</h4>
+ **原理**：文件系统I/O通常要么立即结束（如从缓存返回 ），要么需等待（如磁盘I/O ）。等待时应用程序线程阻塞并让出CPU。非阻塞I/O可避免线程创建开销，调用open()时传入O_NONBLOCK或O_NDELAY选项可使用，读写操作会返回错误代码EAGAIN让应用程序重试，而非阻塞调用，部分文件锁可能支持该功能，具体依文件系统实现而定，第5章也有相关介绍。

<h4 id="xf7JO">8.3.10 内存映射文件</h4>
+ **提升性能原理**：将文件映射到进程地址空间，直接存取内存地址，避免read()和write()系统调用及上下文切换开销，若内核支持还可防止数据复制两次，从而提高文件系统I/O性能。
+ **操作方法**：通过mmap()系统调用创建，munmap()销毁，madvise()调整。部分应用程序提供“mmap模式”选项使用mmap系统调用，如Riak数据库。但在磁盘I/O高延时场景下，使用mmap()消除小的系统调用开销无济于事。在多处理器系统使用存在同步CPU MMU的开销问题，映射删除调用（TLB更新 ）可能产生延时，影响取决于内核和映射项。

<h4 id="a01aS">8.3.11 元数据</h4>
+ **定义分类**：元数据是关于文件和目录的信息，分为逻辑元数据和物理元数据。逻辑元数据是用户（应用程序）读取或写入的信息，如通过stat()读取文件统计信息，创建、删除文件和目录等操作；物理元数据是文件系统实现磁盘布局所需信息，类型依文件系统而定，可能包括超级块、inode、数据块指针等及空闲链表。
+ **应用场景**：“元数据密集”负载指频繁操作逻辑元数据的行为，如Web服务器用stat()查看文件是否修改。

<h4 id="rYqoz">8.3.12 逻辑I/O vs. 物理I/O</h4>
+ **差异原因**：应用程序向文件系统发起的逻辑I/O与磁盘物理I/O可能不相符。文件系统通过缓存、缓冲、维护磁盘元数据等操作，使两者出现差异。
+ **具体表现**：
    - **无关**：磁盘I/O可能源于其他应用程序、租户、内核任务（如重建RAID卷、文件系统校验 ）。
    - **间接**：文件系统预取、缓冲会使磁盘I/O与应用程序I/O无直接对应关系。
    - **缩小**：文件系统缓存命中、写抵消（同一地址多次修改一次性写盘 ）、压缩、归并、内存文件系统等情况会使磁盘I/O小于应用程序I/O。 
    - **放大**：文件系统元数据操作、记录尺寸变化、卷管理器奇偶校验等会使磁盘I/O大于应用程序I/O 。
+ **示例**：应用程序写一字节，文件系统可能因数据不在缓存，从磁盘读取相关数据块，再写入新数据并更新元数据，导致磁盘有多次读和更多写操作。

<h4 id="X3P7l">8.3.13 操作并非平等</h4>
+ **性能差异因素**：不同文件系统操作性能差异大，除操作频率外，还受操作是随机还是连续、读写类型、同步异步、I/O大小、是否含其他操作类型及CPU执行消耗等因素影响。
+ **测试示例**：对ZFS文件系统在Intel Xeon 2.4GHz多核处理器上做操作微型基准测试，如open()平均耗时2.2μs，write(128KB)（异步）平均耗时55.2μs ，测试仅含文件系统软件和CPU速度，未包括存储设备。

<h4 id="aM2CV">8.3.14 特殊文件系统</h4>
文件系统通常用于持久存储数据，特殊文件系统有其他用途，如临时文件（/tmp）、内核设备路径（/dev）和系统统计信息（/proc）。 

<h4 id="SRdrj">8.3.15 访问时间戳</h4>
许多文件系统支持记录文件和目录被访问（读取）的时间，这会增加元数据读取，消耗磁盘I/O资源。部分文件系统会优化合并、推迟写操作，减少对有效负载的干扰。

<h4 id="TnRJe"><font style="color:rgb(0, 0, 0);">  
</font><font style="color:rgb(0, 0, 0);">8.3.16 容量相关内容整理</font></h4>
+ **<font style="color:rgb(0, 0, 0) !important;">性能下降原因</font>**<font style="color:rgba(0, 0, 0, 0.85) !important;">：文件系统存满时性能会下降。写入新数据需花费更多时间寻找磁盘空闲块，此过程消耗计算和 I/O 资源。且磁盘空闲空间变得更小且分散，导致 I/O 操作更趋于小型或随机，进而影响文件系统性能。</font>
+ **<font style="color:rgb(0, 0, 0) !important;">影响因素</font>**<font style="color:rgba(0, 0, 0, 0.85) !important;">：具体影响程度取决于文件系统类型、磁盘数据布局以及存储设备。后续内容将介绍多种文件系统相关情况。</font>

<h3 id="tDWJq"><font style="color:rgba(0, 0, 0, 0.85) !important;">8.4 架构相关内容整理</font></h3>
<h4 id="nxFA7"><font style="color:rgba(0, 0, 0, 0.85) !important;">8.4.1 文件系统I/O栈</font></h4>
+ **模型**<font style="color:rgba(0, 0, 0, 0.85) !important;">：图8.6描绘通用模型，应用程序通过POSIX接口发起系统调用，经文件系统I/O相关层次，包括VFS、文件系统、卷管理器，最终到磁盘设备子系统与磁盘设备交互。裸I/O从系统调用直接到磁盘设备子系统，文件系统I/O穿过VFS和文件系统，直接I/O绕过文件系统缓存 。具体层次依操作系统、版本及文件系统而定，完整图可参考第3章。</font>

![](https://cdn.nlark.com/yuque/0/2025/png/45533403/1746335327194-6da55190-c00d-4c56-8ee3-b1c984f7ed98.png)

<h4 id="RSTwC"><font style="color:rgba(0, 0, 0, 0.85) !important;">8.4.2 VFS（虚拟文件系统接口）</font></h4>
+ **作用**<font style="color:rgba(0, 0, 0, 0.85) !important;">：为不同类型文件系统提供通用接口，一些操作系统（如SunOS最初实现 ）将其分为VFS和vnode两个接口，VFS负责文件系统级别操作。Linux的VFS接口使用inode和超级块指代VFS对象，名称易与磁盘数据结构混淆，VFS inode和超级块存于内存 。</font>
+ **功能**<font style="color:rgba(0, 0, 0, 0.85) !important;">：可作为测量文件系统性能通用平台，利用操作系统统计信息及跟踪技术。</font>

![](https://cdn.nlark.com/yuque/0/2025/png/45533403/1746335335523-65785e92-cadd-4e11-a812-e5162c18671e.png)

<h4 id="Ml3kf"><font style="color:rgba(0, 0, 0, 0.85) !important;">8.4.3 文件系统缓存（以Solaris为例 ）</font></h4>
+ **概述**<font style="color:rgba(0, 0, 0, 0.85) !important;">：UNIX原本有缓冲区高速缓存提升设备访问性能，Linux和Solaris有多种缓存。图8.8展示Solaris文件系统缓存，包含UFS和ZFS缓存 。</font>
+ **具体缓存类型**<font style="color:rgba(0, 0, 0, 0.85) !important;">：</font>
    - **旧式缓冲区高速缓存**<font style="color:rgba(0, 0, 0, 0.85) !important;">：最初UNIX在块设备接口用此缓存磁盘设备，是固定大小缓存，加入页缓存后存在负载调配等问题，SunOS中统一缓冲区高速缓存解决该问题，在Solaris里仅用于UFS inode和文件元数据，缓存大小动态，可通过kstat查看访问次数 。</font>
    - **inode缓存**<font style="color:rgba(0, 0, 0, 0.85) !important;">：能动态增长，保存打开文件及被DNLC映射的inode，还有空闲队列inode备用。</font>
    - **页缓存**<font style="color:rgba(0, 0, 0, 0.85) !important;">：1985年SunOS 4引入，缓存虚拟内存文件，比缓冲区高速缓存更高效，多种文件系统使用，大小动态，系统内存不足时，页面换出后台程序（pageout ）和文件系统flush操作会将数据写回磁盘，页缓存有segvn和segmap两个内核驱动程序用户，分别映射文件到进程地址空间及缓存文件读写。 </font>
    - **DNLC（目录名查找缓存）**<font style="color:rgba(0, 0, 0, 0.85) !important;">：由Kevin Robert Elz开发，记录目录项到vnode映射关系，提升路径名查找性能。Solaris的DNLC有多种功能特性，大小可通过调参数调整，命中和未命中数量可通过kstat查看 。</font>

![](https://cdn.nlark.com/yuque/0/2025/png/45533403/1746335347167-89b4102c-95ac-446c-948b-a07eaea604d8.png)

![](https://cdn.nlark.com/yuque/0/2025/png/45533403/1746335358201-b3a2e281-ac2b-4e52-90d6-401d126d3f56.png)

**缓冲区高速缓存**

+ **演变**：Linux原本和UNIX一样使用缓冲区高速缓存，从2.4开始，该缓存被整合到页缓存中，避免双重缓存和同步开销，功能依然用于提升块设备I/O性能，大小动态，可从/proc查看 。

![](https://cdn.nlark.com/yuque/0/2025/png/45533403/1746335453664-803c990e-0904-47c4-af90-4f8b48962b2d.png)

**页缓存**

+ **功能**：缓存虚拟内存页面，包括文件系统页面，提升文件和目录性能，大小动态，受swappiness控制，随应用程序需求增长或释放。 
+ **写回机制**：Linux 2.6.32之前由pdflush线程池处理脏页面写回，现由flusher thread（线程名flush ）替代，每个设备分配一个线程平衡负载、提高吞吐量。脏页面因以下情况写回磁盘：经过30s；调用sync()、fsync()或msync()等系统调用；脏页面数量超过dirty_ratio；页缓存无可用页面。系统内存不足时，页面换出后台程序（kswapd ，页面扫描器 ）将脏页面写回磁盘释放内存，kswapd和写回线程可通过操作系统性能工具查看。

**目录项缓存（Dcache）**

+ **原理**：记录目录项（struct dentry）到VFS inode的映射关系，类似早期UNIX的DNLC，提高路径名查找性能，缓存存于哈希表，以父目录项和目录项名作为键值。 
+ **算法**：采用读 - 拷贝 - 更新遍历（RCU遍历 ）算法，可遍历路径名且不更新目录项引用计数，解决多CPU系统缓存同步扩展性问题，若目录项不在缓存，自动降为引用计数遍历法。反向缓存提升失败查找性能，目录项缓存动态增长，按LRU原则缩小，大小可通过/proc查看。

**inode缓存**

+ **对象属性**：缓存对象为VFS inode（struct inode ），描述文件系统对象属性，很多属性可通过stat()系统调用获取，被操作系统频繁访问，如检查权限、更新时间戳等，存储在哈希表，以inode号和文件系统超级块为键值。 
+ **缓存动态**：动态增长，保存被目录项缓存映射的inode，系统内存紧张时，释放未与目录项关联的inode以缩小内存占用，大小可通过/proc查看。

<h4 id="CqiVJ">8.4.4 文件系统特性</h4>
**块和区段**

+ **基于块的文件系统**：把数据存储在固定大小块中，由元数据块里指针引用。大文件需大量块指针和元数据块，可能导致数据块摆放零碎，产生随机I/O。部分系统通过连续摆放块解决，或使用变长块大小，随文件增长采用更大数据块，减小元数据开销。
+ **基于区段的文件系统**：预先为文件分配连续空间并按需增长，虽有额外空间开销，但提升连续数据流性能，因文件数据本地化效应提升随机I/O性能。

**日志**

+ **作用**：文件系统日志记录文件系统更改，系统宕机时可原子性回放更改，使文件系统迅速恢复一致状态。无日志保护的文件系统，系统宕机时若数据和元数据未完整写入可能损坏，大文件系统从宕机恢复需遍历结构，耗时久。
+ **实现方式**：日志异步写入磁盘，部分文件系统写在单独设备。有的文件系统日志记录数据和元数据，I/O会写两次，有额外开销；有的日志只有元数据，通过写时复制技术保护数据。日志结构文件系统所有数据和元数据更新写入连续循环日志，利于写操作，可合并成大I/O 。

**写时复制**

+ **原理**：写时复制的文件系统不覆写当前使用块，写入步骤为：将数据写到新块；更新引用指向新块；把老块放空闲链表。系统宕机时维护文件系统完整性，将随机写变为连续写，改善写入性能。

**擦洗**

+ **功能**：文件系统后台读出所有数据块验证校验和，在RAID恢复数据前检测坏盘。
+ **影响**：擦洗操作读I/O严重影响性能，只能以低优先级发出。

<h4 id="FrEvi">8.4.5 文件系统种类</h4>
**FFS（伯克利快速文件系统）**

+ **设计背景**：为解决最初UNIX文件系统性能问题，如inode表和存储块划分导致存取性能问题、固定块尺寸小限制吞吐量且增加元数据开销，以及空闲链表碎片化使性能随使用下降。
+ **性能特性**：将磁盘分区为多个柱面组，文件inode和数据尽量放同一柱面组减少磁盘寻道；块大小增至最小4KB提高吞吐量，减少存储文件所需数据块及间接块数量；采用块交叉，连续摆放磁盘块并留间隔，让内核和处理器有时间发起连续文件读请求，减少延时。

![](https://cdn.nlark.com/yuque/0/2025/png/45533403/1746335648218-0b20e598-6690-430d-aec0-d2a2ba465c3c.png)

**UFS（UNIX文件系统）**

+ **诞生与发展**：1984年引入SunOS 1.0，后续加入多种特性，Linux支持读取不支持写入，但支持类UFS的ext3读写。
+ **关键性能特性**：I/O 聚类，聚集磁盘数据块，检测到连续读负载时执行预取；日志功能仅针对元数据，提高宕机后启动速度，合并元数据写操作提升写负载性能；直接I/O绕过页缓存，避免应用双重缓存。

**ext3（Linux扩展文件系统第3版）**

+ **发展历程**：基于最初UNIX文件系统开发，引入多时间戳、柱面组等概念，具备文件系统扩容和日志功能。
+ **关键性能特性**：日志模式分仅针对元数据和针对数据元数据两种，提高宕机后启动速度，合并写操作提升写负载性能；支持使用外部日志设备，避免日志负载与读负载竞争；Orlov块分配器将顶层目录散布到柱面组，减少随机I/O；引入哈希B树作为目录索引，提高目录查找速度。

**ext4（Linux扩展文件系统第4版）**

+ **发布与改进**：2008年发布，对ext3进行功能扩展和性能提升。
+ **关键性能特性**：区段提高数据连续性，减少随机I/O，增大连续I/O大小；通过fallocate() 系统调用预分配空间，提升写性能；延时分配将块分配推迟到写入磁盘时，方便合并写请求，降低碎片化；更快的fsck通过标记未分配块和inode项，减少检查时间。

**ZFS（Sun公司开发的文件系统）**

+ **整合特性**：2005年发布，整合文件系统、卷管理器及众多企业级特性，如池存储、日志、写时复制（COW）、自适应替换缓存（ARC）等。
+ **关键性能特性**：
    - **存储管理**：池存储将所有存储设备放入池创建文件系统，可使用不同RAID类型建池；COW合并写操作连续写入，保证磁盘结构一致；日志批量写入提高异步写吞吐量。
    - **缓存机制**：ARC通过多种缓存算法提高缓存命中率；L2ARC作为二级缓存，基于闪存的SSD缓存随机读负载数据，防止性能断崖；vdev缓存为每个虚拟设备分配，支持LRU和预读。
    - **数据处理**：变长块根据负载选择配置最大块大小；动态条带横跨存储设备，添加设备时自动增长；智能预取针对不同数据类型采用匹配策略；多预取流跟踪预取，有效发起I/O；快照基于COW架构瞬间建立，按需复制新数据块；ZIO流水线分步处理设备I/O，提高性能；压缩支持多种算法，轻量级算法可减少I/O负载；SLOG将日志同步写入单独设备，提高同步写性能；数据消重避免相同数据存放多份。

**btrfs（B树文件系统）**

+ **架构特点**：基于写时复制的B树，结合文件系统和卷管理器现代架构，功能与ZFS基本相同。
+ **关键性能特性**：池存储将存储设备放入卷建立文件系统，可使用不同RAID类型；COW合并写操作连续写入；在线平衡在存储设备间挪动对象平衡负载；区段提高数据连续摆放性能；快照基于COW架构瞬间建立；压缩支持zlib和LZO；日志为每个子卷建立日志树应对同步COW日志负载。计划特性包括RAID - 5和6、对象级别RAID、递增转储和数据消重。

<h4 id="oWChe">8.4.6 卷和池相关内容</h4>
**卷**

+ **概念**：把多块磁盘组合成虚拟磁盘，在此之上建立文件系统。在整块磁盘建文件系统（非分片或分区）时，卷可隔离负载、降低竞争、缓和性能问题。
+ **管理软件**：如Linux的逻辑卷管理器（LVM）和Solaris卷管理器（SVM） 。卷或虚拟磁盘也可由硬件RAID控制器提供。

**池**

+ **概念**：将多块磁盘放入存储池，可在池上建立多个文件系统。池存储比卷存储更灵活，文件系统可独立增长或缩小，不影响底层设备，现代文件系统如ZFS和btrfs采用此方式。
+ **性能优势与考量**：能让文件系统利用所有磁盘提升性能，但负载未隔离，有时需用多个池隔离负载。使用相关软件管理时，需考虑以下性能因素：
    - **条带宽度**：与负载相匹配。
    - **观察性**：虚拟设备使用率可能不准确，需检查对应物理设备。 
    - **CPU开销**：进行RAID奇偶校验计算时可能产生开销，不过现代CPU使该问题逐渐缓解。 
    - **重建**：也叫重新同步（resilvering），向RAID组添加磁盘时，因数据同步耗时可能严重影响性能，随着存储设备容量增长快于吞吐量，重建问题未来可能更严峻。

<h3 id="ouMur">8.5 文件系统分析和调优方法</h3>
<h4 id="rWZAC">8.5.1 磁盘分析</h4>
以往常聚焦磁盘性能而忽视文件系统，认为I/O瓶颈在磁盘，但现代文件系统复杂，仅分析磁盘易误入歧途，错过问题排查。 

<h4 id="yG26N">8.5.2 延时分析</h4>
+ **定义**：测量文件系统操作从发起至完成的时间，涵盖所有对象操作，不仅是I/O（如sync() ），操作延时 = 完成操作时刻 - 发起操作时刻。
+ **测量层次及优缺**：
    - **应用程序层**：优点是能获取文件系统延时对应用程序影响的一手信息，判断延时是否发生在程序关键功能或异步I/O处；缺点是不同应用程序及软件版本需用不同技术 。
    - **系统调用接口层**：优点是有详尽资料，可通过操作系统工具和静态跟踪观察；缺点是系统调用捕获所有文件系统类型数据，非存储型文件系统（如统计、套接字 ）数据会干扰分析，需过滤 。
    - **VFS层**：优点是作为所有文件系统通用标准接口，操作与系统操作一一对应（如vfs_write() ）；缺点是针对非存储型文件系统需过滤，不同版本文件系统接口不同 。
    - **直接在文件系统层**：优点是能跟踪特定类型文件系统，获取内部环境上下文详情；缺点是特定于某种文件系统，不同版本需不同跟踪技术 。
+ **选择依据**：依据应用程序文档（有无延时指标及收集方法 ）、操作系统工具（能否提供延时指标 ）、动态跟踪（系统是否支持自定义脚本检查 ）来选择测量层。
+ **延时表现形式**：单位时间平均值（如每秒平均延时 ）；全分布（直方图或热图 ）；单位操作延时（列出每个操作 ）。缓存命中率高时，平均值可能掩盖高延时问题，查看全分布、每个操作延时及不同层延时影响，有助于找出问题。找到高延时后，继续分析文件系统找根源。
+ **事务成本**：用文件系统消耗时间百分比衡量（ = 100×所有文件系统阻塞延时 / 应用程序事务时间 ），可从应用程序性能角度量化文件系统操作损耗，预测性能改进，指标可以是一段时间内事务均值或单个事务 。

![](https://cdn.nlark.com/yuque/0/2025/png/45533403/1746335925692-9751c199-0ce8-4243-9035-8508f8a9cb12.png)

<h4 id="Ry2BL">8.5.3 负载特征归纳</h4>
+ **目的**：是容量规划、基准测试和负载模拟重要工作，通过排除不必要工作获最大性能收益。
+ **基本属性**：操作频率和类型、文件I/O吞吐量、文件I/O大小、读写比例、同步比例、文件随机和连续访问比例 。这些特征随时间变化，需关注平均值及最大值，查看跨时段全分布。
+ **高级负载特征归纳/检查清单**：涉及文件系统缓存命中率、缓存大小及当前使用情况、使用文件系统的应用程序或用户、被访问/创建/删除的文件和目录、I/O发起原因、应用程序I/O调用路径、I/O抵达时间分布等问题，可针对单个应用或文件系统，也可用于排查问题。
+ **性能特征归纳**：关注文件系统操作平均延时、是否有高延时离群点、操作延时全分布、是否开启文件系统或磁盘I/O系统资源流控 。前三个问题可针对每个操作类型单独分析。

<h4 id="M4AKp">8.5.4 性能监控</h4>
+ **作用**：识别当前问题及跨时段行为模式。
+ **主要指标**：操作频率和操作延时。操作频率是负载基本特征，操作延时是性能结果，延时好坏取决于负载、环境和需求，可通过微型基准测试调查（如对比缓存命中和未命中负载 ）。操作延时指标可用每秒平均值，也可包含最大值、标准差等，最好查看全分布找离群点等模式。可记录单个操作类型频率和延时数据，调查负载和性能变化，找出操作类型差异。基于文件系统资源控制的系统（如ZFS I/O限流 ），需统计流控及流控时间信息。

<h4 id="kIpH3">8.5.5 事件跟踪</h4>
+ **原理**：捕获文件系统每个操作细节，是观察分析的最后手段，因捕获保存信息到日志文件会增加性能开销。
+ **信息内容**：文件系统类型、挂载点、操作类型（读写、统计等 ）、操作大小（字节数 ）、操作开始和结束时间戳、操作完成状态（错误等 ）、路径名（若适用 ）、进程ID、应用程序名 。利用操作开始和结束时间戳可计算延时，跟踪框架可边跟踪边计算并在日志记录延时数据，还可过滤输出，记录慢于阈值的操作，正确过滤可大幅降低性能损耗。

<h4 id="Ht5mz">8.5.6 静态性能调优</h4>
关注文件系统性能相关的静态配置，包括挂载的文件系统数量、文件系统记录大小、访问时间戳启用情况、缓存配置（种类、最大缓存大小 ）、二级缓存使用情况、存储设备数量及配置（是否用RAID ）、文件系统版本或内核版本、有无需考虑的系统bug补丁、是否启用文件系统I/O资源控制等。需重新审视这些配置选项，排查被忽视的配置问题。 

<h4 id="Gaaws">8.5.7 缓存调优</h4>
内核和文件系统使用多种缓存，如缓冲区高速缓存、目录缓存、inode缓存和文件系统页缓存等。调优时先检查哪些缓存投入使用、使用情况及缓存大小，再根据负载调整缓存 。具体调优方法可参考第2章2.5.17节。 

<h4 id="dDOml">8.5.8 负载分离</h4>
将某些类型负载分离到独占文件系统和磁盘设备，避免随机I/O对旋转磁盘不利影响，提升性能，也称为“单独转轴”。例如让日志文件和数据库文件拥有单独文件系统和磁盘，可提高数据库性能 。

<h4 id="izG5d">8.5.9 内存文件系统</h4>
使用内存文件系统可快速响应请求，因文件内容存于内存。但现代文件系统通常有大文件系统缓存，内存文件系统多用作临时方案。如Solaris挂载tmpfs文件系统用于存储临时文件，Linux也有tmpfs供特殊文件系统使用 。

<h4 id="uYGg6">8.5.10 微型基准测试</h4>
+ **工具用途**：文件系统和磁盘有很多基准测试工具，用于测试多种文件系统性能或某种负载下同一文件系统不同设置的性能。
+ **测试参数**：包括操作类型（读写及其他操作频率 ）、I/O大小（1B - 1MB甚至更大 ）、文件偏移量模式（随机或连续 ）、随机访问模式（统一随机分布或帕累托分布 ）、写类型（异步或同步，如O_SYNC ）、工作集大小（影响文件系统缓存 ）、并发（同时执行I/O数或线程数 ）、内存映射（使用mmap()或read()/write() ）、缓存状态（冷、未填充或热 ）、文件系统可调参数（如压缩、数据消重 ） 。
+ **测试组合及结果**：常见组合有随机读、连续读、随机写、连续写等，重要参数是工作集大小，会影响测试数据来源（主存缓存或磁盘 ）及性能差异。部分基准测试工具可能因测试对象文件大小问题，使测试结果源于缓存而非磁盘，需理解测试文件系统逻辑I/O与测试磁盘物理I/O的区别。有些工具通过文件系统I/O接口发起测试，避免缓存和缓冲影响，但仍有代码路径及文件映射开销。随着系统内存增大，应用程序对缓存命中率依赖降低。 更多内容可参阅第12章。

<h3 id="jtmGK">8.6 基于Linux和Solaris系统的文件系统分析工具整理</h3>
<h4 id="XZIjd">8.6.1 vfsstat（Solaris）</h4>
+ **功能**：VFS层类似iostat的工具，由Bill Pijewski为SmartOS开发，打印单位时间内文件系统操作（逻辑I/O）总结信息，包含用户应用程序感知的平均延时，与应用程序性能联系紧密，不同于展示磁盘I/O（物理I/O）的iostat 。
+ **输出指标**：r/s、w/s（每秒读写次数 ）；kr/s、kw/s（每秒读写KB数 ）；ractv、wactv（正在执行的平均读写操作数 ）；read_t、writ_t（VFS读写平均延时 ）；r、w（VFS读写操作等待时间百分比 ）；d/s、del_t（每秒I/O流控延时时间和平均延时时间 ） 。可刻画负载及性能，还包括ZFS I/O流控信息。

<h4 id="WTBg7">8.6.2 fsstat（Solaris）</h4>
+ **功能**：报告各种文件系统统计数据，用于负载特征归纳，以单个文件系统形式展示信息，但不包括延时统计信息 。

<h4 id="Ft7tk">8.6.3 strace（Linux）、truss（Solaris）</h4>
+ **功能**：系统调用调试器，可测量文件系统操作时间，但会影响性能，仅在性能开销可接受且无法用其他延时分析工具时使用 。

<h4 id="wrAIM">8.6.4 DTrace（Linux、Solaris ）</h4>
+ **操作计数**：可按应用程序和类型统计文件系统操作，为负载特征归纳提供测量信息。如Solaris用fsinfo provider按程序名统计操作，加入tick - 1s探测指令可得到每秒统计；也可按probename统计操作类型，还能过滤特定应用程序 。在Linux的fsinfo provider无法使用时，可通过syscall和fbt provider观察文件系统操作，并用probefunc计算操作类型 。
+ **文件打开**：基于DTrace的opensnoop工具，展示文件系统负载，显示进程、路径名和open()错误信息，有助于性能分析和故障排除 。
+ **系统调用延时**：在系统调用接口级测量文件系统延时，输出直方图（单位ns ），可显示缓存命中和磁盘读取的延时高峰，使用avg()函数可显示均值，但会平均两个峰值，需注意延时是否在应用程序请求期间发生，可通过跟踪应用程序I/O调用栈判断 。
+ **VFS延时**：可通过静态provider（若有 ）或动态跟踪技术（fbt provider ）跟踪VFS延时。在Solaris上用fbt:fop_*()函数跟踪，可匹配读调用变体获取更多信息；Linux上也有相应DTrace原型，可匹配特定文件系统（如ext4 ），展示缓存命中和未命中的延时高峰 。需注意fbt provider接口不稳定，相关命令或脚本需匹配对应内核 。
+ **块设备I/O调用栈**：统计发出块设备I/O时内核调用栈频率，可了解文件系统内部工作机制，如ZFS和ext4的内部I/O调用路径，解释磁盘I/O（异步、元数据 ）产生原因 。
+ **文件系统内部**：跟踪文件系统实现定位延时，如列出Solaris上ZFS的函数入口探测器，利用ZFS和VFS映射关系跟踪ZFS读延时，输出展示缓存命中和未命中的I/O峰值 。深入ZFS内部测量I/O时间会更复杂 。

<h4 id="klmTr">8.6.5 SystemTap（Linux）</h4>
可动态跟踪文件系统事件，有关将DTrace脚本转化的信息可参见第4章4.4节和附录E。 

<h4 id="t2YGf">8.6.6 LatencyTOP</h4>
+ **功能**：报告延时根源的工具，可针对整个系统或单个进程，最初为Linux开发，已移植到基于Solaris的系统 。
+ **报告内容**：如报告文件系统延时，示例中展示了系统及gzip进程的延时来源及占比等信息，需CONFIG_LATENCYTOP和CONFIG_HAVE_LATENCYTOP_SUPPORT两个内核选项支持 。

<h4 id="Eu7CM">8.6.7 free（Linux）</h4>
显示内存和交换区统计信息，buffers列展示缓冲区高速缓存大小，cached列展示页缓存大小，-m选项以MB为单位显示结果 。

<h4 id="xs6AQ">8.6.8 top（Linux、Solaris ）</h4>
有些版本包含文件系统缓存详细信息，如Linux版本可显示缓冲区高速缓存大小，更多信息参见第6章。 

<h4 id="JJEuz">8.6.9 vmstat（Linux、Solaris ）</h4>
+ **Linux**：每秒更新一次，buff列显示缓冲区高速缓存大小，cache显示页缓存大小（单位KB ）。
+ **Solaris**：默认输出不包括缓存大小，但free列以KB为单位显示相关信息，从Solaris 9开始，页缓存被当成空闲内存一部分，-p选项按类型分别标出页换进换出信息，目前文件系统列不包括ZFS文件系统事件 。

<h4 id="XNnCy">8.6.10 sar（Linux、Solaris ）</h4>
+ **功能**：系统活动报告器，提供各种文件系统统计信息，可配置进行长期记录，本书多个章节提及相关统计信息 。
+ **Linux**：运行sar可每隔一段时间报告当前活动，-v选项可打出目录项缓存未用计数、使用中的文件描述符个数、inode个数等信息，还有选项可打印缓冲区高速缓存和页缓存大小（单位KB ）。 
+ **Solaris**：运行sar并指定输出次数，-v选项显示inode - sz（inode缓存大小和最大值 ），-b选项提供旧式缓冲区高速缓存统计信息 。

<h4 id="SGV98">8.6.11 slabtop（Linux）</h4>
打印有关内核slab缓存的信息，部分用于文件系统缓存，如dentry（目录项缓存 ）、inode_cache（inode缓存 ）、ext3_inode_cache（ext3的inode缓存 ）、ext4_inode_cache（ext4的inode缓存 ） 。不使用-o选项输出模式时会不断刷新，使用/proc/slabinfo，在启用CONFIG_SLAB情况下生成 。

<h4 id="uSAF8">8.6.12 mdb::kmastat（Solaris）</h4>
可查看Solaris内核内存分配器详细统计信息，包括文件系统使用的多种缓存，输出有多页，可看出哪块缓存使用最多数据及内核内存使用情况，必要时可跟踪某个缓存分配情况及代码路径和用户 。

<h4 id="rDykY">8.6.13 fcachestat（Solaris）</h4>
开源工具，调用Perl的Sun::Solaris::Kstat库，打印适合UFS缓存活动分析的统计信息，第一行是自启动后信息统计，五组数据代表不同缓存和驱动程序，缓存命中率以百分数表示，另一列是总计访问次数，可能需更新才能正确工作 。

<h4 id="cdK0r">8.6.14 /proc/meminfo（Linux）</h4>
提供内存使用状况分解，如free等工具也读这个文件，包括缓冲区高速缓存（Buffers）和页缓存（Cached）及系统内存使用其他概况分解，第7章有专门叙述 。

<h4 id="YnHJl">8.6.15 mdb::memstat（Solaris）</h4>
mdb -x里的::memstat命令提供Solaris内存使用概况分解，包括ARC里缓存的ZFS File Data及包含UFS数据缓存的Page cache 。

<h4 id="RhgU2">8.6.16 kstat（Solaris ）</h4>
前述工具统计数据可从kstat得到，访问途径包括Perl的Sun::Solaris::Kstat库、C的libkstat库或kstat命令，表8.8列出显示文件系统统计信息的命令及统计项目，不同内核版本统计信息有变化，如在SmartOS/Illumos内核中加入新计数器 。 

<h4 id="pa7YD">8.6.17 其他工具</h4>
+ **df**：报告文件系统使用情况和容量统计信息。
+ **mount**：显示文件系统挂载选项，用于静态性能调优。 
+ **inotify**：Linux文件系统事件监控框架。
+ **ZFS**：提供zpool命令，使用iostat子选项可查看ZFS统计信息，包括池操作频率（读写 ）及吞吐量，arcstat.pl是从kstat读取统计信息的Perl程序 。

<h4 id="y3pHk">8.6.18 可视化</h4>
+ **负载可视化**：在文件系统负载的时间轴上描点，可发现与时间相关使用模式，分别制作读、写和其他文件系统操作图有助于识别使用模式 。
+ **延时可视化**：文件系统延时分布常是双峰的，展示全分布图（如热图 ）可解决问题，图8.15示例展示了随机读取1GB文件的文件系统延时热图，前半部分延时多由磁盘I/O导致，后半部分缓存命中，磁盘I/O消失，该软件可隔离文件系统操作类型 。



<h3 id="CtvCD">8.8 文件系统调优</h3>
<h4 id="nHmIx">8.8.1 应用程序调用</h4>
+ **方法**：通过`fasync()`合并一组请求，相较于使用`open()`标志位`O_DSYNC/O_RSYNC`的单个写入，可提高同步性能。此外，`posix_fadvise()`和`madvise()`函数能为缓存策略提供建议，提高性能。
+ `posix_fadvise()`**函数**：操作文件区域，原型为`int posix_fadvise(int fd, off_t offset, off_t len, int advice);` 。建议标志位包括`POSIX_FADV_SEQUENTIAL`（指定数据范围连续访问 ）、`POSIX_FADV_RANDOM`（指定数据范围随机访问 ）、`POSIX_FADV_NOREUSE`（数据不会被重用 ）、`POSIX_FADV_WILLNEED`（数据会在不远的将来重用 ）、`POSIX_FADV_DONTNEED`（数据不会在不远的将来重用 ） 。内核利用这些建议提高性能，决定预取和缓存数据时机，不同内核支持情况不同。
+ `madvise()`**函数**：对内存映射进行操作，原型为`int madvise(void *addr, size_t length, int advice);` 。建议标志位有`MADV_RANDOM`（偏移量将以随机顺序访问 ）、`MADV_SEQUENTIAL`（偏移量将以连续顺序访问 ）、`MADV_WILLNEED`（数据还会再用，请求缓存 ）、`MADV_DONTNEED`（数据不会再用，请勿缓存 ） 。内核借助这些信息做出更好缓存决策。

<h4 id="ER5OD">8.8.2 ext3（以Linux的ext2、ext3、ext4文件系统为例 ）</h4>
+ **调优命令**：可通过`tune2fs(8)`命令调优，有手动通过`mount(8)`命令，或在启动时通过`/boot/grub/menu.lst`和`/etc/fstab`挂载指定多种选项两种方式，当前设置可通过`tune fs -l设备名`和`mount（不带选项 ）`查看 。
+ **关键选项**：`mount(8)`使用`noatime`选项禁用文件访问时间戳更新，可提高整体性能；`tune2fs -O dir_index /dev/hdX`使用哈希B树提高大目录查找速度；`e2fsck -D -t /dev/hdX`可重建文件系统目录索引，`e2fsck(8)`其他选项与检查和修复文件系统相关 。

<h4 id="j8Soc">8.8.3 ZFS</h4>
+ **可调参数类型**：支持大量文件系统级别的可调参数（属性 ）及少数系统级别的参数（`/etc/system` ）。通过`zfs(1M)`命令可列出文件系统属性，如`zfs get all zpool/var`可查看相关属性、当前值和来源 。
+ **关键文件系统级参数**：
    - `recordsize`：建议的文件系统块大小，默认128KB，设置时需与应用程序I/O匹配，小记录尺寸对小文件无效，因文件以动态记录尺寸存储。
    - `compression`：有`on|off|lzjb|gzip|gzip- [1-9]|zle|l4` 选项，轻量级算法（如lzjb ）在I/O堵塞情况下可提高性能。
    - `atime`：访问时间戳更新策略，可设置`on|off` ，`off`即禁用（引发读或写 ）。
    - `primarycache`** 和 **`secondarycache`：分别控制优先级数据和低优先级数据缓存污染，可设置`all|none|metadata` 。
    - `logbias`：同步写建议，可设`latency|throughput` ，`latency`使用日志设备，`throughput`提高设备写吞吐量。
    - `sync`：同步写行为，可设`standard|always|disabled` 。
+ **系统级参数示例**：如`zfs_txg_syncrate_ms`（目标TXG同步时间，单位ms ）、`zfs_txg_timeout`（TXG超时时间，单位s ）、`metaslab_df_free_pct`（metaslab发生转换时，以时间换空间的百分比 ） 。这些参数默认值不断下调，调整时需查阅供应商文档，且设置改变可能受公司或供应商规定限制。更多ZFS调优资料可参阅“ZFS Evil Tuning Guide” 。

<h2 id="jbukb">第九章 磁盘</h2>
<h3 id="enk5r">章节概述</h3>
    - **重要性**：磁盘I/O可能导致应用程序严重延时，是系统性能分析关键目标。消除磁盘I/O瓶颈可大幅提升性能和应用程序吞吐量 。
    - **磁盘类型**：包括旋转磁性盘片和基于闪存的固态盘（SSD），SSD提升了I/O性能，但容量和I/O速率需求增长使闪存设备也无法完全解决性能问题。
    - **章节结构**：分五部分，前三部分介绍磁盘I/O分析基础知识，后两部分在Linux和Solaris系统应用分析方法。
        * **背景**：介绍存储术语、磁盘设备基本模型及磁盘性能关键概念。
        * **架构**：概述存储硬件和软件架构。 
        * **方法**：描述性能分析的观察性和实验性方法。 
        * **分析**：介绍基于Linux和Solaris系统的磁盘性能分析与实验工具（跟踪和可视化工具 ）。 
        * **调优**：列举磁盘调优参数实例。

<h3 id="Iw0W9">9.1术语</h3>
    - **虚拟磁盘**：存储设备模拟，系统视为物理磁盘，可由多块磁盘组成。
    - **传输总线**：用于通信的物理总线，负责数据传输（I/O）及其他磁盘命令。 
    - **扇区**：磁盘上存储块，通常512B大小 。
    - **I/O**：磁盘I/O仅指读和写，由方向（读/写）、磁盘地址（位置）和大小（字节数）组成。 
    - **磁盘命令**：除读写外，还包括缓存写回等非数据传输命令。 
    - **吞吐量**：磁盘当前数据传输速率，单位B/s。 
    - **带宽**：存储传输或控制器能达到的最大数据传输速率。 
    - **I/O延时**：I/O操作执行时间，在操作系统领域有其他含义，指发起I/O的延时及后续数据传输时间。 
    - **延时离群点**：非寻常的高延时磁盘I/O 。 其他术语会在章节中穿插介绍，还可参考第2章和第3章的术语部分。

<h3 id="wW9kf">9.2 模型内容整理</h3>
<h4 id="dLbpr">9.2.1 简单磁盘</h4>
    - **原理**：现代磁盘有I/O请求队列，I/O请求在队列中等待或处理，类似超市收银口排队，适用于排队理论分析。
    - **优化算法**：磁盘管理器为优化性能，采用如旋转磁盘的电梯寻道算法（详见9.4.1节 ），或对读写I/O准备分开的队列（闪存盘适用 ）。

![](https://cdn.nlark.com/yuque/0/2025/png/45533403/1746336761388-6bcc23ad-5c0d-46b3-951e-f150dee90305.png)

**9.2.2 缓存磁盘**

    - **原理**：磁盘缓存加入后，部分读请求可从更快内存介质（如磁盘设备里的一小块DRAM ）返回。
    - **缓存类型及作用**：
        * 读缓存命中可带来低延时；缓存未命中仍会有高延时，是磁盘设备常见情况。
        * 盘上缓存可用作写回缓存提升写性能，数据写入缓存后即通知写入完成，之后再写入永久磁盘存储介质；与之相对的写穿缓存，需数据完全写入下一层才返回。

![](https://cdn.nlark.com/yuque/0/2025/png/45533403/1746336781561-f7bd8300-a3f7-4eb7-837a-a5269a440020.png)

**9.2.3 控制器**

    - **原理**：简单磁盘控制器（也叫主机总线适配器HBA ）连接CPU I/O传输总线、存储总线及相连磁盘设备。
    - **性能影响因素**：其性能受总线、磁盘控制器、磁盘等任何一个组件限制，更多信息见9.4节。

![](https://cdn.nlark.com/yuque/0/2025/png/45533403/1746336798425-7f7e814a-d4a9-437d-bb5d-6b3ad5a310d8.png)

<h3 id="pO1dv">9.3 概念内容整理</h3>
<h4 id="IgiCO">9.3.1 测量时间</h4>
    - **定义**：存储设备响应时间即磁盘I/O延时，是从I/O请求到结束的时间，由服务时间和等待时间组成。
    - **术语解释**：
        * **服务时间**：I/O主动处理（服务）时间，不包括队列等待时间。
        * **等待时间**：I/O在队列中等待服务的时间。
    - **测量角度**：
        * **操作系统角度**：服务时间从I/O请求发至磁盘设备开始算，到结束中断发生为止，不包括操作系统队列等待时间，反映磁盘设备对操作的总体性能。
        * **磁盘角度**：服务时间从磁盘开始服务I/O起算，不包括磁盘队列等待时间。早期磁盘简单，操作系统服务时间近似磁盘服务时间，现代磁盘有内部队列，操作系统指标称“磁盘响应时间”更准确。
        * **应用程序角度**：“I/O响应时间”可包括系统调用层以下时间总和（服务时间、等待时间和代码执行时间 ）。
    - **计算方法**：磁盘服务时间通常不直接测量，通过IOPS（每秒输入/输出操作次数 ）和使用率推算，公式为磁盘服务时间 = 使用率 / IOPS 。如60%使用率和300 IOPS得出平均服务时间2ms（600ms/300 IOPS ），前提是使用率反映单设备单次处理一个I/O的情况，而磁盘常并发处理多个I/O 。

![](https://cdn.nlark.com/yuque/0/2025/png/45533403/1746336894256-da5f85e3-1180-4019-b7a2-89c04ae01613.png)

<h4 id="fnun1">9.3.2 时间尺度</h4>
    - **范围**：磁盘I/O时间尺度从几十微秒到数千毫秒。最慢端，单个慢磁盘I/O影响应用响应时间；最快端，大量快I/O总和才可能出性能问题。
    - **示例**：表9.1列出磁盘I/O延时时间尺度示例，如磁盘缓存命中延时<100μs ，读闪存约100 - 1000μs ，机械磁盘连续读约1ms等。不同环境对I/O延时容忍度不同，企业存储领域大于10ms的磁盘I/O可能有性能问题，云计算领域基于Web应用程序可能容忍100ms以上延时 。磁盘可返回缓存命中（<100μs ）和缓存未命中（1 - 8ms或更慢 ）两种延时，平均延时表达可能误导，实际是双峰分布。

![](https://cdn.nlark.com/yuque/0/2025/png/45533403/1746336909883-a9de5b77-79e8-4637-87c3-3dfd52f85eb8.png)

<h4 id="PTLqT">9.3.3 缓存</h4>
    - **原理**：最佳磁盘I/O性能是无I/O，许多软件通过读缓存和写缓存避免磁盘I/O。
    - **类型**：表9.2列出磁盘I/O缓存类型，包括设备缓存（如ZFS vdev ）、块缓存（缓冲区高速缓存 ）、磁盘控制器缓存（RAID卡缓存 ）、存储阵列缓存（阵列缓存 ）、磁盘缓存（磁盘数据控制器DDC附带DRAM ）等，这些缓存对提高随机I/O负载性能重要。

![](https://cdn.nlark.com/yuque/0/2025/png/45533403/1746336921560-f137726b-ef5e-4e1e-86fc-edb31fe61397.png)

<h4 id="dDrvJ">9.3.4 随机 vs. 连续 I/O</h4>
根据磁盘上 I/O 的相对位置（磁盘偏移量），可用随机和连续描述磁盘 I/O 负载 ，第 8 章在文件访问模式中有相关讨论。

+ **连续负载**：也叫流负载 ，“流”一般用于应用程序层描述对“磁盘”（文件系统）的流式读写。
+ **随机与连续 I/O 在磁性旋转磁盘时代对比**：是研究重点。随机 I/O 因磁头寻道和盘片旋转产生额外延时（如图 9.5 ，磁头从扇区 1 到扇区 2 需寻道和旋转，理想是越直接越好）。性能调优工作包括识别并通过缓存、分离随机 I/O 到不同磁盘、减少寻道距离等方式排除随机 I/O 。 
+ **其他类型硬盘情况**：基于闪存的 SSD 执行随机和连续 I/O 通常差别不大，但因硬盘本身因素（如地址查找缓存应对连续 I/O ，对随机 I/O 无效）存在细微差别 。
+ **特殊说明**：从操作系统角度看到的磁盘偏移量不一定是物理磁盘偏移量，如虚拟磁盘映射情况。有时随机 I/O 并非通过检查偏移量确定，而是依据服务时间上升判断。

![](https://cdn.nlark.com/yuque/0/2025/png/45533403/1746368490468-f0b02b57-59dc-46e5-bc70-c632726fe95b.png)】

<h3 id="mktBH">9.3.5 读/写比</h3>
除随机或连续负载外，读写比例是衡量磁盘 I/O 负载的另一特征，与 IOPS、吞吐量相关，常以一段时间内比例表示，如“系统启动后读比例占 80%” 。理解该比例有助于系统设计配置，读频率高可增缓存提性能，写频率高可加磁盘提吞吐量和 IOPS 。读写模式多样，读可能随机，写可能连续，不同文件系统 I/O 大小有差异。 

<h3 id="he9I9">9.3.6 I/O 大小</h3>
I/O 平均大小（字节数）或分布是负载特征 。更大 I/O 通常提供更大吞吐量，存储单位 I/O 延时有所上升。磁盘子系统会改变 I/O 大小（如量化到 512 字节块） ，且 I/O 从应用层发起时，因文件系统等组件会放大或缩小。基于闪存设备对不同读写大小行为不同，理想 I/O 大小可查阅供应商文档或基准测试获得，当前 I/O 大小可通过观察工具获取。 

<h3 id="w32vz">9.3.7 IOPS 并不平等</h3>
因 I/O 随机或连续、I/O 大小、读写比等特征，IOPS 并非固定，不同设备和负载下不能简单比较 。如机械磁盘 5000 IOPS 连续负载可能比 1000 IOPS 随机负载快，基于闪存 IOPS 也难比较 。有意义的 IOPS 需包含随机或连续、I/O 大小、读写比等信息，还应考虑基于时间的指标（使用率、服务时间）反映性能。 

<h3 id="ewI71">9.3.8 非数据传输磁盘命令</h3>
磁盘除读写 I/O 外，还接收其他命令，如带缓存磁盘将缓存写回磁盘的命令，这类命令不传输数据但影响性能，会使磁盘运转导致其他 I/O 等待。 

<h3 id="aVPx9">9.3.9 使用率</h3>
使用率通过磁盘运行忙时占比计算 。0%使用率磁盘空闲，100%使用率磁盘持续 I/O 及其他磁盘命令，可能引发性能问题，实际中磁盘使用率常停留在 100%以下，因磁盘队列或操作系统排队影响，特定使用率下磁盘性能可能不再理想，合适使用率取决于磁盘、负载和延时需求，如 MD1 与 60%使用率 。为确定高使用率是否致应用程序问题，需研究磁盘反应时间及应用程序是否被 I/O 阻塞。磁盘 I/O 可能突发，使用率指标会被长时间统计摊平 。

+ **虚拟磁盘使用率**：操作系统对虚拟磁盘（如磁盘控制器）只能知忙时，难知底层硬件性能，导致虚拟与实际磁盘使用率有差异 。含写回缓存虚拟磁盘空载时看似不忙，实际底层磁盘可能忙；100%占用虚拟磁盘若基于多物理磁盘，可能还能处理更多工作；基于内存、有热插拔功能的虚拟磁盘使用率也难处理，操作系统应显示物理磁盘使用率供查看，物理磁盘达 100%使用率会致磁盘饱和。

<h4 id="qAGOd">9.3.10 饱和度</h4>
饱和度衡量因资源服务能力不足而排队的工作 ，对于磁盘设备，可通过操作系统磁盘等待队列长度（启用排队时）计算。100%使用率磁盘可能未饱和（有排队）或已饱和，使用率窗口会影响判断，如 50%使用率磁盘在某一时段内可能实际达 100%使用率 。了解 I/O 时间对判断饱和度很重要。 

<h4 id="lYdLQ">9.3.11 I/O 等待</h4>
I/O 等待是针对单个 CPU 的性能指标，指 CPU 分发队列中线程因磁盘 I/O 阻塞所耗空闲时间 ，区分 CPU 真正空闲时间和被磁盘 I/O 阻塞时间。高 I/O 等待时间可能意味着磁盘瓶颈致 CPU 等待 。该指标可能因其他进程执行而变化，如软件升级提高效率使 I/O 等待问题暴露 。在 Solaris 中，I/O 等待指标在 10 版被废除，用数值表示繁忙程度 ；在 Linux 中仍是常用指标，可识别磁盘瓶颈 ，非并发 I/O 更易因 I/O 等待暴露问题成为应用程序 I/O 瓶颈。 

<h4 id="v2RJw">9.3.12 同步 vs. 异步</h4>
如果应用程序和磁盘 I/O 异步，磁盘 I/O 延时不直接影响应用程序性能 ，常见于写回缓存场景。应用程序可预读执行异步避免阻塞 ，文件系统也可能预读 。即使应用程序同步等待 I/O ，其代码路径可能不在关键路径上，对客户端应用程序响应影响小，更多分析可参见第 8 章相关小节。 

<h4 id="h4CXq">9.3.13 磁盘 vs. 应用程序 I/O</h4>
磁盘 I/O 是文件系统、设备驱动等多个内核组件终点 ，与应用程序发出的 I/O 在频率和大小上常不匹配，原因包括：

+ 文件系统放大、缩小或不相关的 I/O 操作 。
+ 系统内存短缺导致的换页 。 
+ 设备驱动对 I/O 大小进行向上取整或碎片化处理 。

<h3 id="Lfx1p">9.4 架构</h3>
磁盘架构在容量规划和性能问题排查时需重点研究，判断问题源于架构选择还是负载及调优。

<h4 id="vR1fq">9.4.1 磁盘类型</h4>
+ **磁性旋转机械盘（HDD）**：由磁碟（涂有氧化铁颗粒，磁化方向存储数据）、机械臂和磁头构成 。磁碟旋转，机械臂带动磁头读写数据。其 I/O 受磁头寻道和盘片旋转影响，耗时数毫秒。连续 I/O 理想情况是请求 I/O 位于当前服务 I/O 结束位置，需寻道或等待盘片旋转的是随机 I/O 。降低寻道和旋转等待时间方法有：缓存；合理文件系统布置；分散负载到不同磁盘或系统；电梯寻道；使用高密度磁盘；分区配置；用更快转速磁盘（5400 转、7200 转、10000 转、15000 转 ）。最大吞吐量可通过“每磁道最大扇区数×扇区大小×rpm / 60s”计算 。还有短行程（外侧磁道负载，减少寻道时间）、扇区分区（外侧磁道长可记录更多扇区）、扇区大小（高级格式支持 4KB 扇区，提升吞吐量 ）等特性。磁盘缓存用小块内存缓存读写数据，SCSI 有标记命令排队（TCQ），SATA 有原生指令排队（NCQ） 。电梯算法（电梯寻道）重排 I/O 命令队列，提高效率 。磁盘每个扇区结尾有纠错码（ECC），用于验证和纠错 。振动会影响磁盘性能 。存在怠工磁盘，I/O 超时但不报错，增加问题排查难度 。
+ **基于闪存的 SSD**：正替代旋转磁盘，但旋转磁盘在经济的高密度存储（单位 MB 低成本 ）等场景仍有竞争力。

**基于闪存的 SSD**

+ **闪存**：多数 SSD 用 NAND 闪存制造，基于电子陷阱存储数据，可永久存储。闪存写入前需擦除，读/写性能不对称，写性能有差异 。单电平单元（SLC）一个单元存一位数据，多电平单元（MLC）存多位（常两位 ），三电平单元（TLC）存三位 ，eMLC 是面向企业的高级 MLC 。SLC 成本高但性能好，常用于企业级领域。
+ **控制器**：负责输入（页面读写，擦除后写，一次擦除 32 - 64 页 ）和输出（适配磁盘驱动器接口，指定扇区读写 ） 。闪存转换层（FTL）负责转换并跟踪空闲块，类似日志结构文件系统。写负载大且 I/O 尺寸小于闪存块大小时可能有写放大问题，部分驱动器用电池供电缓存缓解 。不同驱动器性能不同，需基准测试评估。TRIM 等命令可优化 SSD 性能 。
+ **寿命**：闪存存在耗尽、数据消失和读干扰问题 。控制器用磨损均衡技术延长寿命，企业级驱动器用超额配置存储和可靠闪存提高耐用性，消费级 MLC 驱动器写入周期有限 。
+ **病理学**：可能出现老化导致的延时离群点、碎片化造成的高延时、内部压缩导致的低吞吐量等问题 。

**接口**

+ **SCSI**：小型计算机系统接口，早期并行传输，带宽有限，后因竞争和同步等问题转向串行（SAS） 。
+ **SAS**：串行 SCSI 接口，高速点对点传输，避免总线冲突，支持链路聚合等特性 。
+ **SATA**：类似 SCSI，并行转串行，支持原生命令队列，采用 8b/10b 编码，在消费级和笔记本电脑大量使用 。

**存储架构**

+ **磁盘设备**：服务器内几块磁盘由操作系统分别控制，连接磁盘控制器，简单架构易分析，部分磁盘控制器支持“一堆磁盘（JBOD）”架构 。
+ **RAID**：高级磁盘控制器提供的冗余独立磁盘阵列架构 。由硬件实现的硬件 RAID 可卸载校验计算，软件 RAID（如 ZFS）可降低复杂度和硬件开销并提高监控能力 。
    - **种类**：RAID - 0（连接、条带）可提高随机读或 I/O 性能；RAID - 1（镜像）有不错随机和连续读性能，吞吐量受限于最慢磁盘 ；RAID - 10 结合 RAID - 0 和 RAID - 1 优势；RAID - 5 数据条带存储并带奇偶校验，写性能受读 - 改 - 写周期影响 ；RAID - 6 类似 RAID - 5 但有两块校验盘 。
    - **监控性**：硬件虚拟磁盘设备使操作系统监控难，软件实现的 RAID 管理类似普通磁盘设备 。
    - **读 - 改 - 写**：含校验数据条带存储时，写 I/O 可能因条带宽度产生额外读 I/O ，可调整条带宽度优化 。
    - **缓存**：RAID - 5 磁盘控制器可用写回缓存减少读 - 改 - 写性能下降，电池供电保证断电数据不丢失 。
    - **其他特性**：高级磁盘控制器卡有影响性能特性，如 Dell PERC 5 板卡的巡逻读、缓存刷新间隔等 。
+ **存储阵列**：接入多磁盘，用高级磁盘控制器配置 RAID ，提供大缓存（数 GB ）提升读写性能，常电池供电支持写回模式 。连接受外部存储控制器卡限制，常双通道连接提高性能可靠性 。 
+ **网络连接存储（NAS）**：通过网络暴露，支持 NFS、SMB/CIFS、iSCSI 等协议，是独立系统，性能分析需考虑网络因素 。

<h4 id="gHAeL">9.4.4 操作系统磁盘 I/O 栈</h4>
磁盘 I/O 栈组件和层次依操作系统、版本及软硬件技术而定 。

![](https://cdn.nlark.com/yuque/0/2025/png/45533403/1746369178169-10a06bcf-e4b4-4862-9036-04a06d11fe62.png)

+ **块设备接口**：早期 UNIX 创建，以 512B 块存取存储设备，提供缓冲区高速缓存提升性能，在 Linux 和 Solaris 仍存在，不过缓冲区高速缓存职责已缩小 。UNIX 有裸块设备 I/O（裸 I/O ）可绕过缓冲区高速缓存，通过特殊设备使用，在某些方面与文件系统“直接 I/O”特性类似 。块 I/O 设备可通过操作系统性能工具（如 iostat ）监控，也是静态跟踪常见位置，Linux 改进内核此部分并增加特性 。

![](https://cdn.nlark.com/yuque/0/2025/png/45533403/1746369187024-ae863287-10b6-4adc-a6b2-8320dbccaa1a.png)

+ **Linux**：其块层包括块设备接口、虚拟块驱动、提升层（含 I/O 调度器 ）、物理块驱动 。电梯层提供排序、合并、聚合请求发送等功能，提升吞吐量并降低 I/O 延时 。I/O 调度器可对 I/O 排队排序（或重新调度 ），策略有：
    - **空操作**：无需调度时使用，如 RAMdisk 。
    - **截止时间**：设定截止时间解决延时和饥饿问题，用三个队列（读 FIFO、写 FIFO、排序队列 ） 。
    - **预期**：预测 I/O 性能减少磁头寻道 。 
    - **CFQ**：公平分配 I/O 时间片，可通过 ionice 命令调整用户进程优先级 。I/O 调度后请求放入块设备队列等待发送 。

![](https://cdn.nlark.com/yuque/0/2025/png/45533403/1746369194917-38a43966-3d1c-4ddb-90b0-92dc9d1bd201.png)

+ **Solaris**：内核用简单块设备接口，目标驱动程序（sd ）有队列 。高级 I/O 调度由 ZFS 提供，可按优先级排序和合并 I/O （含跨间隔合并 ） 。ZFS 集卷管理器和文件系统为一体，管理虚拟磁盘设备和 I/O 队列 。底层驱动程序依服务器硬件和配置，有目标设备驱动程序（sd、ssd ）、多路径 I/O 驱动程序（sesi_vhci、mpxio ）、主机总线适配器驱动程序（pmcs、mpt、nv_sata、ata ） 。本文围绕磁盘I/O分析和调优方法展开，具体内容如下：

<h3 id="lunJI">9.4 磁盘性能方法汇总</h3>
| 方法 | 类型 |
| --- | --- |
| 工具法 | 观察分析 |
| USE方法 | 观察分析 |
| 性能监控 | 观察分析、容量规划 |
| 负载特征归纳 | 观察分析、容量规划 |
| 延时分析 | 观察分析 |
| 事件跟踪 | 观察分析 |
| 静态性能调优 | 观察分析、容量规划 |
| 缓存调优 | 观察分析、调优 |
| 资源控制 | 调优 |
| 微基准测试 | 实验分析 |
| 伸缩 | 容量规划、调优 |


<h3 id="d7Gdo">方法介绍</h3>
+ **工具法**：使用可用工具检查关键指标，可能用到iostat（找繁忙磁盘、高平均服务时间、高IOPS ）、iotop（找引发磁盘I/O的进程 ）、dtrace/stap/perf（含iosnoop工具，查磁盘I/O延时 ）及磁盘控制器专用工具。发现问题后检查工具数据了解细节，也可结合其他方法。
+ **USE方法**：早期性能调查时找瓶颈和错误。磁盘设备检查使用率（忙碌时间）、饱和度（I/O排队程度）、错误；磁盘控制器检查使用率（对比最大吞吐量和操作频率 ）、饱和度（I/O等待程度 ）、错误（控制器错误 ）。
+ **性能监控**：发现一段时间内问题和行为模式，重要指标有磁盘使用率、响应时间。高使用率和响应时间增长可能影响性能，指标应按磁盘分别检查，可结合其他数据如IOPS和吞吐量做容量规划。
+ **负载特征归纳**：容量规划等重要活动，归纳磁盘I/O负载特征（I/O频率、吞吐量、大小、随机和连续比例、读写比 ），记录平均值、最大值等。还可从系统、磁盘、控制器角度描述负载，列出高级检查清单及性能特征归纳问题。 
+ **延时分析**：调查磁盘I/O请求到中断间时间找延时来源。通过分析I/O栈各层延时判断，如I/O A延时集中在磁盘层，I/O B延时来自文件系统层。每层延时可通过单位时间I/O平均值（系统工具报告 ）、I/O全分布（直方图或热图 ）、单位I/O延时值体现。

![](https://cdn.nlark.com/yuque/0/2025/png/45533403/1746369325660-0ff5ba5f-1278-4d0d-ade9-93430b5de328.png)

<h4 id="aLzxO">事件跟踪</h4>
跟踪每个I/O事件信息并记录，写入日志文件供日后分析。日志至少包含磁盘设备ID、I/O类型（读/写 ）、偏移量（磁盘位置 ）、大小（字节数 ）、请求时间戳（发设备时间 ）、完成时间戳（完成中断时间 ）、完成状态（错误 ），还可能有PID、UID等。可利用静态跟踪点跟踪请求和完成，用动态跟踪进行高级分析，捕捉块设备驱动I/O、接口驱动命令、磁盘设备驱动命令。 

<h4 id="drb4b">静态性能调优</h4>
关注磁盘配置环境问题，检查磁盘数量、类型、版本，磁盘控制器数量、接口类型、插槽情况、固件版本，RAID配置（包括条带宽度 ），多路径可用性及配置，磁盘设备驱动版本，存储设备驱动及操作系统补丁，磁盘I/O资源操作等，排查被忽视配置问题。 

<h4 id="FyrEi">缓存调优</h4>
系统存在多种缓存，如应用程序、文件系统、磁盘控制器和磁盘自身缓存。先检查缓存是否投入使用、使用情况、大小，再根据缓存调整负载，或根据负载调整缓存。 

<h4 id="CsATb">资源控制</h4>
操作系统提供方法分配磁盘I/O资源，如固定IOPS和吞吐量极限，或更灵活共享资源，具体机制与实现相关。 

<h4 id="sheDx">微基准测试</h4>
通过操作系统设备接口测试磁盘I/O，避开文件系统干扰。测试参数包括方向（读/写 ）、磁盘偏移量模式（随机/连续 ）、偏移量范围、I/O大小、并发度、设备数量。磁盘测试有最大吞吐量、操作频率、随机读IOPS等建议负载；磁盘控制器测试有最大吞吐量、操作频率建议负载。 

<h4 id="gO2Y7">伸缩</h4>
磁盘和磁盘控制器有吞吐量和IOPS极限，可通过微基准测试得出。调优无法满足性能需求时需伸缩磁盘。基于资源的容量规划方法：确定目标负载吞吐量和IOPS；考虑缓存，计算单位磁盘I/O的CPU周期及所需CPU数量；设置目标使用率（如50% ）并按比例伸缩；检查传输极限并伸缩传输线；根据磁盘和类型选取合适最大单位磁盘吞吐量和IOPS数值，可结合微基准测试和负载特征归纳确定I/O类型极限。

<h3 id="ED47x">磁盘I/O性能分析工具介绍</h3>
介绍基于Linux和Solaris操作系统的磁盘I/O性能分析工具，涵盖工具使用策略及各工具功能、选项和输出信息等内容。

| 工具 | Linux | Solaris | 描述 |
| --- | --- | --- | --- |
| iostat | 有 | 有 | 汇总单个磁盘统计信息，提供负载特征归纳、使用率和饱和度指标 |
| sar | 有 | 有 | 磁盘历史统计信息 |
| pidstat、iotop | 有 | iotop | 按进程列出磁盘I/O使用情况 |
| blktrace | 有 | iosnoop | 磁盘I/O事件跟踪 |
| DTrace | 有 | DTrace | 自定义静态和动态跟踪 |
| MegaCli | 有 | 无 | LSI控制器统计信息 |
| smartctl | 有 | 无 | 磁盘控制器统计信息 |


<h4 id="eran5">iostat工具详解</h4>
+ **功能概述**：汇总单个磁盘统计信息，是调查磁盘I/O问题常用命令，统计信息由内核维护。名字源于“IO Statistics” ，不同操作系统版本有差异，可通过sysstat添加到Linux系统，Solaris系统默认包含。 
+ **Linux系统下iostat**
    - **常用选项**： -c显示CPU报告； -d显示磁盘报告； -k使用KB代替块数目； -m使用MB代替块数目； -p包括单个分区统计信息； -t时间戳输出； -x扩展统计信息； -z不显示空活动汇总 。默认 -c和 -d报告，指定其一则不显示另一个 。
    - **输出信息**：默认显示系统总结信息（内核版本等 ），然后是CPU和磁盘设备统计信息。磁盘设备信息包括tps（每秒事务数/IOPS ）、kB_read/s（每秒读取KB数 ）、kB_wrtn/s（每秒写入KB数 ）等 。扩展输出（ -x选项 ）包含avgqu -sz（平均请求数 ）、await（平均I/O响应时间 ）等对分析有用信息 。还可通过 -t添加时间戳， -p ALL引入每个分区统计信息 。
+ **Solaris系统下iostat**
    - **常用选项**：使用 -h列出所有选项（虽提示 “非法选项” 错误 ）。 -I打印计数而非计算的时间区段汇总； -n显示易理解的磁盘设备名； -z忽略全零行（空闲设备 ）； -C显示每个控制器统计信息； -p显示每个分区信息； -Td使输出附时间戳； -e加入错误计数器 。
    - **输出信息**：默认输出包含终端输入输出字符、磁盘设备信息（kps每秒KB数、tps每秒事务数、serv服务时间 ）、CPU统计信息 。其他输出信息包括r/s（每秒读取数 ）、kr/s（每秒读取KB数 ）、wait（块驱动队列等待请求数 ）等 。可通过选项获取不同维度统计信息，如错误计数器等 ，也可利用选项实现类似USE方法中磁盘使用率（ %b ）、饱和度（ actv 、wait ）、错误（ errors tot ）等指标获取 。

<h3 id="shPBG">sar工具</h3>
系统活动报告器，可监控当前活动，归档和报告历史统计信息。通过 -d选项输出磁盘汇总信息。

+ **Linux系统**：输出与iostat类似，tps指设备每秒数据传输量，rd_sec/s、wr_sec/s为每秒读取和写入扇区数（512B ） 。
+ **Solaris系统**：输出列与iostat(1M)扩展模式类似，如 %busy对应iostat(1M)的 %b ，await和asversv对应iostat(1M)的wsvc_t和asvc_t 。

<h3 id="zybyN">pidstat工具（Linux）</h3>
默认输出CPU使用情况， -d选项可输出磁盘I/O统计信息（内核2.6.20及以上版本可用 ），输出包括kb_rd/s（每秒读取KB数 ）、kb_wd/s（每秒发出写入KB数 ）、kb_ccwr/s（每秒取消写入KB数 ） ，仅超级用户可访问不属于自己进程的磁盘统计信息（通过 /proc/PID/io获取 ）。 

<h3 id="Wcw39">DTrace工具</h3>
能从内核角度检查磁盘I/O事件，支持工作负载特征归纳和延时分析。

+ **provider**：分为稳定和不稳定provider ，磁盘I/O栈主要用io provider ，其探测器包括io:::start（I/O请求发设备 ）、io:::done（I/O请求设备上完成 ）等 ，探测器参数提供I/O详细信息，如I/O大小、设备偏移量等 。
+ **事件跟踪**：可跟踪每个磁盘I/O请求，显示PID、进程名和I/O大小 。
+ **I/O大小汇总**：可按应用程序名称等汇总磁盘I/O大小，生成分布图可视化全分布 ，还可按设备名、I/O方向等汇总 。
+ **I/O寻道汇总**：通过脚本计算同一应用程序对同一设备连续I/O间寻道距离，按进程输出直方图 。 
+ **I/O延时汇总**：脚本跟踪块I/O开始和结束事件，将延时分布汇总成直方图 。
+ **I/O栈**：统计I/O请求内核调用栈，显示从系统调用接口开始到块设备驱动的代码路径及栈出现次数 ，可用于调查额外I/O 。
+ **SCSI脚本**：跟踪SCSI层，报告I/O结束原因 。
+ **高级跟踪**：动态跟踪内核I/O栈各层，不同脚本实现不同功能，如显示磁盘I/O统计信息、I/O等待队列时间分布图等 ，但与内核实现紧密相关，需随内核更新维护。

<h4 id="enF47">SystemTap（Linux）</h4>
可动态跟踪磁盘I/O事件，用ioblock provider进行静态跟踪，相关帮助信息参考第4章4.4节及附录E。 

<h4 id="yLnOo">perf（Linux）</h4>
提供跟踪点跟踪基本信息，如block:block_rq_issue等。示例中通过perf record -age block:block_rq_issue sleep 10跟踪块设备问题，展示产生块设备I/O的不同代码路径。 

<h4 id="u5A7i">iotop</h4>
含磁盘I/O版本的top工具，第一版基于Solaris系统用DTrace开发 。

+ **Linux**：需内核2.6.20或更新版本及特定内核选项。选项可定制输出，如 -o仅显示I/O进程， -a输出累计I/O 。默认清屏输出一秒内汇总信息，批量模式（ -b ）可滚动输出。
+ **Solaris**：位置可能在 /opt/DTT或 /usr/DTT等。默认输出间隔5秒，统计信息以字节为单位， -P显示磁盘使用率， -C滚动输出结果。还有为SystemTap开发的版本desktop.stp，通过跟踪VFS实现对磁盘读写的跟踪。

<h4 id="HgRxt">iosnoop（基于DTrace）</h4>
通过块设备接口同时跟踪所有磁盘，为每个磁盘I/O打印输出，有多个可选输出细节的选项。可跟踪磁盘I/O ，如启动vim时的I/O情况，输出包括PID、进程名、I/O方向、块地址、I/O大小、文件系统路径名等 。还可通过DTIME（us）和DELTA（us）列分析I/O响应时间及从I/O结束到前一次磁盘事件的时间 。 

<h4 id="WhdRF">blktrace（Linux）</h4>
块设备I/O事件自定制跟踪工具，包括blktrace(8)、blkparse(1)和btrace(8) 。blktrace(8)启用内核驱动跟踪机制获取数据，供blkparse(1)处理生成可读输出，btrace(8)调用前两者 。默认输出包括设备主要、次要编号，CPU ID，序号，活动时间，进程ID，活动标识符，RWBS描述等 。可通过 -f自定义输出，还能通过选项过滤事件类型，如 -a issue仅跟踪发出I/O 。 

<h4 id="WmDdx">MegaCli（针对LSI磁盘控制器 ）</h4>
某些特定磁盘控制器（如LSI ）的分析工具，可查看控制器事件，如巡检读开始和完成事件 。还有很多选项可显示适配器、磁盘设备、虚拟设备、机箱、电池状态和物理错误等信息，帮助发现配置问题和错误。 

<h4 id="Aa15J">smartctl（Linux ）</h4>
用于获取磁盘的SMART（自监控、分析和报告技术 ）数据，包括多种健康统计信息，如设备型号、序列号、温度、错误计数器、SMART健康状态等 。但不能像内核跟踪框架一样解答单个磁盘慢I/O问题。 

<h4 id="TCO7X">可视化</h4>
性能监控解决方案常将磁盘IOPS、吞吐量和使用率绘成折线图，体现基于时间的模式，但平均值可能掩盖不均衡行为 。还可通过散点图等方式可视化磁盘I/O性能相关数据。

<h4 id="aTK2y">主动测试磁盘I/O性能工具及方法</h4>
+ **Ad Hoc测试**：利用dd(1)命令（设备到设备拷贝 ）进行特定连续磁盘性能测试，如用 “dd if=/dev/sda1 of=/dev/null bs=1024k count=1k” 执行1MB的I/O连续读测试 。理想情况下，磁盘设备路径为字符特殊文件可直接施加载荷，Solaris系统在 /dev/rdsk默认提供，Linux系统若有raw(8)命令可在 /dev/raw创建字符特殊版本文件，使用块特殊文件需考虑缓冲 。连续写测试方式类似，但要注意避免销毁磁盘数据。 
+ **自定义负载生成器**：需自行编写负载生成器（如一段短C程序 ）对设备路径施加载荷，并用iostat(1)测量性能。Linux上，块设备特殊文件可用O_DIRECT打开避免缓冲，使用高级语言时尽量用系统级别接口避免缓冲（如Perl的sysread() ） 。 
+ **微基准测试工具**：如Linux上的hdparm(8) ，-T选项测试带缓存的读， -t选项测试磁盘设备读，结果显示缓存命中和未命中的差异 。使用时需研究工具文档，了解相关警告，更多微基准测试背景信息参见第12章 。

<h4 id="heTS3">随机读实验示例</h4>
开发自定义工具对磁盘设备路径施加随机8KB读负载，一至五个工具实例并发执行，同时运行iostat(1) 。

+ **Linux结果**：展示不同设备的rqm/s（每秒合并请求数 ）、r/s（每秒读请求数 ）等指标数据，可见avgqu -sz（平均请求队列长度 ）列等差递增，await（平均I/O响应时间 ）列延时增加 。
+ **Solaris结果**：同样展示相关指标数据，actv（设备上的活动请求数 ）列等差递增，asvc_t（设备上的平均服务时间 ）列延时增加 。该测试针对以RAID卡为后端的虚拟磁盘设备，其可接纳多个并发I/O（上限256 ，由sd_max_throttle定义 ） ，物理磁盘设备并发设置较低，I/O更早进入驱动队列，使wait列数值增加而非actv 。

<h4 id="tDtoa">操作系统可调参数</h4>
+ **ionice**：Linux的ionice(1)命令可设置进程I/O调度级别和优先级。调度级别有0（无，内核选默认值 ）、1（实时，最高级别访问 ）、2（默认 ）、3（空闲，磁盘空闲后允许I/O ） 。示例 “ionice -c 3 -p 1623” 将ID为1623的进程放入空闲I/O调度级别，适合长时运行的备份任务，避免与生产负载冲突。
+ **资源控制**：现代操作系统（如Linux的控制组cgroups ）可自定义资源控制方式，管理磁盘或文件系统使用情况，能按比例或固定限制进行存储设备资源控制，包括读写及IOPS、吞吐量控制 。Solaris有ZFS I/O流控，在文件系统级别按区控制I/O 。
+ **可调参数**：操作系统有可调参数，如Linux的 /sys/block/sda/queue/scheduler可选择I/O调度器策略；Solaris的sd_max_throttle限制sd存储设备同时运行的最大命令数，可通过命令分析当前值与上限距离 ，如需调整可参考厂商文档 。

<h4 id="VnfER">磁盘设备可调参数</h4>
Linux的hdparm(8)工具、Solaris的format(1M)命令可设置多种磁盘设备可调参数。 

<h4 id="t1gT8">磁盘控制器可调参数</h4>
不同磁盘控制器型号和厂商有不同可调参数。以Dell PERC 6卡为例，通过MegaCli命令可获取如Adaptive Fail Poll Interval（自适应故障轮询间隔 ）、Interrupt Throttle Active Count（中断节流活动计数 ）等设置信息，具体内容在厂商文档中有详细描述。

<h2 id="gT3Wg">第十章  网络</h2>
<h3 id="D5t7G">章节概述：</h3>
+ 在系统日益分布式（尤其云计算环境）下，网络在性能方面愈发关键，任务包括改进网络延时、吞吐量，消除丢包引发的延时异常。网络分析涉及硬件（物理网络组件如网卡、交换机等）和软件（内核协议栈如TCP/IP 及相关协议行为）。因网络易因性能差受责，本章旨在挖掘实际情况、排除网络责任以推进分析。由五部分构成：
    1. **背景**：介绍网络术语、基本模型和关键性能概念。
    2. **架构介绍**：讲解硬件网络组件和网络栈。 
    3. **方法**：阐述性能分析的观察法和实验法。 
    4. **分析**：说明Linux和Solaris系统中的网络性能工具和实验。 
    5. **调优**：讲解可调节参数范例。

<h3 id="QgHjV">10.1术语</h3>
    6. **接口**：“接口端口”指网络物理连接器；“接口”或“连接”指操作系统可见且可配置的网络接口端口逻辑实例。
    7. **数据包**：通常指IP级可路由的报文。
    8. **帧**：物理网络级报文，如以太网帧。 
    9. **带宽**：对应网络类型的最大数据传输率，单位b/s ，如10GbE是10Gb/s的以太网。 
    10. **吞吐量**：当前两个网络端点间的数据传输率，单位b/s或B/s 。 
    11. **延时**：一个报文往返端点时间或建立连接时间（如TCP握手），不含后续数据传输时间。 其他术语会在本章贯穿，也可参考第2和3章术语部分。

<h3 id="adlhX">10.2 模型</h3>
<h4 id="H0RKk">10.2.1 网络接口</h4>
    - 网络接口是网络连接的操作系统端点，是系统管理员可配置和管理的抽象层。它被映射到物理网络端口，端口连接网络，有分离的传输和接收通道 。

<h4 id="mBwIG">10.2.2 控制器</h4>
    - 网络接口卡（NIC）为系统提供一个或多个网络端口，设有网络控制器，即端口与系统I/O传输通道间传输包的微处理器 。控制器可作为独立板卡或内置于系统主板。

<h4 id="IGH3Y">10.2.3 协议栈</h4>
    - 网络通信由协议栈组成，每层实现特定目标。TCP/IP已成为标准，了解OSI模型也有帮助，如OSI会话层常以BSD套接字形式出现在TCP/IP栈里 。

<h3 id="PnpIx">10.3 概念</h3>
<h4 id="d2OSr">10.3.1 网络和路由</h4>
    - 网络是由网络协议地址相连的主机组，分多个网络是出于扩展性等原因。广播报文可通过子网隔离。路由管理报文跨网络传递，路由包地址信息在IP数据包头中 。有单播（一对一）和多播（一对多）传输 。

<h4 id="mGRF5">10.3.2 协议</h4>
    - 网络协议如IP、TCP、UDP是系统与设备通信必要条件。不同协议版本（如IPv4和IPv6 ）因内核代码路径不同有不同性能特性。系统可调参数也影响协议性能。包长影响性能，TCP/IP和以太网包长54 - 9054B 。

<h4 id="rwaol">10.3.3 封装</h4>
    - 封装在负载前后添加元数据（包头、包尾 ），增加报文长度和传输开销。

<h4 id="rwrPK">10.3.4 包长度</h4>
    - 包长受网络接口最大传输单元（MTU）限制，以太网多设为1500B ，也支持近9000B的巨型帧，可提高网络吞吐、降低延时。但老旧硬件和错误配置防火墙会影响巨型帧接收，为避免问题，许多系统用1500 MTU 。网络接口卡功能（如TCP卸载、大块段卸载 ）提升了1500 MTU帧性能 。

<h4 id="TX6uK">10.3.5 延时</h4>
+ **主机名解析延时**：与远程主机建立连接时，主机名解析为IP地址过程所需时间，如DNS解析，最坏情况超时需几十秒，禁用可避免该延时。
+ **ping延时**：用ping命令测量ICMP echo请求到echo响应的时间，衡量主机间网络跳跃的往返延时，简单易用，许多操作系统默认响应。接收端ICMP echo请求常做中断处理，发送端测量含内核上下文切换等时间 。
+ **连接延时**：传输数据前建立网络连接的时间，对于TCP连接即TCP握手时间，从客户端发送SYN到接收SYN - ACK的时间 。类似ping延时，但包含更多建立连接及重传丢包的内核代码运行时间 。
+ **首字节延时**：从连接建立到接收第一个字节数据的时间，包括远程主机接受连接、调度线程及发送首个字节等时间，涵盖目标服务器处理延时 。
+ **往返时间**：网络包在两个端点往返所需时间。
+ **连接生命周期**：网络连接从建立到关闭的时间，一些协议支持长连接策略以减少系统开销和连接延时。

<h4 id="M8NB6">10.3.6 缓冲</h4>
+ 发送端和接收端的缓冲可使网络吞吐量保持高速率，较大缓冲能缓解往返延时影响 。TCP利用缓冲和可变发送窗口提升吞吐量，网络套接字及应用程序也可能利用缓冲 。外部网络组件（如交换机、路由器 ）使用高缓冲可能导致缓冲膨胀问题，影响性能，Linux 3.x内核有相关解决功能 。缓冲由端点提供效果最佳，遵循端到端原则。

<h4 id="GuIAI">10.3.7 连接积压队列</h4>
+ TCP的积压队列用于暂存SYN请求，当连接请求过多超过处理能力，积压队列满会丢弃客户端SYN包，其重传会增加连接时间 。测量积压队列导致的丢包可衡量网络连接饱和度。

<h4 id="wKkbr">10.3.8 接口协商</h4>
+ 网络接口通过与对端自动协商工作于不同模式，如以太网带宽有10、100、1000、10000MB/s 等，双工模式有半双工和全双工 。网络接口通常用较低速率协商，全双工可双向同时传输利用全部带宽，半双工仅单向传输 。

<h4 id="N1yXK">10.3.9 使用率</h4>
+ 网络接口使用率用当前吞吐量除以最大带宽计算，全双工下分别计算每个方向使用率，以该方向当前吞吐量除当前协商带宽 。网络接口使用率达100%会成瓶颈。一些操作系统性能统计按包报告，因包长度可变，难以通过吞吐量或使用率建立包与字节数联系 。

<h4 id="IICIW">10.3.10 本地连接</h4>
+ 同一系统的两个应用程序间的网络连接为本地连接，使用虚拟网络接口（回送接口 ）。分布式应用程序环境中同主机的组件（如Web服务器、数据库服务器、应用服务器 ）通过本地连接通信 。
+ 通过IP用本地套接字（UNIX域套接字UDS ）通信，UDS省略TCP/IP内核代码及协议封装开销，性能更好 。对于TCP/IP套接字，内核检测到本地连接可短路TCP/IP栈以提高性能，在Solaris系统中称为TCP融合 。

<h3 id="Rxry7">10.4 架构</h3>
<h4 id="JgElR">10.4.1 协议</h4>
+ **TCP**
    - **性能特性**：传输控制协议，用于建立可靠网络连接。利用缓冲和可变窗口在高延时网络提供高吞吐量，通过阻塞控制避免过度发送。特性包括可变窗口（根据接收方意愿调整发送量）、阻塞避免（防止网络阻塞）、慢启动（小阻塞窗口开始递增） 、选择性确认（减少重要重传）、快速重传（重传确认非连续包）、快速恢复（检测重复确认后恢复性能） 。
    - **重要内容**
        * **三次握手**：主机间建立连接需三次握手，服务器监听，客户端发起连接 。
        * **重复确认检测**：发送方根据接收方确认包判断包是否丢失，用于快速重传和恢复 。
        * **阻塞控制（Reno和Tahoe ）**：BSD 4.3引入算法，Reno在三次重复确认时减半阻塞窗口等；Tahoe触发快速重传等 。部分系统可选择算法调优。
        * **Nagle算法**：推迟小尺寸包传输，减少网络小包数量，系统可提供禁用参数 。
        * **延时ACK**：推迟最多500ms发送ACK，合并多个ACK及控制报文，减少网络包数量 。
        * **SACK与FACK**：SACK允许接收方通知发送方收到非连续数据块；Linux默认支持FACK（基于SACK扩展 ），更好控制未完成数据传输，提高性能 。
+ ![](https://cdn.nlark.com/yuque/0/2025/png/45533403/1746370740882-4788c66d-af82-49cc-9a11-88fc8ec71827.png)
+ **UDP**
    - **性能特性**：用户数据报协议，用于发送网络数据报文。协议头简单短小，降低计算和长度开销；无状态，降低连接与传输控制开销；无重新传输，增加连接延时，不保证数据可靠，可能丢失或乱序，不适合多数连接，缺乏阻塞避免 。一些服务（如某些版本NFS ）可按需选用TCP或UDP，广播、多播服务只能用UDP 。

<h4 id="kKwx8">10.4.2 硬件</h4>
+ **接口**：物理网络接口发送接收帧，处理传输错误，分电学、光学或无线类型，对应第2层网络标准 。不同接口带宽不同（如以太网有1GbE、10GbE等 ），带宽越高延时越低、成本越高。接口使用率用吞吐量除以协商带宽衡量，多接口全双工时各通道需分别研究 。
+ **控制器**：物理网络接口由控制器提供，集成于系统板或扩展卡，由微处理器驱动，通过I/O传输通道（如PCI ）接入系统，可能成为网络吞吐量瓶颈 。
+ **交换机、路由器**
    - **交换机**：为连入主机提供专用通信路径，允许多主机同时传输，取代集线器，避免“载波侦听多路访问/碰撞检测”冲突问题 。
    - **路由器**：在网络间传递数据包，用网络协议和路由表确定最佳路径，平衡负载，因多个路径存在，数据包可能乱序，可能受网络拥塞影响 。
+ **其他**：网络设备还包括集线器、网桥、中继器、调制解调器等，可能影响网络性能 。

<h4 id="bZycj">10.4.3 软件</h4>
+ **网络栈**
    - 网络通信软件包含网络栈、TCP和设备驱动程序。现代内核中网络栈多线程，传入包可由多个CPU处理，可基于IP地址哈希或就近CPU原则分配 。Linux和Solaris系统有不同框架支持该行为。

![](https://cdn.nlark.com/yuque/0/2025/png/45533403/1746370830785-80c57501-54c5-4652-a38f-74ff54116604.png)

+ **Linux**
    - TCP、IP及通用网络驱动软件是内核核心组件，设备驱动程序为附加模块，数据包以struct sk_buff数据类型在内核组件间传递 。
    - 数据包高处理率可通过多个CPU处理包和TCP/IP栈实现，如Linux 3.7内核有多种方式：
+ ![](https://cdn.nlark.com/yuque/0/2025/png/45533403/1746370843365-e1f9a225-c610-441d-b82e-2ab6d01159f7.png)
        * **RSS（接收端缩放）**：<u><font style="color:#DF2A3F;">现代NIC支持多队列，根据包哈希（如IP地址、TCP端口号 ）置入不同队列，由不同CPU处理，同一连接包可由同一CPU处理。</font></u>
        * **RPS（接收数据包转向）**：针对不支持多队列NIC的RSS软件实现，短中断服务例行程序按包头字段哈希映射数据包到CPU。 
        * **RFS（接收转向）**：类似RPS，但偏向之前处理套接字的CPU，提高CPU缓存命中率和内存本地性。 
        * **加速接收数据流转向**：支持该功能NIC的RFS硬件实现，用流信息更新NIC确定中断哪个CPU 。
        * **XPS（传输数据包转向）**：支持多传输队列NIC，允许多个CPU传输队列 。缺乏CPU负载均衡策略时，NIC可能中断同一CPU致其使用率100%成瓶颈，可通过如RFS实现的缓存一致性等策略，或irqbalancer进程分配中断请求给CPU提升网络性能 。
+ **Solaris**
    - 套接字层是内核的sockfs模块，TCP、UDP和IP协议合并为ip模块，数据包以消息块mblk_t穿过内核 。
    - GLDv3软件利用垂直视野（每CPU同步机制 ）提升性能，用序列化队列（squeue）处理每个连接 。激活IP fanout可在多个CPU间负载均衡传入数据包 。Erik Nordmark在Solaris IP Datapath Refactoring项目中简化网络栈内部结构 。

![](https://cdn.nlark.com/yuque/0/2025/png/45533403/1746370868445-6c13bb55-10be-460b-91b8-a2175f102743.png)

+ **TCP**
    - **积压队列**：突发连接由积压队列处理，有SYN积压队列（处理TCP握手完成前未完成连接 ）和侦听积压队列（处理等待应用程序接收的已建立会话 ） 。早期内核单队列易受SYN洪水攻击，双队列可优化处理，队列长度可调整 。

![](https://cdn.nlark.com/yuque/0/2025/png/45533403/1746370877855-41db5e93-40f5-43b8-b00e-c76dfce6d891.png)

    - **缓冲**：利用套接字的发送和接收缓冲提升数据吞吐量。写通道中，数据缓存在TCP发送缓冲区，再送IP发送，TCP尽量按MSS长度发送段避免IP分段 。发送和接收缓冲区大小可调，更大尺寸以消耗更多内存为代价提升吞吐性能，Linux内核可基于连接活跃度自动增加缓冲区 。

![](https://cdn.nlark.com/yuque/0/2025/png/45533403/1746370888505-dacb6a4e-1b06-4202-85b2-b1dfb3b8fa75.png)

+ **网络设备驱动**
    - 网络设备驱动有环形缓冲区，用于在内核内存与NIC间收发数据包 。随着10GbE引入，利用中断结合模式（如动态轮询 ），中断仅在计时器激活或达到一定数据包数量时发送，减少内核与NIC通信频率，缓冲更多发送，提升吞吐量，牺牲一定延时 。

<h3 id="jj2zn">10.5 方法</h3>
本章论述网络分析和调优的多种方法，这些方法可单独或混合使用，建议使用顺序为性能监测、USE方法、静态性能调优和工作负载特征归纳 。

+ **工具法**：遍历可用工具，检查关键指标，可能忽略工具不可见或低可见度问题，操作较费时。用于网络通信时，可检查netstat -s（查高流量重传和乱序数据包 ）、netstat -i（查接口错误计数器 ）、ifconfig（仅限Linux，查“错误”“丢弃”“超限” ）、吞吐量（Linux用ip(8) ，Solaris用nicstat(1)或dladm(1M) ）、tcpdump/snoop（短期使用查网络使用者和可消除操作 ）、dtrace/stap/perf（查应用程序与线路选中数据 ） 。发现问题需检查工具各字段了解更多上下文。
+ **USE方法**：定位瓶颈和跨组件错误，对每个网络接口及传输（TX）、接收（RX）方向，检查使用率（接口忙于收发帧时间 ）、饱和度（接口负载、额外队列、缓冲或阻塞程度 ）、错误（接收校验错误等，传输延时碰撞 ） 。操作系统监控工具一般不直接提供使用率，可用当前吞吐量除以协商带宽计算。实施网络带宽限制环境中，使用率或需按应用限制测量，饱和度可通过网络接口、内核统计信息及TCP层重传统计信息衡量。USE方法可用于网络控制器与处理器间传输通道，基于网络接口统计信息和网络拓扑推测指标 。
+ **工作负载特征归纳**：做容量规划、基准测试和负载模拟时，分析应用负载特征重要。网络工作负载基础属性包括网络接口吞吐量（RX和TX，B/s ）、网络接口IOPS（RX和TX，帧/秒 ）、TCP连接率（主动和被动，连接数/秒 ） 。这些特征随时间变化，需随时间推移监测。高级工作负载特征归纳可涵盖平均数据包大小、协议类型、活跃端口、使用网络的进程等信息 。
+ **延时分析**：研究不同网络延时有助于理解和表述网络性能，常见网络延时包括系统调用发送/接受延时、系统调用连接延时、TCP连接初始化时间、TCP首字节延时、TCP连接持续时间、TCP重传输、网络往返时间、中断延时、跨栈延时等 。延时可通过单位时间间隔平均（隔离中间网络差别 ）、完整分布（直方图或热度图 ）、每操作延时（列出事件源与目的IP地址 ）展示，TCP重传输可能导致延时异常值，可用完整分布或每操作延时跟踪发现 。
+ **性能监测**：发现当前问题及随时间推移的行为模式，捕捉最终用户数量、定时活动、应用程序活动变化。关键网络指标有吞吐量（网络接口每秒接收与传输字节数 ）、连接数（TCP每秒连接数 ）、错误（丢包计数器 ）、TCP重传输、TCP乱序数据包 。使用网络带宽限制环境中，需收集应用的限额统计数据 。
+ **数据包嗅探**：也称数据包捕捉，从网络捕捉数据包检查协议报文头和数据，CPU和存储系统开销高，可能是观察性分析最后手段。内核可能利用环形缓冲区减少开销，如Linux的PF_RING选项 。数据包捕捉日志可创建于服务器端用其他工具分析，日志包含时间戳、数据包（协议头和负载数据 ）、元数据（数据包数量、丢包数量 ） 。捕捉实现通常允许用户提供过滤表达式和内核过滤以减少系统开销 。
+ **TCP分析**：除延时分析中介绍内容外，还可调查TCP发送/接收缓冲使用、TCP积压队列使用、积压队列导致的内核丢包、阻塞窗口大小（包括零长度通知 ）、TCP TIME - WAIT间隔中接收到的SYN 。服务器频繁用相同源和目的IP地址连接另一服务器同一目标端口时，客户端源端口（短暂端口 ）受操作系统参数限制，可能出现新连接与处于TIME - WAIT的TCP会话关联而被拒的问题，Linux内核尝试重用或回收连接避免此问题 。
+ **挖掘分析**：挖掘处理数据包的层次直至网络接口驱动，按需研究内核网络栈内部运行，用于检查网络可调参数是否需调整、确认内核网络性能特性是否生效、解释内核丢包 ，常采用动态跟踪审查内核网络栈函数执行，操作耗时 。
+ **静态性能调优**：注重解决配置完成环境中的问题，对网络性能，检查网络接口数量、最大速度、协商速度、双工模式、MTU、链路聚合、设备驱动及IP/TCP层可调参数、非默认值参数、路由配置、默认路由、数据路径组件吞吐量、数据转发、系统是否作路由器、DNS设置、网络接口固件和设备驱动及内核TCP/IP栈性能问题、软件施加的网络吞吐量限制等 。这些问题答案可能揭示被忽视的配置选择，网络吞吐量受限问题与云计算尤其相关 。 
+ **资源控制**：操作系统按连接类型、进程或进程组设置控制限制网络资源，包括网络带宽限制（内核针对不同协议或应用程序的允许带宽 ）、IP服务品质（网络组件应用的网络流量优先排序，如IP报文头业务类型位 ） 。网络可能存在高低优先级混合流量，资源控制方案可节流低优先级流量，为高优先级流量提供更好性能 。 
+ **微基准测试**：许多基准测试工具可用于网络，调查分布式应用程序环境吞吐量问题时，可确认网络能否达到预期吞吐量，未达到时用其调查网络性能，一般比应用程序简单易测。可测试要素包括方向（发送或接收 ）、协议（TCP或UDP及端口 ）、线程数、缓冲长度、接口MTU长度 。高速网络接口（如10Gb/s ）可能需多个客户线程驱动达到最大带宽 ，10.7.1节会介绍网络微基准测试工具iperf 。

<h3 id="nIDdL">10.6 分析</h3>
介绍基于Linux和Solaris操作系统的网络性能分析工具，使用策略参考前文，工具列表如下：

| Linux | Solaris | 描述 |
| --- | --- | --- |
| netstat | netstat | 多种网络栈和接口统计信息 |
| sar | - | 统计信息历史 |
| ifconfig | ifconfig | 接口配置 |
| ip | dladm | 网络接口统计信息 |
| nicstat | nicstat | 网络接口吞吐量和使用率 |
| ping | ping | 测试网络连通性 |
| traceroute | traceroute | 测试网络路由 |
| pathchar | pathchar | 确定网络路径特征 |
| tcpdump | snoop/tcpdump | 网络数据包嗅探器 |
| Wireshark | Wireshark | 图形化网络数据包检查器 |
| DTrace, perf | DTrace | TCP/IP栈跟踪：连接、数据包、丢包、延时 |


<h4 id="HRqxb">10.6.1 netstat</h4>
+ **功能**：基于不同选项，netstat命令可报告多种网络统计数据。
+ **Linux系统**
    - **选项功能**：默认列出连接的套接字；-a列出所有套接字信息；-s提供网络栈统计信息；-i显示网络接口信息；-r列出路由表；-n不解析IP地址为主机名；-v（可用时）显示冗长详细信息 。
    - **接口统计信息示例**：数据包括网络接口、MTU及接收（RX - ）和传输（TX - ）指标，如OK（成功传输数据包 ）、ERR（错误数据包 ）、DRP（丢包 ）、OVR（超限 ） 。丢包和超限是网络接口饱和指针，可结合USE方法检查 。-c连续模式与 -i 一起使用，每秒输出累积计数器及数据包速率数据 。
    - **网络栈统计数据示例**：输出多项按协议分组的网络数据（多来自TCP ），部分指标如比接收总包数更高速的包转发率（检查服务器是否应转发数据包 ）、开放的被动连接（监视客户端连接负载 ）、比发送的段数更高的数据报重传数（指示网络不稳定 ）、套接字缓冲导致的数据包从接收队列删除（网络饱和标志 ） 。部分统计信息名称拼写有误，可从/proc/net/snmp和/proc/net/netstat读取，也用于SNMP管理信息库（MIB ） 。netstat按时间间隔连续输出累计计数器，后期处理可计算计数速率 。
+ **Solaris系统**
    - **接口统计信息示例**：数据包括网络接口、MTU、网络、接口地址及Ipkts（输入数据包 ）、Ierrs（输入数据包错误 ）、Opkts（输出数据包 ）、Oerrs（输出数据包错误 ）、Collis（碰撞数据包 ）、Queue（队列，常为零 ） 。提供时间间隔参数时按时间总结接口，-I选项可指定显示接口 。
    - **网络栈统计数据示例**：输出多种按协议分组的网络统计信息，许多名称基于SNMP网络MIB 。值得关注指标类似Linux，如tcpListenDrop和tcpListenDropQ0（显示套接字监听积压队列和SYN积压队列丢弃数据包 ） 。报告可从kstat、libkstat接口查阅 。提供时间间隔可输出系统启动至今总结及按时间间隔总结（仅显示该间隔统计信息，速率易见 ） 。

<h4 id="NYlWx">10.6.2 sar</h4>
+ **功能**：系统活动报告工具，可观测当前活动，能配置保存和报告历史统计数据。
+ **Linux版本**：提供网络统计信息，选项及对应统计信息如下：
    - **-n DEV**：网络接口统计信息，如rxpck/s（接收数据包/秒 ）、txpck/s（传输数据包/秒 ）等。
    - **-n EDEV**：网络接口错误，像rxerr/s（接收数据包错误/秒 ）等。 
    - **-n IP**：IP数据报信息，包括irec/s（输入数据报文/秒 ）等。 
    - **-n EIP**：IP错误统计，如idisc/s（输出的丢弃报文/秒 ） 。 
    - **-n TCP**：TCP统计信息，有active/s（新的被动TCP连接/秒 ）等。 
    - **-n ETCP**：TCP错误统计，如retrans/s（TCP段重传/秒 ） 。 
    - **-n SOCK**：套接字使用，包括totsck（TCP使用的总数据包 ）等 。
+ **Solaris版本**：不提供网络统计信息，可使用netstat(1M)、nicstat(1)和dladm(1M)替代 。

<h4 id="bA1XA">10.6.3 ifconfig</h4>
+ **功能**：可手动设置网络接口，列出接口当前配置，辅助静态性能调优。
+ **Linux版本**：已被ip(8)命令淘汰，可显示网络接口设置，如网卡硬件地址、IP地址等，计数器与netstat -i命令一致，txqueuelen（发送队列长度 ）可优化，对延迟敏感设备设较小值可防高速传输拥塞 。
+ **Solaris版本**：功能逐渐被ipadm(1M)和dladm(1M)命令取代 。

<h4 id="KTefO">10.6.4 ip</h4>
+ **功能**：Linux的ip(8)命令可配置网络接口和路由，观测其状态与统计信息。
+ **统计信息**：能显示连接统计信息，除接收（RX）和传输（TX）字节数（与netstat -i一致 ），还方便观测吞吐量，但不支持按时间间隔输出（可利用sar ） 。

<h4 id="WvwMQ">10.6.5 nicstat</h4>
+ **功能**：最初基于Solaris系统编写的开源工具，输出网络接口统计信息，如吞吐量、使用率等，适用于USE方法。
+ **输出信息**：包括自系统启动以来总结及按时间间隔总结的数据，字段有接口名称（Int）、最大使用率（%Util）、反映接口饱和度统计信息（Sat）等 ，支持多种选项，如 -z忽略数值为0的接口，-t显示TCP统计信息 。

<h4 id="iSGj7">10.6.6 dladm</h4>
+ **功能**：在Solaris系统中，dladm(1M)命令可提供接口统计信息（数据包和字节率、错误率、使用率 ），显示物理接口状态 。
+ **输出信息**：dladm show - link输出接口接收和传输字节总和等，dladm show - link -S可显示千字节速率、数据包率及%Util列；dladm show - phy可显示物理接口状态（如速率、双工模式 ） ，检查接口是否协商到最高速率有助于静态性能调优 。

<h4 id="LbFpL">10.6.7 ping</h4>
+ **功能**：发送ICMP echo请求测试网络连通性。
+ **输出信息**：输出显示每个往返时间、包丢失率等统计信息，因时间戳由ping命令自己计算，包含处理网络I/O的CPU代码路径时间 。在Solaris中，路由器处理ICMP数据包优先级可能较低，延时可能比正常情况高 。

<h4 id="pJM5G">10.6.8 traceroute</h4>
+ **功能**：发送一系列数据包探测到主机当前路由，利用数据包协议生存时间（TTL）让网关按顺序发送ICMP超时响应报文 。
+ **输出信息**：显示每一跳响应的三个RTT值，可用于估计网络延时，路径可作为静态性能调优研究对象，网络动态变化可能降低其有效性 。

<h4 id="oGFDl">10.6.9 pathchar</h4>
+ **功能**：类似于traceroute，通过多次重复发送不同长度网络数据包并统计分析，确定每一跳带宽 。
+ **现状**：很难找到在现代操作系统上工作的版本，运行耗时，虽有减少时间的方法但仍不太实用 。

<h4 id="ltCoP">10.6.10 tcpdump</h4>
+ **功能**：捕捉并检查网络数据包，可输出数据包信息到标准输出或写入文件供后续分析 。
+ **输出信息**：输出显示数据包时间（毫秒精度 ）、源和目标IP地址、TCP报头值等 。可使用选项深入分析，如 -n避免解析IP地址为主机名，-e显示链路层报头 ，在性能分析中，可按时间差（-ttt）或自第一个数据包时间（-tttt）显示 ，还可用表达式过滤数据包（参考pcap - filter(7) ） 。因对CPU和存储开销大，尽量短时间使用，Solaris系统可添加该工具 。

<h4 id="QyBtb">10.6.11 snoop</h4>
+ **功能**：Solaris系统中默认的数据包捕捉和检查工具，工作模式类似tcpdump，能捕捉数据包到文件供后续检查，数据包捕捉文件遵从RFC 1761标准 。
+ **使用示例**：如捕捉ixgbe0接口数据到/tmp文件，可使用命令`snoop -d ixgbe0 > /tmp/out.snoop` 。输出显示已接收数据包，可用安静模式（-q）阻止输出避免网络数据拥塞 。可通过多种选项检查数据包，如 -e -l显示详细信息，-v打印冗长输出，-s设置折断长度截短数据包 ，还可用表达式过滤数据包以提高效率 。

<h4 id="E6DSn">10.6.12 Wireshark</h4>
+ **功能**：图形化的数据包捕捉和检查工具，过去称作Ethereal，可从tcpdump或snoop的转储文件导入数据包，能识别网络接口及相关数据包，翻译数百种协议包头，适用于深层次网络分析 。

<h4 id="pFIzk">10.6.13 DTrace</h4>
+ **功能**：可在内核和应用程序内部检查网络事件，如套接字连接、套接字I/O、TCP事件、数据包传输、积压队列丢包、TCP重传等，支持工作负载特征归纳和延时分析 。
+ **provider列表**：不同层次有稳定和不稳定的provider，应用层稳定的是app，不稳定的是pid等；系统调用层不稳定的是syscall等 。使用稳定provider更可取，若不可用，可使用不稳定的，但需常更新脚本适配软件改动 。
+ **套接字连接**
    - **出站连接计数**：用`dtrace -n 'syscall::connect:entry {@[execname] = count();}'`可计算connect()系统调用次数，还可输出如PID、进程参数及connect()参数等其他细节 。
    - **入站连接计数**：通过`dtrace -n 'syscall::accept:return {@[execname] = count();}'`可统计accept()调用次数及相关进程接收连接数 。
    - **跟踪特定进程连接**：如跟踪ssh进程的connect()，用`dtrace -n 'syscall::connect:entry /execname == "ssh"/ { ustack(); }'` ，还能检查相关参数，由soconnect.d脚本实现 。
+ **套接字I/O**
    - **关联数组跟踪**：跟踪`syscall::socket:return`建立关联数组（如`is_socket[pid,arg1] = 1;` ），对比后续I/O系统调用确认套接字文件描述符，注意清除`syscall::close:entry`的值 。
    - **VFS跟踪**：若DTrace版本支持，可通过`fds[].fi_fs`状态跟踪套接字I/O，如用`dtrace -n 'syscall::read:entry,syscall::recv:entry /fds[arg0].fi_fs == "sockfs"/ { @[probefunc] = count(); }'`按execname计数套接字读取 ，用类似命令可计数套接字写入 。同时需注意操作系统可能使用系统调用变种（如ready() ），需跟踪 ，还可跟踪系统调用返回值检查I/O长度 。
+ **套接字延时**
    - **连接延时**：同步系统调用下，是connect()消耗时间；非阻塞I/O时，是执行connect()至poll()或select()（或其他系统调用）报告套接字就绪的时间 。
    - **首字节延时**：从执行connect()或从accept()返回，到第一个字节数据由I/O系统调用从套接字接收的时间 。
    - **套接字持续时间**：同一个文件描述符从socket()到close()的时间，可从connect()或accept()开始计时 ，可通过命令或脚本在系统调用层或其他网络栈层执行测量 。
+ **套接字内部活动**：在Linux中，用`dtrace -ln 'fbt::sock*:entry'`可列出以sock_开头的函数，单独跟踪这些函数及其参数和时间戳，有助于了解套接字行为 。
+ **TCP事件**
    - **tcp provider探针**：稳定的tcp provider有多个探针，如`accept-established`（接收入站连接 ）、`connect-request`（启动出站连接 ）等 ，通过`dtrace -n 'tcp::accept-established { @[args[3]->tope_saddr, args[3]->tope_lport] = count(); }'`可跟踪主机接收TCP连接及远程IP地址和本地端口情况 。
    - **fbt provider跟踪**：类似套接字，可用fbt provider跟踪TCP内核运行，列出`dtrace -ln 'tcp:::'`可查看相关探针，用fbt provider跟踪能发掘更多内核代码细节 。
+ **数据包传输**：当tcp provider无法触及或不可用时，可用fbt provider调查内核内部运行 。如在Linux中，用`dtrace -n 'fbt::ip_output:entry { @[stack(100)] = count(); }'`跟踪ip_output()，用`dtrace -n 'fbt::tcp_sendmsg:entry { @["TCP send bytes"] = quantize(arg3); }'`统计TCP发送长度 ，可使用单命令或脚本调查TCP重传和积压队列丢包等问题 。

<h4 id="UzWkF">重传输跟踪</h4>
+ **功能**：研究TCP重传输有助于调查网络健康状况，DTrace能以较低系统开销实时检查重传输 。
+ **Linux 3.2/6版本内核脚本示例**：使用脚本跟踪tcp_retransmit_skb()函数，输出包含时间、目的IP地址及内核栈跟踪信息，有助于解释重传输原因 ，还可独立跟踪每个内核栈函数了解更多细节 。
+ **SmartOS脚本示例**：针对SmartOS开发的脚本输出显示TCP重传输目的IP地址及内核TCP状态 。

<h4 id="yzDAj">积压队列丢包</h4>
+ **功能**：通过脚本判断积压队列调优是否必要及有效 。
+ **脚本原理**：该脚本利用获取TCP状态的fbt provider和丢包计数的mib provider 。
+ **示例输出分析**：输出显示缓存的进程ID（cpid）、当前套接字积压队列最大长度（max_q）以及积压队列长度分布图 。如PID 11504有34个积压队列丢包，最大积压队列长度为128，多数时间队列长度为0，可能需增加队列长度；而另一个示例中积压队列常达极限128，意味着应用程序过载且资源（通常是CPU）不足 。通常在丢包发生时调整积压队列，netstat -s中的tcpListenDrops可观测丢包，此DTrace脚本可预测丢包并在问题出现前调整 。

<h4 id="YlM8q">更多跟踪</h4>
+ **高级网络跟踪脚本列表**：表10.8列出众多DTrace脚本，涵盖socket、IP、TCP、UDP、ICMP等不同层次和协议 。如soconnect.d跟踪客户端套接字connect()，显示进程和主机；tcpaccept.d是入站TCP连接摘要等 。这些脚本可在《Network Lower-Level Protocols》中DTrace章节找到，也可在互联网获取 。此外，《DTrace》书中的Application Level Protocols章节还提供了跟踪NFS、CIFS、HTTP等协议的脚本 。但部分动态跟踪脚本与特定内核内部结构紧密相关，需随内核版本更新维护，部分脚本基于特定DTrace provider，可能特定操作系统暂无可用版本 。

![](https://cdn.nlark.com/yuque/0/2025/png/45533403/1746371659897-51f5834c-f0e5-4ed7-b8a8-850d6299b939.png)

![](https://cdn.nlark.com/yuque/0/2025/png/45533403/1746371696012-237d8454-7581-4a85-b0c9-99bad6efcbca.png)

<h4 id="RQnqi">10.6.14 SystemTap</h4>
+ **功能**：在Linux系统中可动态跟踪文件系统事件，如需转换之前的DTrace脚本，可参考第4章4.4节及附录E 。

<h4 id="jm0cp">10.6.15 perf</h4>
+ **功能**：作为第6章介绍的LPE工具包中的工具，能提供静态和动态跟踪网络事件功能，可确定导致内核网络活动的栈跟踪，通过后期处理可开发更高级工具 。
+ **示例**：用`perf probe --add='tcp_sendmsg'`添加新事件，`perf record -e probe:tcp_sendmsg -aR -g sleep 5`记录事件，`perf report --stdio`显示调用图（栈跟踪 ），输出展示了导致内核调用tcp_sendmsg()并由TCP连接发送数据的sshd栈跟踪 。此外，还有如`skb_free_skb`等预定义网络跟踪点事件，可用于套接字缓存事件和网络设备相关的网络调查 。

<h4 id="XTD8y">10.6.16 其他工具</h4>
+ **Linux工具**
    - **strace(1)**：跟踪套接字相关系统调用并检查使用选项，但系统开销较高 。
    - **lsof(8)**：按进程ID列出包括套接字细节在内的打开文件 。 
    - **ss(8)**：提供套接字统计信息 。 
    - **nfsstat(8)**：获取NFS服务器和客户机统计信息 。 
    - **iftop(8)**：按主机（嗅探）总结网络接口吞吐量 。 
    - **/proc/net**：包含许多网络统计信息文件 。
+ **Solaris工具**
    - **truss(1)**：跟踪套接字相关系统调用并检查使用选项，系统开销较高 。
    - **pfiles(1)**：按进程检查使用中的套接字，包括选项和套接字缓存大小 。 
    - **routeadm(1M)**：检查路由和IP转发状态 。 
    - **nfsstat(1M)**：提供NFS服务器和客户机统计信息 。 
    - **kstat**：提供源自网络栈和网络设备驱动的统计信息（很多在源代码外无文档 ） 。
+ **其他**：还有许多基于SNMP或定制代理软件的网络监测解决方案 。

<h3 id="JDJlM">10.8 调优</h3>
网络调优通常基于已调整好的可调节参数和能动态响应工作负载的网络栈，需先理解网络使用情况，借助前文工具进行工作负载特征分析和静态性能调优，不同操作系统版本参数有别，需参考对应文档 。

<h4 id="VnIzw">10.8.1 Linux</h4>
+ **参数查看与设置**：可用sysctl(8)命令查看和设置可调参数，写入/etc/sysctl.conf，也可在/proc文件系统中读写（位于/proc/sys/net下 ） 。如通过`sysctl -a | grep tcp`可查看TCP相关参数，内核（3.2.63）中有63个含tcp的可调参数，net下更多，涉及IP、Ethernet、路由和网络接口 。
+ **套接字和TCP缓冲**
    - **设置最大套接字缓冲**：所有协议类型读（rmem_max）和写（wmem_max）的最大套接字缓冲大小可设置，如`net.core.rmem_max = 16777216` ，对于10GbE连接，数值可能需设为16MB或更高 。
    - **启用TCP接收缓冲自动调整**：`tcp_moderate_rcvbuf = 1` 。
    - **TCP读写缓冲自动调优参数**：`net.ipv4.tcp_rmem`和`net.ipv4.tcp_wmem`分别设置读、写缓冲相关参数，每个参数有最小、默认和最大字节数，默认值自动调整，提高吞吐量可增加最大值 。
+ **TCP积压队列**
    - **半开连接积压队列**：`tcp_max_syn_backlog = 4096` 。
    - **侦听积压队列**：`net.core.somaxconn = 1024` ，两者数值或许需从默认值调高，如4096和1024或更高，以应对突发负载 。
+ **设备积压队列**：增加每CPU的网络设备积压队列长度，如`net.core.netdev_max_backlog = 10000` ，对于10GbE的NIC，可能需增加到10000 。
+ **TCP阻塞控制**
    - **查看可用算法**：`sysctl net.ipv4.tcp_available_congestion_control` ，当前可用`cubic reno` 。
    - **添加新算法**：如添加htcp算法，可执行相关命令`modprobe tcp_htcp`等 。
    - **选择算法**：`net.ipv4.tcp_congestion_control = cubic` 。
+ **TCP选项**：如SACK和FACK扩展相关参数，`net.ipv4.tcp_sack = 1`等，可在高延时网络中提高吞吐量，但以一定CPU为代价 。`tcp_tw_reuse`可使两个主机在安全连接（如Web服务器和数据库服务器 ）间重用TIME - WAIT会话，避免达到16b的TIME - WAIT会话临时端口极限；`tcp_tw_recycle`也是重用方法，但安全性不如前者 。
+ **网络接口**：可使用ifconfig(8)增加TX队列长度，如`ifconfig eth0 txqueuelen 10000` ，对于10GbE NIC可能必需，可添加到/etc/rc.local以便启动时应用 。
+ **资源控制**：控制组（cgroups）的网络优先级（net.prio）子系统能对进程或进程组出站网络通信应用优先级，照顾高优先级网络通信（如生产负载 ），配置优先级数值会翻译为IP ToS水平或类似方案并包含于数据包中 。

<h4 id="JazXz">10.8.2 Solaris</h4>
+ **参数设置与查看**：参数设置或查看可由ndd(1M)命令完成，也可通过/etc/system设置由ndd(1M)查看，现正迁移到ipadm(1M)命令（统一灵活的管理IP栈属性工具 ） ，如用`ipadm show - prop`可列出相关属性 。通常需参考对应版本的《Solaris Tunable Parameters Reference Manual》，确定参数能否修改及修改前是否需被供应商禁止 。
+ **缓冲**
    - **设置多种可调参数**：如`ndd -set /dev/tcp tcp_max_buf 16777216`等，`tcp_max_buf`是setsockopt()能设置的最大套接字缓冲，`tcp_cwnd_max`是TCP阻塞窗口最大值，`tcp_xmit_hiwat`和`tcp_recv_hiwat`用于设置网络设备发送和接收TCP窗口大小，在ipadm(1M)中对应`send_maxbuf`和`recv_maxbuf` 。
+ **TCP积压队列**
    - **调整积压队列参数**：如`ndd -set /dev/tcp tcp_conn_req_max_q0 4096` ，`q0`参数对应半开队列，`q`参数对应监听积压队列，两者数值或许需从默认值调高，如4096和1024或更高，以应对突发负载 。
+ **TCP选项**
    - **调优参数**：`tcp_smallest_anon_port`可降低到10000，提高可用临时端口范围；`tcp_time_wait_interval`可由默认的60000降低（单位毫秒 ），但RFC 1122禁止重用TIME - WAIT会话，此参数可能被禁止；`tcp_iss_incr`降低有助于内核检测新会话并自动重用TIME - WAIT会话（已添加到Illumos内核 ）；`tcp_slow_start_initial`通常设为允许的最大值，可使TCP会话更快达到高输出 。
+ **网络设备**
    - **激活IP扇出**：`ndd ip:ip_squeue_fanout=1` ，可将网络负载分布到所有CPU提高性能，默认关联处理创建连接的CPU，可能导致负载分配不均，使部分CPU使用率达100%成瓶颈 。
+ **资源控制**：dladm(1M)工具可设置网络接口属性（如maxbw设最大带宽 ），应用到虚拟接口（如云计算客户租户 ）使其成为控制流量的机制 ；flowadm(1M)工具提供更精细控制，能定义流、分配传输（TCP）和端口，包含maxbw和priority属性，优先级可设为“低”“正常”“高”或“实时” ，基于Solaris的系统还有IP QoS（IPQoS），由ipqosadm(1M)设置 。

<h4 id="K5hO2">10.8.3 配置</h4>
+ **以太网巨型帧**：若网络基础架构支持，将默认MTU从1500增加到9000左右可提高网络吞吐性能 。
+ **链路聚合**：多个网络接口可组合合并带宽成一个接口，需交换机支持并配置才能正常工作 。
+ **套接字选项**：应用程序可用setsockopt()调优缓冲区大小，增加到系统极限可提高网络吞吐性能，此方法对Linux和Solaris操作系统通用 。

